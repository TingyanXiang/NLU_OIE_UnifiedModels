{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import jieba\n",
    "import re\n",
    "\n",
    "from config import vocab_pred, vocab_pred_size, vocab_prefix\n",
    "from config import UNK_index, PAD_index, SOS_index, EOS_index \n",
    "from config import OOV_pred_index, PAD_pred_index, EOS_pred_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('Machine_Translation_NLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceMisspred(predicate):\n",
    "    '''replace missing predicate\n",
    "    '''\n",
    "    if predicate == '_':\n",
    "        return 'P'\n",
    "    else:\n",
    "        return predicate\n",
    "    \n",
    "def character_segmentation(string):\n",
    "    res = []\n",
    "    for part in list(jieba.cut(string, cut_all=False)):\n",
    "        if re.match('^[\\da-zA-Z]+$', part):\n",
    "            res.append(part)\n",
    "        else:\n",
    "            res.extend(list(part))\n",
    "    return res\n",
    "\n",
    "# def replaceMissinfo(aaa):\n",
    "#     '''replace missing info for subjects/objects\n",
    "#     '''\n",
    "#     placeholder = ['Z','Y','X']\n",
    "#     for i in range(len(aaa)):\n",
    "#         if aaa[i] == '_':\n",
    "#             aaa = aaa[:i] + placeholder.pop() + aaa[i+1:]\n",
    "#     return aaa\n",
    "\n",
    "def load_preprocess_data(data_add):\n",
    "    saoke = []\n",
    "    with open(data_add, 'r') as f:\n",
    "        for line in f:\n",
    "            saoke.append(json.loads(line))\n",
    "    data = []\n",
    "    # list of dict\n",
    "    for sample in saoke:\n",
    "        # remove some exceptions with empty facts\n",
    "        if sample['logic'] == []:\n",
    "            continue\n",
    "        # tokenize src sentence\n",
    "        sample_processed = dict()\n",
    "        sample_processed['src_org'] = sample['natural']\n",
    "        #sample_processed['src'] = list(jieba.cut(sample['natural'], cut_all=False))\n",
    "        sample_processed['src'] = character_segmentation(sample['natural'])\n",
    "        \n",
    "        # transform fact list into str and tokenize\n",
    "        # $ separates facts; @ separate elements for one fact; & separate objects for one fact\n",
    "        sample_processed['tgt_org'] = sample['logic']\n",
    "        logic_list = []\n",
    "        for fact in sample['logic']:\n",
    "            logic_list.append('@'.join([fact['subject'], replaceMisspred(fact['predicate']), \n",
    "                                       '&'.join(fact['object'])]))\n",
    "        logic_str = '$'.join(logic_list)\n",
    "        sample_processed['tgt'] = character_segmentation(logic_str)\n",
    "        #list(jieba.cut(logic_str, cut_all=False))\n",
    "\n",
    "        data.append(sample_processed)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import dropwhile\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name, emb_pretrained_add=None, max_vocab_size=None):\n",
    "        self.name = name\n",
    "        self.word2index = None #{\"$PAD$\": PAD_token, \"$SOS$\": SOS_token, \"$EOS$\": EOS_token, \"$UNK$\": UNK_token}\n",
    "        #self.word2count = None #{\"$PAD$\": 0, \"$SOS$\" : 0, \"$EOS$\": 0, \"$UNK$\": 0}\n",
    "        self.index2word = None #{PAD_token: \"$PAD$\", SOS_token: \"$SOS$\", EOS_token: \"$EOS$\", UNK_token: \"$UNK$\"}\n",
    "        self.max_vocab_size = max_vocab_size  # Count SOS and EOS\n",
    "        self.vocab_size = None\n",
    "        self.emb_pretrained_add = emb_pretrained_add\n",
    "        self.embedding_matrix = None\n",
    "\n",
    "    def build_vocab(self, data):\n",
    "        all_tokens = []\n",
    "        for sample in data:\n",
    "            all_tokens.extend(sample['src'])\n",
    "            all_tokens.extend(sample['tgt'])  \n",
    "        token_counter = Counter(all_tokens)\n",
    "        print('The number of unique tokens totally in dataset: ', len(token_counter))\n",
    "        # remove word with freq==1 \n",
    "        for key, count in dropwhile(lambda key_count: key_count[1] > 1, token_counter.most_common()):\n",
    "            del token_counter[key]\n",
    "        \n",
    "        if self.max_vocab_size:\n",
    "            vocab, count = zip(*token_counter.most_common(self.max_vocab_size))\n",
    "        else:\n",
    "            vocab, count = zip(*token_counter.most_common())\n",
    "        \n",
    "        self.index2word = vocab_prefix + list(vocab)\n",
    "        word2index = dict(zip(self.index2word, range(0, len(self.index2word)))) \n",
    "#         word2index = dict(zip(vocab, range(len(vocab_prefix),len(vocab_prefix)+len(vocab)))) \n",
    "#         for idx, token in enumerate(vocab_prefix):\n",
    "#             word2index[token] = idx\n",
    "        self.word2index = word2index\n",
    "        self.vocab_size = len(self.index2word)\n",
    "        return None \n",
    "\n",
    "    def build_emb_weight(self):\n",
    "        words_emb_dict = load_emb_vectors(self.emb_pretrained_add)\n",
    "        emb_weight = np.zeros([self.vocab_size, 300])\n",
    "        for i in range(len(vocab_prefix), self.vocab_size):\n",
    "            emb = words_emb_dict.get(self.index2word[i], None)\n",
    "            if emb is not None:\n",
    "                try:\n",
    "                    emb_weight[i] = emb\n",
    "                except:\n",
    "                    pass\n",
    "                    #print(len(emb), self.index2word[i], emb)\n",
    "        self.embedding_matrix = emb_weight\n",
    "        return None\n",
    "\n",
    "def load_emb_vectors(fasttest_home):\n",
    "    max_num_load = 500000\n",
    "    words_dict = {}\n",
    "    with open(fasttest_home) as f:\n",
    "        for num_row, line in enumerate(f):\n",
    "            if num_row >= max_num_load:\n",
    "                break\n",
    "            s = line.split()\n",
    "            words_dict[s[0]] = np.asarray(s[1:])\n",
    "    return words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2index(data, key, word2index):\n",
    "    '''\n",
    "    transform tokens into index as input for both src and tgt\n",
    "    '''\n",
    "    indexdata = []\n",
    "    for line in data:\n",
    "        line = line[key]\n",
    "        indexdata.append([word2index[c] if c in word2index.keys() else UNK_index for c in line])\n",
    "        #indexdata[-1].append(EOS_index)\n",
    "    print('finish indexing')\n",
    "    return indexdata\n",
    "\n",
    "def construct_Lang(name, data, emb_pretrained_add = None, max_vocab_size = None):\n",
    "    lang = Lang(name, emb_pretrained_add, max_vocab_size)\n",
    "    lang.build_vocab(data)\n",
    "    if emb_pretrained_add:\n",
    "        lang.build_emb_weight()\n",
    "    return lang\n",
    "\n",
    "def text2symbolindex(data, key, word2index):\n",
    "    '''get generation label for tgt \n",
    "    '''\n",
    "    indexdata = []\n",
    "    for line in data:\n",
    "        line = line[key]\n",
    "        indexdata.append([word2index[c] if c in word2index.keys() else OOV_pred_index for c in line])\n",
    "        #indexdata[-1].append(EOS_index)\n",
    "    print('symbol label finish')\n",
    "    return indexdata\n",
    "\n",
    "def copy_indicator(data, src_key='src', tgt_key='tgt'):\n",
    "    '''get copy label for tgt\n",
    "    '''\n",
    "    indicator = []\n",
    "    for sample in data:\n",
    "        tgt = sample[tgt_key]\n",
    "        src = sample[src_key]\n",
    "        matrix = np.zeros((len(tgt), len(src)), dtype=int)\n",
    "        for m in range(len(tgt)):\n",
    "            for n in range(len(src)):\n",
    "                if tgt[m] == src[n]:\n",
    "                    matrix[m,n] = 1\n",
    "        indicator.append(matrix)\n",
    "    return indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_add = '/scratch/tx443/NLU/project/SAOKE_DATA.json'\n",
    "data = load_preprocess_data(data_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train val test\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_train_data = sorted(train_data, key=lambda x: len(x['tgt']), reverse=False)\n",
    "# train_data = sorted_train_data[0:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique tokens totally in dataset:  9364\n"
     ]
    }
   ],
   "source": [
    "# build vocab from train for input indexing\n",
    "trainLang = construct_Lang('train', train_data)\n",
    "\n",
    "# build generation vocab for prediction\n",
    "word2symbolindex = {}\n",
    "for idx, token in enumerate(vocab_pred):\n",
    "        word2symbolindex[token] = idx\n",
    "\n",
    "# check\n",
    "assert(UNK_index==trainLang.word2index['<UNK>'])\n",
    "assert(PAD_index==trainLang.word2index['<PAD>'])\n",
    "assert(SOS_index==trainLang.word2index['<SOS>'])\n",
    "assert(EOS_index==trainLang.word2index['<EOS>'])\n",
    "\n",
    "assert(OOV_pred_index==word2symbolindex['<OOV>'])\n",
    "assert(PAD_pred_index==word2symbolindex['<PAD>'])\n",
    "assert(EOS_pred_index==word2symbolindex['<EOS>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permute facts at this place; data['tgt']\n",
    "train_len = len(train_data)\n",
    "for i in range(train_len):\n",
    "    facts_list = ''.join(train_data[i]['tgt']).split('$')\n",
    "    facts_list_pm = facts_list[::-1]\n",
    "    train_data[i]['tgt'] = character_segmentation('$'.join(facts_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish indexing\n",
      "finish indexing\n"
     ]
    }
   ],
   "source": [
    "# input indexing for src\n",
    "train_src_input_index = text2index(train_data, 'src', trainLang.word2index) \n",
    "val_src_input_index = text2index(val_data, 'src', trainLang.word2index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish indexing\n",
      "finish indexing\n"
     ]
    }
   ],
   "source": [
    "# input indexing for tgt\n",
    "train_tgt_input_index = text2index(train_data, 'tgt', trainLang.word2index) \n",
    "val_tgt_input_index = text2index(val_data, 'tgt', trainLang.word2index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symbol label finish\n",
      "symbol label finish\n"
     ]
    }
   ],
   "source": [
    "# get generation label\n",
    "train_label_symbolindex = text2symbolindex(train_data, 'tgt', word2symbolindex)\n",
    "val_label_symbolindex = text2symbolindex(val_data, 'tgt', word2symbolindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get copy label\n",
    "train_indicator = copy_indicator(train_data, 'src', 'tgt')\n",
    "val_indicator = copy_indicator(val_data, 'src', 'tgt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28564, 28564, 28564, 28564, 28564)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_src_input_index),len(train_tgt_input_index),len(train_label_symbolindex),len(train_indicator),len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6121, 6121, 6121, 6121, 6121)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_src_input_index),len(val_tgt_input_index),len(val_label_symbolindex),len(val_indicator),len(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "# from Data_utils import VocabDataset, vocab_collate_func\n",
    "# from preprocessing_util import preposs_toekn, Lang, text2index, construct_Lang\n",
    "from Multilayers_Encoder import EncoderRNN\n",
    "from Multilayers_Decoder import DecoderAtten, sequence_mask\n",
    "from config import device, embedding_freeze\n",
    "import random\n",
    "from evaluation import similarity_score, check_fact_same, predict_facts, evaluate_prediction\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(src_data, tgt_data, encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
    "          teacher_forcing_ratio, vocab):\n",
    "    src_org_batch, src_tensor, src_true_len = src_data\n",
    "    tgt_org_batch, tgt_tensor, tgt_label_vocab, tgt_label_copy, tgt_true_len = tgt_data\n",
    "    '''\n",
    "    finish train for a batch\n",
    "    '''\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    batch_size = src_tensor.size(0)\n",
    "    encoder_hidden, encoder_cell = encoder.initHidden(batch_size)\n",
    "    loss = 0\n",
    "    encoder_outputs, encoder_hidden, encoder_cell = encoder(src_tensor, encoder_hidden, src_true_len, encoder_cell)\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_index]*batch_size], device=device).transpose(0,1)\n",
    "    decoder_hidden, decoder_cell = encoder_hidden, decoder.initHidden(batch_size)\n",
    "    step_log_likelihoods = []\n",
    "    #print(decoder_hidden.size())\n",
    "    #print('encoddddddddddder finishhhhhhhhhhhhhhh')\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        ### Teacher forcing: Feed the target as the next input\n",
    "        decoding_token_index = 0\n",
    "        tgt_max_len_batch = tgt_true_len.cpu().max().item()\n",
    "        assert(tgt_max_len_batch==tgt_tensor.size(1))\n",
    "        while decoding_token_index < tgt_max_len_batch:\n",
    "            decoder_output, decoder_hidden, decoder_attention, decoder_cell = decoder(\n",
    "                decoder_input, decoder_hidden, src_true_len, encoder_outputs, decoder_cell)\n",
    "\n",
    "            decoding_label_vocab = tgt_label_vocab[:, decoding_token_index]\n",
    "            decoding_label_copy = tgt_label_copy[:, decoding_token_index, :]\n",
    "            copy_log_probs = decoder_output[:, vocab_pred_size:]+(decoding_label_copy.float()+1e-45).log()\n",
    "            #mask sample which is copied only\n",
    "            gen_mask = ((decoding_label_vocab!=OOV_pred_index) | (decoding_label_copy.sum(-1)==0)).float() \n",
    "            log_gen_mask = (gen_mask + 1e-45).log().unsqueeze(-1)\n",
    "            #mask log_prob value for oov_pred_index when label_vocab==oov_pred_index and is copied \n",
    "            generation_log_probs = decoder_output.gather(1, decoding_label_vocab.unsqueeze(1)) + log_gen_mask\n",
    "            combined_gen_and_copy = torch.cat((generation_log_probs, copy_log_probs), dim=-1)\n",
    "            step_log_likelihood = torch.logsumexp(combined_gen_and_copy, dim=-1)\n",
    "            step_log_likelihoods.append(step_log_likelihood.unsqueeze(1))\n",
    "            #loss += criterion(decoder_output, tgt_tensor[:,decoding_token_index])\n",
    "            decoder_input = tgt_tensor[:,decoding_token_index].unsqueeze(1)  # Teacher forcing\n",
    "            decoding_token_index += 1\n",
    "\n",
    "    else:\n",
    "        ### Without teacher forcing: use its own predictions as the next input\n",
    "        decoding_token_index = 0\n",
    "        tgt_max_len_batch = tgt_true_len.cpu().max().item()\n",
    "        assert(tgt_max_len_batch==tgt_tensor.size(1))\n",
    "        while decoding_token_index < tgt_max_len_batch:\n",
    "            decoder_output, decoder_hidden, decoder_attention_weights, decoder_cell = decoder(\n",
    "                decoder_input, decoder_hidden, src_true_len, encoder_outputs, decoder_cell)\n",
    "\n",
    "            decoding_label_vocab = tgt_label_vocab[:, decoding_token_index]\n",
    "            decoding_label_copy = tgt_label_copy[:, decoding_token_index, :]\n",
    "            copy_log_probs = decoder_output[:, vocab_pred_size:]+(decoding_label_copy.float()+1e-45).log()\n",
    "            #mask sample which is copied only\n",
    "            gen_mask = ((decoding_label_vocab!=OOV_pred_index)|(decoding_label_copy.sum(-1)==0)).float() \n",
    "            log_gen_mask = (gen_mask + 1e-45).log().unsqueeze(-1)\n",
    "            #mask log_prob value for oov_pred_index when label_vocab==oov_pred_index and is copied \n",
    "            generation_log_probs = decoder_output.gather(1, decoding_label_vocab.unsqueeze(1)) + log_gen_mask\n",
    "            combined_gen_and_copy = torch.cat((generation_log_probs, copy_log_probs), dim=-1)\n",
    "            step_log_likelihood = torch.logsumexp(combined_gen_and_copy, dim=-1)\n",
    "            step_log_likelihoods.append(step_log_likelihood.unsqueeze(1))\n",
    "\n",
    "            topv, topi = decoder_output.topk(1, dim=-1)\n",
    "            next_input = topi.detach().cpu().squeeze(1)\n",
    "            decoder_input = []\n",
    "            for i_batch in range(batch_size):\n",
    "                pred_list = vocab_pred+src_org_batch[i_batch]\n",
    "                next_input_token = pred_list[next_input[i_batch].item()]\n",
    "                decoder_input.append(vocab.word2index.get(next_input_token, UNK_index))\n",
    "            decoder_input = torch.tensor(decoder_input, device=device).unsqueeze(1)\n",
    "            decoding_token_index += 1\n",
    "\n",
    "    # average loss\n",
    "    log_likelihoods = torch.cat(step_log_likelihoods, dim=-1)\n",
    "    # mask padding for tgt\n",
    "    tgt_pad_mask = sequence_mask(tgt_true_len).float()\n",
    "    log_likelihoods = log_likelihoods*tgt_pad_mask\n",
    "    loss = -log_likelihoods.sum()/batch_size\n",
    "    loss.backward()\n",
    "\n",
    "    ### TODO\n",
    "    # clip for gradient exploding \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return (loss*batch_size/tgt_pad_mask.sum()).item() #torch.div(loss, tgt_true_len.type_as(loss).mean()).item()  #/tgt_true_len.mean()\n",
    "\n",
    "\n",
    "def trainIters(train_loader, val_loader, encoder, decoder, num_epochs, learning_rate, \n",
    "               teacher_forcing_ratio, model_save_info, tgt_max_len, beam_size, vocab):\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    if model_save_info['model_path_for_resume'] is not None:\n",
    "        check_point_state = torch.load(model_save_info['model_path_for_resume'])\n",
    "        encoder.load_state_dict(check_point_state['encoder_state_dict'])\n",
    "        encoder_optimizer.load_state_dict(check_point_state['encoder_optimizer_state_dict'])\n",
    "        decoder.load_state_dict(check_point_state['decoder_state_dict'])\n",
    "        decoder_optimizer.load_state_dict(check_point_state['decoder_optimizer_state_dict'])\n",
    "\n",
    "    for epoch in range(num_epochs): \n",
    "        n_iter = -1\n",
    "        losses = np.zeros((len(train_loader),))\n",
    "        start_time = time.time()\n",
    "        for src_tensor, src_true_len, tgt_tensor, tgt_true_len, tgt_label_vocab, tgt_label_copy, src_org_batch, tgt_org_batch in train_loader:\n",
    "            n_iter += 1\n",
    "            #print('start_step: ', n_iter)\n",
    "            src_data = (src_org_batch, src_tensor, src_true_len)\n",
    "            tgt_data = (tgt_org_batch, tgt_tensor, tgt_label_vocab, tgt_label_copy, tgt_true_len)\n",
    "            loss = train(src_data, tgt_data, encoder, decoder, encoder_optimizer, \n",
    "                         decoder_optimizer, teacher_forcing_ratio, vocab)\n",
    "            losses[n_iter] = loss\n",
    "            if n_iter % 500 == 0:\n",
    "                pass\n",
    "                #print('Loss:', loss)\n",
    "                #eva_start = time.time()\n",
    "#                 precision, recall, val_loss = evaluate_batch(val_loader, encoder, decoder, tgt_max_len, vocab, vocab_pred_size)\n",
    "#                 #print((time.time()-eva_start)/60)\n",
    "#                 print('epoch: [{}/{}], step: [{}/{}], train_loss:{}, val_precision: {}, val_recall: {}, val_loss: {}'.format(\n",
    "#                     epoch, num_epochs, n_iter, len(train_loader), loss, precision.mean(), recall.mean(), val_loss))\n",
    "               # print('Decoder parameters grad:')\n",
    "               # for p in decoder.named_parameters():\n",
    "               #     print(p[0], ': ',  p[1].grad.data.abs().mean().item(), p[1].grad.data.abs().max().item(), p[1].data.abs().mean().item(), p[1].data.abs().max().item(), end=' ')\n",
    "               # print('\\n')\n",
    "               # print('Encoder Parameters grad:')\n",
    "               # for p in encoder.named_parameters():\n",
    "               #     print(p[0], ': ',  p[1].grad.data.abs().mean().item(), p[1].grad.data.abs().max().item(), p[1].data.abs().mean().item(), p[1].data.abs().max().item(), end=' ')\n",
    "               # print('\\n')\n",
    "        val_loss, src_org, tgt_org, tgt_pred = predict_facts(val_loader, encoder, decoder, tgt_max_len, vocab)\n",
    "        precision, recall = evaluate_prediction(tgt_org, tgt_pred)\n",
    "        print('epoch: [{}/{}], step: [{}/{}], train_loss:{}, val_precision: {}, val_recall: {}, val_loss: {}'.format(\n",
    "            epoch, num_epochs, n_iter, len(train_loader), losses.mean(), precision.mean(), recall.mean(), val_loss))\n",
    "\n",
    "#         if (epoch+1) % model_save_info['epochs_per_save_model'] == 0:\n",
    "#             check_point_state = {\n",
    "#                 'epoch': epoch,\n",
    "#                 'encoder_state_dict': encoder.state_dict(),\n",
    "#                 'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "#                 'decoder_state_dict': decoder.state_dict(),\n",
    "#                 'decoder_optimizer_state_dict': decoder_optimizer.state_dict()\n",
    "#                 }\n",
    "#             torch.save(check_point_state, '{}epoch_{}.pth'.format(model_save_info['model_path'], epoch))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = dict( \n",
    "    tgt_max_len = 130,\n",
    "    max_src_len_dataloader =94,\n",
    "    max_tgt_len_dataloader =127,\n",
    "\n",
    "    emb_size = 200,\n",
    "    en_hidden_size = 128,\n",
    "    en_num_layers = 2,\n",
    "    en_num_direction = 2,\n",
    "    de_hidden_size = 256,\n",
    "    de_num_layers = 3,\n",
    "    rnn_type = 'GRU', # {LSTM, GRU}\n",
    "    attention_type = 'dot_prod', #'dot_prod', general, concat #dot-product need pre-process\n",
    "    teacher_forcing_ratio = 1,\n",
    "\n",
    "    learning_rate = 1e-3,\n",
    "    num_epochs = 40,\n",
    "    batch_size = 64, \n",
    "    beam_size = 5,\n",
    "    dropout_rate = 0.0,\n",
    "\n",
    "    model_save_info = dict(\n",
    "        model_path = 'nmt_models/model1/',\n",
    "        epochs_per_save_model = 2,\n",
    "        model_path_for_resume = None #'nmt_models/epoch_0.pth'\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_max_len = paras['tgt_max_len']\n",
    "max_src_len_dataloader = paras['max_src_len_dataloader']\n",
    "max_tgt_len_dataloader = paras['max_tgt_len_dataloader']\n",
    "\n",
    "teacher_forcing_ratio = paras['teacher_forcing_ratio']\n",
    "emb_size = paras['emb_size']\n",
    "en_hidden_size = paras['en_hidden_size']\n",
    "en_num_layers = paras['en_num_layers']\n",
    "en_num_direction = paras['en_num_direction']\n",
    "de_hidden_size = paras['de_hidden_size']\n",
    "de_num_layers = paras['de_num_layers']\n",
    "\n",
    "learning_rate = paras['learning_rate']\n",
    "num_epochs = paras['num_epochs']\n",
    "batch_size = paras['batch_size']\n",
    "rnn_type = paras['rnn_type']\n",
    "attention_type = paras['attention_type']\n",
    "beam_size = paras['beam_size']\n",
    "model_save_info = paras['model_save_info']\n",
    "dropout_rate = paras['dropout_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VocabDataset(train_src_input_index, train_tgt_input_index, \n",
    "                             train_label_symbolindex, train_indicator, train_data, \n",
    "                             max_src_len_dataloader, max_tgt_len_dataloader)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=vocab_collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "val_dataset = VocabDataset(val_src_input_index, val_tgt_input_index, \n",
    "                           val_label_symbolindex, val_indicator, val_data,\n",
    "                           max_src_len_dataloader, max_tgt_len_dataloader)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=vocab_collate_func,\n",
    "                                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tgt_max_len': 130, 'max_src_len_dataloader': 94, 'max_tgt_len_dataloader': 127, 'emb_size': 200, 'en_hidden_size': 128, 'en_num_layers': 2, 'en_num_direction': 2, 'de_hidden_size': 256, 'de_num_layers': 3, 'rnn_type': 'GRU', 'attention_type': 'dot_prod', 'teacher_forcing_ratio': 1, 'learning_rate': 0.001, 'num_epochs': 40, 'batch_size': 64, 'beam_size': 5, 'dropout_rate': 0.0, 'model_save_info': {'model_path': 'nmt_models/model1/', 'epochs_per_save_model': 2, 'model_path_for_resume': None}}\n",
      "dot_prod\n",
      "Encoder:\n",
      "EncoderRNN(\n",
      "  (embedding): Embedding(8776, 200)\n",
      "  (dropout): Dropout(p=0.0)\n",
      "  (gru): GRU(200, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (transform_en_hid): Linear(in_features=512, out_features=768, bias=False)\n",
      ")\n",
      "Decoder:\n",
      "DecoderAtten(\n",
      "  (dropout): Dropout(p=0.0)\n",
      "  (embedding): Embedding(8776, 200)\n",
      "  (gru): GRU(200, 256, num_layers=3, batch_first=True)\n",
      "  (atten): AttentionLayer()\n",
      "  (linear): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (copy_mech): CopyMechanism(\n",
      "    (generate_linear): Linear(in_features=256, out_features=20, bias=True)\n",
      "    (copy_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (LogSoftmax): LogSoftmax()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# make dir for saving models\n",
    "if not os.path.exists(model_save_info['model_path']):\n",
    "    os.makedirs(model_save_info['model_path'])\n",
    "### save model hyperparameters\n",
    "with open(model_save_info['model_path']+'model_params.pkl', 'wb') as f:\n",
    "    model_hyparams = paras\n",
    "    pickle.dump(model_hyparams, f)\n",
    "print(model_hyparams)\n",
    "\n",
    "# read all data\n",
    "### save srcLang and tgtLang\n",
    "\n",
    "#for src; keep original src_org and index based on vocab src_tensor\n",
    "\n",
    "#for tgt; vocab_pred_label, copy_label\n",
    "\n",
    "\n",
    "# test_dataset = VocabDataset(test_data)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "#                                            batch_size=BATCH_SIZE,\n",
    "#                                            collate_fn=vocab_collate_func,\n",
    "#                                            shuffle=False)\n",
    "\n",
    "# embedding_src_weight = torch.from_numpy(srcLang.embedding_matrix).type(torch.FloatTensor).to(device)\n",
    "# embedding_tgt_weight = torch.from_numpy(tgtLang.embedding_matrix).type(torch.FloatTensor).to(device)\n",
    "# print(embedding_src_weight.size(), embedding_tgt_weight.size())\n",
    "\n",
    "encoder = EncoderRNN(trainLang.vocab_size, emb_size, en_hidden_size, en_num_layers, \n",
    "                     en_num_direction, (de_num_layers, de_hidden_size), rnn_type=rnn_type, \n",
    "                     dropout_rate=dropout_rate)\n",
    "decoder = DecoderAtten(trainLang.vocab_size, vocab_pred_size, emb_size, de_hidden_size, \n",
    "                       de_num_layers, (en_num_layers, en_num_direction, en_hidden_size), \n",
    "                       rnn_type=rnn_type, atten_type=attention_type, \n",
    "                       dropout_rate=dropout_rate)\n",
    "\n",
    "encoder, decoder = encoder.to(device), decoder.to(device)\n",
    "print('Encoder:')\n",
    "print(encoder)\n",
    "print('Decoder:')\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: [0/40], step: [446/447], train_loss:1.713569215613457, val_precision: 0.20918120646034197, val_recall: 0.20564652405446393, val_loss: 0\n",
      "epoch: [1/40], step: [446/447], train_loss:0.688048206786448, val_precision: 0.281360057013545, val_recall: 0.2913922229271028, val_loss: 0\n",
      "epoch: [2/40], step: [446/447], train_loss:0.5351252585849506, val_precision: 0.3015026361211625, val_recall: 0.32026617848378997, val_loss: 0\n",
      "epoch: [3/40], step: [446/447], train_loss:0.46199089588734926, val_precision: 0.34572786894711416, val_recall: 0.333882645155313, val_loss: 0\n",
      "epoch: [4/40], step: [446/447], train_loss:0.4080293056548842, val_precision: 0.356959809418558, val_recall: 0.35049893289560013, val_loss: 0\n",
      "epoch: [5/40], step: [446/447], train_loss:0.3710157597891703, val_precision: 0.35694701112933425, val_recall: 0.35594693521911297, val_loss: 0\n",
      "epoch: [6/40], step: [446/447], train_loss:0.33719327822494294, val_precision: 0.3583590864535443, val_recall: 0.35972051601033794, val_loss: 0\n",
      "epoch: [7/40], step: [446/447], train_loss:0.310761472722828, val_precision: 0.34411943635518205, val_recall: 0.34846547016127144, val_loss: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-9431065f0299>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m trainIters(train_loader, val_loader, encoder, decoder, num_epochs, learning_rate, \n\u001b[0;32m----> 2\u001b[0;31m            teacher_forcing_ratio, model_save_info, tgt_max_len, beam_size, trainLang)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-105-b083bfe976dc>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(train_loader, val_loader, encoder, decoder, num_epochs, learning_rate, teacher_forcing_ratio, model_save_info, tgt_max_len, beam_size, vocab)\u001b[0m\n\u001b[1;32m    136\u001b[0m                \u001b[0;31m#     print(p[0], ': ',  p[1].grad.data.abs().mean().item(), p[1].grad.data.abs().max().item(), p[1].data.abs().mean().item(), p[1].data.abs().max().item(), end=' ')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                \u001b[0;31m# print('\\n')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_org\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_org\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_facts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_max_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_org\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         print('epoch: [{}/{}], step: [{}/{}], train_loss:{}, val_precision: {}, val_recall: {}, val_loss: {}'.format(\n",
      "\u001b[0;32m/scratch/tx443/NLU/project/NLU_OIE_UnifiedModels/Machine_Translation_NLP/evaluation.py\u001b[0m in \u001b[0;36mpredict_facts\u001b[0;34m(loader, encoder, decoder, tgt_max_length, vocab)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitHidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_true_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_cell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSOS_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitHidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/tx443/NLU/project/NLU_OIE_UnifiedModels/Machine_Translation_NLP/Multilayers_Encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden, lengths, cell)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'GRU'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mrnn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mrnn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hx, batch_sizes)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mbatch_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbidirectional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvariable_length\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             dropout_ts)\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainIters(train_loader, val_loader, encoder, decoder, num_epochs, learning_rate, \n",
    "           teacher_forcing_ratio, model_save_info, tgt_max_len, beam_size, trainLang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainIters(train_loader, val_loader, encoder, decoder, num_epochs, learning_rate, \n",
    "#            teacher_forcing_ratio, model_save_info, tgt_max_len, beam_size, trainLang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = val_loader\n",
    "tgt_max_length = tgt_max_len\n",
    "loss, src_org, tgt_org, tgt_pred = predict_facts(loader, encoder, decoder, tgt_max_length, trainLang)\n",
    "precision, recall = evaluate_prediction(tgt_org, tgt_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.352119460166 0.354060595688\n"
     ]
    }
   ],
   "source": [
    "print(precision.mean(), recall.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset1 = VocabDataset(val_src_input_index, val_tgt_input_index, \n",
    "                             val_label_symbolindex, val_indicator, val_data)\n",
    "val_loader1 = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                               batch_size=2,\n",
    "                                               collate_fn=vocab_collate_func,\n",
    "                                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(fact1, fact2):\n",
    "    elem1 = fact1.split('@')\n",
    "    elem2 = fact2.split('@')\n",
    "    n1 = len(elem1)\n",
    "    n2 = len(elem2)\n",
    "    sim = 0\n",
    "    for i in range(min(n1,n2)):\n",
    "        sim += difflib.SequenceMatcher(None,elem1[i],elem2[i]).ratio()\n",
    "    return sim/max(n1,n2)\n",
    "\n",
    "def check_fact_same(org_fact, pred_fact):\n",
    "    org_fact_ele = org_fact.split('@')\n",
    "    pred_fact_ele = pred_fact.split('@')\n",
    "    if len(org_fact_ele) == len(pred_fact_ele):\n",
    "        ele_num = len(org_fact_ele)\n",
    "        if difflib.SequenceMatcher(None,org_fact,pred_fact).ratio() > 0.85:\n",
    "            return True       \n",
    "        ele_sim = np.zeros((ele_num,))\n",
    "        for ele_i in range(ele_num):\n",
    "            ele_sim[ele_i] = difflib.SequenceMatcher(None,org_fact_ele[ele_i],pred_fact_ele[ele_i]).ratio()\n",
    "        if ele_sim.mean() > 0.85:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'difflib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-c8e3af94c446>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0morg_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morg_facts_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpred_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_facts_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0msimilarity_ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morg_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarity_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morg_facts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morg_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_facts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0mrow_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_sum_assignment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msimilarity_ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-c8e3af94c446>\u001b[0m in \u001b[0;36msimilarity_score\u001b[0;34m(fact1, fact2)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0msim\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdifflib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequenceMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0melem1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0melem2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'difflib' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "tgt_max_length = 130\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    \n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "tgt_pred = []\n",
    "src_org = []\n",
    "tgt_org = []\n",
    "loss = 0\n",
    "loader = val_loader\n",
    "\n",
    "for src_tensor, src_true_len, tgt_tensor, tgt_true_len, tgt_label_vocab, tgt_label_copy, src_org_batch, tgt_org_batch in loader:\n",
    "    batch_size = src_tensor.size(0)\n",
    "    encoder_hidden, encoder_cell = encoder.initHidden(batch_size)\n",
    "    encoder_outputs, encoder_hidden, encoder_cell = encoder(src_tensor, encoder_hidden, src_true_len, encoder_cell)\n",
    "    decoder_input = torch.tensor([SOS_index]*batch_size, device=device).unsqueeze(1)\n",
    "    decoder_hidden, decoder_cell = encoder_hidden, decoder.initHidden(batch_size)\n",
    "\n",
    "    decoding_token_index = 0\n",
    "    stop_flag = [False]*batch_size\n",
    "    step_log_likelihoods = []\n",
    "    tgt_pred_batch = [[] for i_batch in range(batch_size)]\n",
    "    tgt_true_len_max = tgt_true_len.cpu().numpy().max()\n",
    "    while decoding_token_index < tgt_max_length:\n",
    "        decoder_output, decoder_hidden, _, decoder_cell = decoder(decoder_input, decoder_hidden, src_true_len, encoder_outputs, decoder_cell)\n",
    "\n",
    "        # compute loss \n",
    "        if decoding_token_index < tgt_true_len_max:\n",
    "            decoding_label_vocab = tgt_label_vocab[:, decoding_token_index]\n",
    "            decoding_label_copy = tgt_label_copy[:, decoding_token_index, :]\n",
    "            copy_log_probs = decoder_output[:, vocab_pred_size:]+(decoding_label_copy.float()+1e-45).log()\n",
    "            #mask sample which is copied only\n",
    "            gen_mask = ((decoding_label_vocab!=OOV_pred_index) | (decoding_label_copy.sum(-1)==0)).float() \n",
    "            log_gen_mask = (gen_mask + 1e-45).log().unsqueeze(-1)\n",
    "            #mask log_prob value for oov_pred_index when label_vocab==oov_pred_index and is copied \n",
    "            generation_log_probs = decoder_output.gather(1, decoding_label_vocab.unsqueeze(1)) + log_gen_mask\n",
    "            combined_gen_and_copy = torch.cat((generation_log_probs, copy_log_probs), dim=-1)\n",
    "            step_log_likelihood = torch.logsumexp(combined_gen_and_copy, dim=-1)\n",
    "            step_log_likelihoods.append(step_log_likelihood.unsqueeze(1))\n",
    "\n",
    "        #\n",
    "        topv, topi = decoder_output.topk(1, dim=-1)\n",
    "        next_input = topi.detach().cpu().squeeze(1)\n",
    "        decoder_input = []\n",
    "        for i_batch in range(batch_size):\n",
    "            pred_list = vocab_pred+src_org_batch[i_batch]\n",
    "            next_input_token = pred_list[next_input[i_batch].item()]\n",
    "            if next_input_token == vocab_pred[EOS_pred_index]:\n",
    "                stop_flag[i_batch] = True\n",
    "            if not stop_flag[i_batch]:\n",
    "                tgt_pred_batch[i_batch].append(next_input_token)\n",
    "            decoder_input.append(trainLang.word2index.get(next_input_token, UNK_index))\n",
    "        decoder_input = torch.tensor(decoder_input, device=device).unsqueeze(1)\n",
    "        decoding_token_index += 1\n",
    "        if all(stop_flag):\n",
    "            break\n",
    "    log_likelihoods = torch.cat(step_log_likelihoods, dim=-1)\n",
    "    # mask padding for tgt\n",
    "    tgt_pad_mask = sequence_mask(tgt_true_len).float()\n",
    "    log_likelihoods = log_likelihoods*tgt_pad_mask[:,:log_likelihoods.size(1)]\n",
    "    loss += -(log_likelihoods.sum()/tgt_pad_mask.sum()).item()\n",
    "    tgt_pred.extend(tgt_pred_batch)\n",
    "    src_org.extend(src_org_batch)\n",
    "    tgt_org.extend(tgt_org_batch)\n",
    "loss = loss/len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "from Multilayers_Decoder import sequence_mask\n",
    "\n",
    "eval_len = len(tgt_pred)\n",
    "precision = np.zeros((eval_len,))\n",
    "recall = np.zeros((eval_len,))\n",
    "for i in range(eval_len):\n",
    "    org_facts = ''.join(tgt_org[i]).split('$')\n",
    "    pred_facts = ''.join(tgt_pred[i]).split('$')\n",
    "    org_facts_num = len(org_facts)\n",
    "    pred_facts_num = len(pred_facts)\n",
    "    org_match_num = np.zeros((org_facts_num))\n",
    "    pred_match_num = np.zeros((pred_facts_num))\n",
    "    similarity_ma = np.zeros((org_facts_num, pred_facts_num))\n",
    "    for org_i in range(org_facts_num):\n",
    "        for pred_i in range(pred_facts_num):\n",
    "            similarity_ma[org_i, pred_i] = similarity_score(org_facts[org_i], pred_facts[pred_i])\n",
    "    row_ind, col_ind = linear_sum_assignment(-similarity_ma)\n",
    "    \n",
    "    for org_i, pred_i in zip(row_ind, col_ind):\n",
    "        org_fact = org_facts[org_i]\n",
    "        pred_fact = pred_facts[pred_i]\n",
    "        fact_same = check_fact_same(org_fact, pred_fact)\n",
    "        if fact_same:\n",
    "            org_match_num[org_i] = 1\n",
    "            pred_match_num[pred_i] = 1\n",
    "#     print(pred_match_num)\n",
    "#     print(org_match_num)\n",
    "    precision[i] = pred_match_num.mean()\n",
    "    recall[i] = org_match_num.mean()\n",
    "if False:\n",
    "    random_sample = np.random.randint(eval_len)\n",
    "    print('src: ', src_org[random_sample])\n",
    "    print('Ref: ', tgt_org[random_sample])\n",
    "    print('pred: ', tgt_pred[random_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.359069622131\n"
     ]
    }
   ],
   "source": [
    "print(precision.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.342696988792\n"
     ]
    }
   ],
   "source": [
    "print(recall.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision1 = precision\n",
    "recall1 = recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
