{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('/scratch/tx443/NLU/project/NLU_OIE_UnifiedModels/Machine_Translation_NLP')\n",
    "# # print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import jieba\n",
    "import re\n",
    "\n",
    "from config import vocab_pred, vocab_pred_size, vocab_prefix\n",
    "from config import UNK_index, PAD_index, SOS_index, EOS_index \n",
    "from config import OOV_pred_index, PAD_pred_index, EOS_pred_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceMisspred(predicate):\n",
    "    '''replace missing predicate\n",
    "    '''\n",
    "    if predicate == '_':\n",
    "        return 'P'\n",
    "    else:\n",
    "        return predicate\n",
    "    \n",
    "def character_segmentation(string):\n",
    "    res = []\n",
    "    for part in list(jieba.cut(string, cut_all=False)):\n",
    "        if re.match('^[\\da-zA-Z]+$', part):\n",
    "            res.append(part)\n",
    "        else:\n",
    "            res.extend(list(part))\n",
    "    return res\n",
    "\n",
    "# def replaceMissinfo(aaa):\n",
    "#     '''replace missing info for subjects/objects\n",
    "#     '''\n",
    "#     placeholder = ['Z','Y','X']\n",
    "#     for i in range(len(aaa)):\n",
    "#         if aaa[i] == '_':\n",
    "#             aaa = aaa[:i] + placeholder.pop() + aaa[i+1:]\n",
    "#     return aaa\n",
    "\n",
    "def load_preprocess_data(data_add):\n",
    "    saoke = []\n",
    "    with open(data_add, 'r') as f:\n",
    "        for line in f:\n",
    "            saoke.append(json.loads(line))\n",
    "    data = []\n",
    "    # list of dict\n",
    "    for sample in saoke:\n",
    "        # remove some exceptions with empty facts\n",
    "        if sample['logic'] == []:\n",
    "            continue\n",
    "        # tokenize src sentence\n",
    "        sample_processed = dict()\n",
    "        sample_processed['src_org'] = sample['natural']\n",
    "        #sample_processed['src'] = list(jieba.cut(sample['natural'], cut_all=False))\n",
    "        sample_processed['src'] = character_segmentation(sample['natural'])\n",
    "        \n",
    "        # transform fact list into str and tokenize\n",
    "        # $ separates facts; @ separate elements for one fact; & separate objects for one fact\n",
    "        sample_processed['tgt_org'] = sample['logic']\n",
    "        logic_list = []\n",
    "        logic_set = set()\n",
    "        for fact in sample['logic']:\n",
    "            fact = '@'.join([fact['subject'], replaceMisspred(fact['predicate']), '&'.join(fact['object'])])\n",
    "            if not fact in logic_set:\n",
    "                logic_set.add(fact)\n",
    "                logic_list.append(fact)\n",
    "        sample_processed['tgt_list'] = logic_list #remove duplicates\n",
    "        logic_str = '$'.join(logic_list)\n",
    "        sample_processed['tgt'] = character_segmentation(logic_str)\n",
    "        \n",
    "        data.append(sample_processed)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import dropwhile\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name, emb_pretrained_add=None, max_vocab_size=None):\n",
    "        self.name = name\n",
    "        self.word2index = None #{\"$PAD$\": PAD_token, \"$SOS$\": SOS_token, \"$EOS$\": EOS_token, \"$UNK$\": UNK_token}\n",
    "        #self.word2count = None #{\"$PAD$\": 0, \"$SOS$\" : 0, \"$EOS$\": 0, \"$UNK$\": 0}\n",
    "        self.index2word = None #{PAD_token: \"$PAD$\", SOS_token: \"$SOS$\", EOS_token: \"$EOS$\", UNK_token: \"$UNK$\"}\n",
    "        self.max_vocab_size = max_vocab_size  # Count SOS and EOS\n",
    "        self.vocab_size = None\n",
    "        self.emb_pretrained_add = emb_pretrained_add\n",
    "        self.embedding_matrix = None\n",
    "\n",
    "    def build_vocab(self, data):\n",
    "        all_tokens = []\n",
    "        for sample in data:\n",
    "            all_tokens.extend(sample['src'])\n",
    "            all_tokens.extend(sample['tgt'])  \n",
    "        token_counter = Counter(all_tokens)\n",
    "        print('The number of unique tokens totally in dataset: ', len(token_counter))\n",
    "        # remove word with freq==1 \n",
    "        for key, count in dropwhile(lambda key_count: key_count[1] > 1, token_counter.most_common()):\n",
    "            del token_counter[key]\n",
    "        \n",
    "        if self.max_vocab_size:\n",
    "            vocab, count = zip(*token_counter.most_common(self.max_vocab_size))\n",
    "        else:\n",
    "            vocab, count = zip(*token_counter.most_common())\n",
    "        \n",
    "        self.index2word = vocab_prefix + list(vocab)\n",
    "        word2index = dict(zip(self.index2word, range(0, len(self.index2word)))) \n",
    "#         word2index = dict(zip(vocab, range(len(vocab_prefix),len(vocab_prefix)+len(vocab)))) \n",
    "#         for idx, token in enumerate(vocab_prefix):\n",
    "#             word2index[token] = idx\n",
    "        self.word2index = word2index\n",
    "        self.vocab_size = len(self.index2word)\n",
    "        return None \n",
    "\n",
    "    def build_emb_weight(self):\n",
    "        words_emb_dict = load_emb_vectors(self.emb_pretrained_add)\n",
    "        emb_weight = np.zeros([self.vocab_size, 300])\n",
    "        for i in range(len(vocab_prefix), self.vocab_size):\n",
    "            emb = words_emb_dict.get(self.index2word[i], None)\n",
    "            if emb is not None:\n",
    "                try:\n",
    "                    emb_weight[i] = emb\n",
    "                except:\n",
    "                    pass\n",
    "                    #print(len(emb), self.index2word[i], emb)\n",
    "        self.embedding_matrix = emb_weight\n",
    "        return None\n",
    "\n",
    "def load_emb_vectors(fasttest_home):\n",
    "    max_num_load = 500000\n",
    "    words_dict = {}\n",
    "    with open(fasttest_home) as f:\n",
    "        for num_row, line in enumerate(f):\n",
    "            if num_row >= max_num_load:\n",
    "                break\n",
    "            s = line.split()\n",
    "            words_dict[s[0]] = np.asarray(s[1:])\n",
    "    return words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2index(data, key, word2index):\n",
    "    '''\n",
    "    transform tokens into index as input for both src and tgt\n",
    "    '''\n",
    "    indexdata = []\n",
    "    for line in data:\n",
    "        line = line[key]\n",
    "        indexdata.append([word2index[c] if c in word2index.keys() else UNK_index for c in line])\n",
    "        #indexdata[-1].append(EOS_index)\n",
    "    print('finish indexing')\n",
    "    return indexdata\n",
    "\n",
    "def construct_Lang(name, data, emb_pretrained_add = None, max_vocab_size = None):\n",
    "    lang = Lang(name, emb_pretrained_add, max_vocab_size)\n",
    "    lang.build_vocab(data)\n",
    "    if emb_pretrained_add:\n",
    "        lang.build_emb_weight()\n",
    "    return lang\n",
    "\n",
    "def text2symbolindex(data, key, word2index):\n",
    "    '''get generation label for tgt \n",
    "    '''\n",
    "    indexdata = []\n",
    "    for line in data:\n",
    "        line = line[key]\n",
    "        indexdata.append([word2index[c] if c in word2index.keys() else OOV_pred_index for c in line])\n",
    "        #indexdata[-1].append(EOS_index)\n",
    "    print('symbol label finish')\n",
    "    return indexdata\n",
    "\n",
    "def copy_indicator(data, src_key='src', tgt_key='tgt'):\n",
    "    '''get copy label for tgt\n",
    "    '''\n",
    "    indicator = []\n",
    "    for sample in data:\n",
    "        tgt = sample[tgt_key]\n",
    "        src = sample[src_key]\n",
    "        matrix = np.zeros((len(tgt), len(src)), dtype=int)\n",
    "        for m in range(len(tgt)):\n",
    "            for n in range(len(src)):\n",
    "                if tgt[m] == src[n]:\n",
    "                    matrix[m,n] = 1\n",
    "        indicator.append(matrix)\n",
    "    return indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from itertools import dropwhile\n",
    "\n",
    "class VocabDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, src_index, tgt_index, tgt_symbolindex, tgt_indicator, data, src_clip=None, tgt_clip=None):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.src_clip = src_clip\n",
    "        self.tgt_clip = tgt_clip\n",
    "        self.src_list, self.tgt_list = src_index, tgt_index\n",
    "        self.data = data\n",
    "        self.tgt_symbolindex, self.tgt_indicator  = tgt_symbolindex, tgt_indicator\n",
    "        \n",
    "        assert (len(self.src_list) == len(self.tgt_list) == len(self.tgt_symbolindex)== len(self.tgt_indicator))\n",
    "        #self.word2index = word2index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        src = self.src_list[key]\n",
    "        tgt = self.tgt_list[key]\n",
    "        src_org = self.data[key]['src']\n",
    "        tgt_org = self.data[key]['tgt']\n",
    "        tgt_sym = self.tgt_symbolindex[key]\n",
    "        tgt_ind = self.tgt_indicator[key]\n",
    "        \n",
    "        if self.src_clip is not None:\n",
    "            src = src[:self.src_clip]\n",
    "            src_org = src_org[:self.src_clip]\n",
    "            tgt_ind = tgt_ind[:,:self.src_clip]\n",
    "        src_length = len(src)\n",
    "\n",
    "        if self.tgt_clip is not None:\n",
    "            tgt = tgt[:self.tgt_clip]\n",
    "            tgt_org = tgt_org[:self.tgt_clip]\n",
    "            tgt_sym = tgt_sym[:self.tgt_clip]\n",
    "            tgt_ind = tgt_ind[:self.tgt_clip,:]\n",
    "        tgt_length = len(tgt)\n",
    "        \n",
    "        return src, src_length, tgt, tgt_length, tgt_sym, tgt_ind, src_org, tgt_org\n",
    "        \n",
    "        #return src_org, src_tensor, src_true_len, tgt_org, tgt_tensor, tgt_true_len, tgt_label_vocab, tgt_label_copy \n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    src_list = []\n",
    "    tgt_list = []\n",
    "    src_length_list = []\n",
    "    tgt_length_list = []\n",
    "    tgt_symbol_list = []\n",
    "    tgt_indicator_list = []\n",
    "    src_org_list = []\n",
    "    tgt_org_list = []\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for datum in batch:\n",
    "        src_length_list.append(datum[1]) # 不用加1；eos不算\n",
    "        tgt_length_list.append(datum[3]+1) \n",
    "    \n",
    "    batch_max_src_length = np.max(src_length_list)\n",
    "    batch_max_tgt_length = np.max(tgt_length_list)\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        #+[EOS_index] -1\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0, batch_max_src_length-datum[1])),\n",
    "                                mode=\"constant\", constant_values=PAD_index)\n",
    "        src_list.append(padded_vec)\n",
    "        \n",
    "        padded_vec = np.pad(np.array(datum[2]+[EOS_index]),\n",
    "                                pad_width=((0, batch_max_tgt_length-datum[3]-1)),\n",
    "                                mode=\"constant\", constant_values=PAD_index)\n",
    "        tgt_list.append(padded_vec)\n",
    "        \n",
    "        padded_vec = np.pad(np.array(datum[4]+[EOS_pred_index]),\n",
    "                                pad_width=((0, batch_max_tgt_length-datum[3]-1)),\n",
    "                                mode=\"constant\", constant_values=PAD_pred_index)\n",
    "        tgt_symbol_list.append(padded_vec)\n",
    "        \n",
    "        indicator = np.pad(datum[5], pad_width=((0,1),(0,0)), \n",
    "                           mode='constant', constant_values=0)\n",
    "        #indicator[-1,-1] = 1  -1\n",
    "        padded_vec = np.pad(indicator,\n",
    "                            pad_width=((0, batch_max_tgt_length-datum[3]-1),((0, batch_max_src_length-datum[1]))),\n",
    "                            mode=\"constant\", constant_values=0)\n",
    "        #print(padded_vec.dtype, padded_vec.shape)\n",
    "        tgt_indicator_list.append(padded_vec)\n",
    "        \n",
    "        src_org_list.append(datum[6])\n",
    "        tgt_org_list.append(datum[7])\n",
    "    \n",
    "    # re-order\n",
    "    ind_dec_order = np.argsort(src_length_list)[::-1]\n",
    "    \n",
    "    src_list = np.array(src_list)[ind_dec_order]\n",
    "    src_length_list = np.array(src_length_list)[ind_dec_order]\n",
    "    tgt_list = np.array(tgt_list)[ind_dec_order]\n",
    "    tgt_length_list = np.array(tgt_length_list)[ind_dec_order]\n",
    "    tgt_symbol_list = np.array(tgt_symbol_list)[ind_dec_order]\n",
    "    #print(tgt_indicator_list[0].dtype, tgt_indicator_list[0][:5][:5])\n",
    "    tgt_indicator_list = np.array(tgt_indicator_list)[ind_dec_order]\n",
    "    #print(tgt_indicator_list.dtype, tgt_indicator_list.shape)\n",
    "    src_org_list = [src_org_list[i] for i in ind_dec_order]\n",
    "    tgt_org_list = [tgt_org_list[i] for i in ind_dec_order]\n",
    "    \n",
    "    #print(type(np.array(data_list)),type(np.array(label_list)))\n",
    "    \n",
    "    return [torch.from_numpy(src_list).to(device), \n",
    "            torch.LongTensor(src_length_list).to(device), \n",
    "            torch.from_numpy(tgt_list).to(device), \n",
    "            torch.LongTensor(tgt_length_list).to(device),\n",
    "            torch.from_numpy(tgt_symbol_list).to(device),\n",
    "            torch.from_numpy(tgt_indicator_list).to(device),\n",
    "            src_org_list,\n",
    "            tgt_org_list,           \n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /state/partition1/job-844128/jieba.cache\n",
      "Loading model cost 0.767 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data_add = '/scratch/tx443/NLU/project/SAOKE_DATA.json'\n",
    "data = load_preprocess_data(data_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train val test\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_train_data = sorted(train_data, key=lambda x: len(x['tgt']), reverse=False)\n",
    "# train_data = sorted_train_data[0:3000]\n",
    "\n",
    "# sorted_val_data = sorted(val_data, key=lambda x: len(x['tgt']), reverse=False)\n",
    "# val_data = sorted_val_data[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique tokens totally in dataset:  9364\n"
     ]
    }
   ],
   "source": [
    "# build vocab from train for input indexing\n",
    "trainLang = construct_Lang('train', train_data)\n",
    "\n",
    "# build generation vocab for prediction\n",
    "word2symbolindex = {}\n",
    "for idx, token in enumerate(vocab_pred):\n",
    "        word2symbolindex[token] = idx\n",
    "\n",
    "# check\n",
    "assert(UNK_index==trainLang.word2index['<UNK>'])\n",
    "assert(PAD_index==trainLang.word2index['<PAD>'])\n",
    "assert(SOS_index==trainLang.word2index['<SOS>'])\n",
    "assert(EOS_index==trainLang.word2index['<EOS>'])\n",
    "\n",
    "assert(OOV_pred_index==word2symbolindex['<OOV>'])\n",
    "assert(PAD_pred_index==word2symbolindex['<PAD>'])\n",
    "assert(EOS_pred_index==word2symbolindex['<EOS>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permute facts at this place; data['tgt']\n",
    "def identity(facts_list):\n",
    "    return facts_list\n",
    "\n",
    "def reverse(facts_list):\n",
    "    return facts_list[::-1]\n",
    "\n",
    "def random_pm(facts_list):\n",
    "    random_order = np.random.permutation(len(facts_list))\n",
    "    return [facts_list[idx] for idx in random_order]\n",
    "\n",
    "def last3_pm(facts_list):\n",
    "    if len(facts_list) < 4:\n",
    "        return facts_list\n",
    "    else:\n",
    "        return facts_list[-3:]+facts_list[:-3]\n",
    "\n",
    "def permute_factOrder_tgt(data, pm_fn):\n",
    "    data_len = len(data)\n",
    "    for i in range(data_len):\n",
    "        facts_list = data[i]['tgt_list']\n",
    "        facts_list_pm = pm_fn(facts_list)\n",
    "        data[i]['tgt'] = character_segmentation('$'.join(facts_list_pm))\n",
    "    return None\n",
    "\n",
    "# permute_factOrder_tgt(train_data, last3_pm)\n",
    "# train_len = len(train_data)\n",
    "# for i in range(train_len):\n",
    "#     facts_list = train_data[i]['tgt_list']\n",
    "#     facts_list_pm = facts_list[::-1]\n",
    "#     train_data[i]['tgt'] = character_segmentation('$'.join(facts_list_pm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish indexing\n",
      "finish indexing\n",
      "0.6389546394348145\n"
     ]
    }
   ],
   "source": [
    "# input indexing for src\n",
    "start_time = time.time()\n",
    "train_src_input_index = text2index(train_data, 'src', trainLang.word2index) \n",
    "val_src_input_index = text2index(val_data, 'src', trainLang.word2index) \n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish indexing\n",
      "finish indexing\n"
     ]
    }
   ],
   "source": [
    "# input indexing for tgt\n",
    "train_tgt_input_index = text2index(train_data, 'tgt', trainLang.word2index) \n",
    "val_tgt_input_index = text2index(val_data, 'tgt', trainLang.word2index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symbol label finish\n",
      "symbol label finish\n"
     ]
    }
   ],
   "source": [
    "# get generation label\n",
    "train_label_symbolindex = text2symbolindex(train_data, 'tgt', word2symbolindex)\n",
    "val_label_symbolindex = text2symbolindex(val_data, 'tgt', word2symbolindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.269481182098389\n"
     ]
    }
   ],
   "source": [
    "# get copy label\n",
    "start_time = time.time()\n",
    "train_indicator = copy_indicator(train_data, 'src', 'tgt')\n",
    "val_indicator = copy_indicator(val_data, 'src', 'tgt')\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28564, 28564, 28564, 28564, 28564)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_src_input_index),len(train_tgt_input_index),len(train_label_symbolindex),len(train_indicator),len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6121, 6121, 6121, 6121, 6121)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_src_input_index),len(val_tgt_input_index),len(val_label_symbolindex),len(val_indicator),len(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "# from Data_utils import VocabDataset, vocab_collate_func\n",
    "# from preprocessing_util import preposs_toekn, Lang, text2index, construct_Lang\n",
    "from config import device, embedding_freeze\n",
    "import random\n",
    "from evaluation import similarity_score, check_fact_same, predict_facts, evaluate_prediction\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bridge(context):\n",
    "    return State(context=context, batch_first=True)\n",
    "\n",
    "def train(src_data, tgt_data, encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
    "          teacher_forcing_ratio, vocab):\n",
    "    src_org_batch, src_tensor, src_true_len = src_data\n",
    "    tgt_org_batch, tgt_tensor, tgt_label_vocab, tgt_label_copy, tgt_true_len = tgt_data\n",
    "    '''\n",
    "    finish train for a batch\n",
    "    ''' \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    batch_size = src_tensor.size(0)\n",
    "    encoder_context = encoder(src_tensor)\n",
    "    state = bridge(encoder_context)\n",
    "    \n",
    "    decoder_input = torch.tensor([SOS_index]*batch_size, device=device).unsqueeze(1)\n",
    "    step_log_likelihoods = []\n",
    "    #print(decoder_hidden.size())\n",
    "    #print('encoddddddddddder finishhhhhhhhhhhhhhh')\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        ### Teacher forcing: Feed the target as the next input\n",
    "        decoding_token_index = 0\n",
    "        tgt_max_len_batch = tgt_true_len.cpu().max().item()\n",
    "        assert(tgt_max_len_batch==tgt_tensor.size(1))\n",
    "        while decoding_token_index < tgt_max_len_batch:\n",
    "            decoder_output, _ = decoder(decoder_input, state) # state update at each step\n",
    "            #decoder_output = decoder_output.squeeze(1)\n",
    "\n",
    "            decoding_label_vocab = tgt_label_vocab[:, decoding_token_index]\n",
    "            decoding_label_copy = tgt_label_copy[:, decoding_token_index, :]\n",
    "            copy_log_probs = decoder_output[:, vocab_pred_size:]+(decoding_label_copy.float()+1e-45).log()\n",
    "            #mask sample which is copied only\n",
    "            gen_mask = ((decoding_label_vocab!=OOV_pred_index) | (decoding_label_copy.sum(-1)==0)).float() \n",
    "            log_gen_mask = (gen_mask + 1e-45).log().unsqueeze(-1)\n",
    "            #mask log_prob value for oov_pred_index when label_vocab==oov_pred_index and is copied \n",
    "            generation_log_probs = decoder_output.gather(1, decoding_label_vocab.unsqueeze(1)) + log_gen_mask\n",
    "            combined_gen_and_copy = torch.cat((generation_log_probs, copy_log_probs), dim=-1)\n",
    "            step_log_likelihood = torch.logsumexp(combined_gen_and_copy, dim=-1)\n",
    "            step_log_likelihoods.append(step_log_likelihood.unsqueeze(1))\n",
    "            #loss += criterion(decoder_output, tgt_tensor[:,decoding_token_index])\n",
    "            decoder_input = tgt_tensor[:,decoding_token_index].unsqueeze(1)  # Teacher forcing\n",
    "            decoding_token_index += 1\n",
    "\n",
    "    else:\n",
    "        ### Without teacher forcing: use its own predictions as the next input\n",
    "        decoding_token_index = 0\n",
    "        tgt_max_len_batch = tgt_true_len.cpu().max().item()\n",
    "        assert(tgt_max_len_batch==tgt_tensor.size(1))\n",
    "        while decoding_token_index < tgt_max_len_batch:\n",
    "            decoder_output, _ = decoder(decoder_input, state)\n",
    "            #decoder_output = decoder_output.squeeze(1)\n",
    "            \n",
    "            decoding_label_vocab = tgt_label_vocab[:, decoding_token_index]\n",
    "            decoding_label_copy = tgt_label_copy[:, decoding_token_index, :]\n",
    "            copy_log_probs = decoder_output[:, vocab_pred_size:]+(decoding_label_copy.float()+1e-45).log()\n",
    "            #mask sample which is copied only\n",
    "            gen_mask = ((decoding_label_vocab!=OOV_pred_index)|(decoding_label_copy.sum(-1)==0)).float() \n",
    "            log_gen_mask = (gen_mask + 1e-45).log().unsqueeze(-1)\n",
    "            #mask log_prob value for oov_pred_index when label_vocab==oov_pred_index and is copied \n",
    "            generation_log_probs = decoder_output.gather(1, decoding_label_vocab.unsqueeze(1)) + log_gen_mask\n",
    "            combined_gen_and_copy = torch.cat((generation_log_probs, copy_log_probs), dim=-1)\n",
    "            step_log_likelihood = torch.logsumexp(combined_gen_and_copy, dim=-1)\n",
    "            step_log_likelihoods.append(step_log_likelihood.unsqueeze(1))\n",
    "\n",
    "            topv, topi = decoder_output.topk(1, dim=-1)\n",
    "            next_input = topi.detach().cpu().squeeze(1)\n",
    "            decoder_input = []\n",
    "            for i_batch in range(batch_size):\n",
    "                pred_list = vocab_pred+src_org_batch[i_batch]\n",
    "                next_input_token = pred_list[next_input[i_batch].item()]\n",
    "                decoder_input.append(vocab.word2index.get(next_input_token, UNK_index))\n",
    "            decoder_input = torch.tensor(decoder_input, device=device).unsqueeze(1)\n",
    "            decoding_token_index += 1\n",
    "\n",
    "    # average loss\n",
    "    log_likelihoods = torch.cat(step_log_likelihoods, dim=-1)\n",
    "    # mask padding for tgt\n",
    "    tgt_pad_mask = sequence_mask(tgt_true_len).float()\n",
    "    log_likelihoods = log_likelihoods*tgt_pad_mask\n",
    "    loss = -log_likelihoods.sum()/batch_size\n",
    "    loss.backward()\n",
    "\n",
    "    ### TODO\n",
    "    # clip for gradient exploding \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return (loss*batch_size/tgt_pad_mask.sum()).item() #torch.div(loss, tgt_true_len.type_as(loss).mean()).item()  #/tgt_true_len.mean()\n",
    "\n",
    "\n",
    "def trainIters(train_loader, val_loader, encoder, decoder, num_epochs, learning_rate, \n",
    "               teacher_forcing_ratio, tfr_decay_rate, model_save_info, tgt_max_len, \n",
    "               beam_size, vocab):\n",
    "    \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    if model_save_info['model_path_for_resume'] is not None:\n",
    "        check_point_state = torch.load(model_save_info['model_path_for_resume'])\n",
    "        encoder.load_state_dict(check_point_state['encoder_state_dict'])\n",
    "        encoder_optimizer.load_state_dict(check_point_state['encoder_optimizer_state_dict'])\n",
    "        decoder.load_state_dict(check_point_state['decoder_state_dict'])\n",
    "        decoder_optimizer.load_state_dict(check_point_state['decoder_optimizer_state_dict'])\n",
    "\n",
    "    for epoch in range(num_epochs): \n",
    "        start_time = time.time()\n",
    "        n_iter = -1\n",
    "        losses = np.zeros((len(train_loader),))\n",
    "        if tfr_decay_rate is not None:\n",
    "            teacher_forcing_ratio *= tfr_decay_rate\n",
    "        for src_tensor, src_true_len, tgt_tensor, tgt_true_len, tgt_label_vocab, tgt_label_copy, src_org_batch, tgt_org_batch in train_loader:\n",
    "            n_iter += 1\n",
    "            #print('start_step: ', n_iter)\n",
    "            src_data = (src_org_batch, src_tensor, src_true_len)\n",
    "            tgt_data = (tgt_org_batch, tgt_tensor, tgt_label_vocab, tgt_label_copy, tgt_true_len)\n",
    "            loss = train(src_data, tgt_data, encoder, decoder, encoder_optimizer, \n",
    "                         decoder_optimizer, teacher_forcing_ratio, vocab)\n",
    "            losses[n_iter] = loss\n",
    "            if n_iter % 500 == 0:\n",
    "                pass\n",
    "                #print('Loss:', loss)\n",
    "                #eva_start = time.time()\n",
    "#                 precision, recall, val_loss = evaluate_batch(val_loader, encoder, decoder, tgt_max_len, vocab, vocab_pred_size)\n",
    "#                 #print((time.time()-eva_start)/60)\n",
    "#                 print('epoch: [{}/{}], step: [{}/{}], train_loss:{}, val_precision: {}, val_recall: {}, val_loss: {}'.format(\n",
    "#                     epoch, num_epochs, n_iter, len(train_loader), loss, precision.mean(), recall.mean(), val_loss))\n",
    "               # print('Decoder parameters grad:')\n",
    "               # for p in decoder.named_parameters():\n",
    "               #     print(p[0], ': ',  p[1].grad.data.abs().mean().item(), p[1].grad.data.abs().max().item(), p[1].data.abs().mean().item(), p[1].data.abs().max().item(), end=' ')\n",
    "               # print('\\n')\n",
    "               # print('Encoder Parameters grad:')\n",
    "               # for p in encoder.named_parameters():\n",
    "               #     print(p[0], ': ',  p[1].grad.data.abs().mean().item(), p[1].grad.data.abs().max().item(), p[1].data.abs().mean().item(), p[1].data.abs().max().item(), end=' ')\n",
    "               # print('\\n')\n",
    "        val_loss, src_org, tgt_org, tgt_pred = predict_facts(val_loader, encoder, decoder, tgt_max_len, vocab)\n",
    "        precision, recall = evaluate_prediction(tgt_org, tgt_pred)\n",
    "        epoch_train_time = (time.time()-start_time)/60\n",
    "        print_str = 'epoch: [{}/{}]({}m), step: [{}/{}], train_loss:{}, val_precision: {}, val_recall: {}, val_loss: {}'.format(\n",
    "            epoch, num_epochs, epoch_train_time, n_iter, len(train_loader), losses.mean(), precision.mean(), recall.mean(), val_loss)\n",
    "        print_info.append(print_str)\n",
    "        print(print_str)\n",
    "        \n",
    "        if (epoch+1) % model_save_info['epochs_per_save_model'] == 0:\n",
    "            check_point_state = {\n",
    "                'epoch': epoch,\n",
    "                'encoder_state_dict': encoder.state_dict(),\n",
    "                'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                'decoder_optimizer_state_dict': decoder_optimizer.state_dict()\n",
    "                }\n",
    "            torch.save(check_point_state, '{}epoch_{}.pth'.format(model_save_info['model_path'], epoch))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = dict( \n",
    "    tgt_max_len = 130,\n",
    "    max_src_len_dataloader =94,\n",
    "    max_tgt_len_dataloader =127,\n",
    "    \n",
    "    hidden_size=512, \n",
    "    emb_size=300,\n",
    "    num_layers=2, \n",
    "    num_heads=8, \n",
    "    inner_linear=2048,  \n",
    "    prenormalized=False,\n",
    "    dropout=0.0,\n",
    "    layer_norm=True,\n",
    "    weight_norm=False,\n",
    "    stateful=None,\n",
    "    classifier_type='copy', \n",
    "\n",
    "    teacher_forcing_ratio = 1,\n",
    "    tfr_decay_rate = None, #'None means no decay'\n",
    "\n",
    "    learning_rate = 1e-4,\n",
    "    num_epochs = 30,\n",
    "    batch_size = 64, \n",
    "    beam_size = 5,\n",
    "\n",
    "    model_save_info = dict(\n",
    "        model_path = 'nmt_models/T2/round3/',\n",
    "        epochs_per_save_model = 1,\n",
    "        model_path_for_resume = 'nmt_models/T2/round3/epoch_11.pth' #'nmt_models/epoch_0.pth'\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_max_len = paras['tgt_max_len']\n",
    "max_src_len_dataloader = paras['max_src_len_dataloader']\n",
    "max_tgt_len_dataloader = paras['max_tgt_len_dataloader']\n",
    "\n",
    "hidden_size = paras['hidden_size']\n",
    "emb_size = paras['emb_size']\n",
    "num_layers = paras['num_layers']\n",
    "num_heads= paras['num_heads'] \n",
    "inner_linear = paras['inner_linear']  \n",
    "prenormalized = paras['prenormalized']\n",
    "dropout = paras['dropout']\n",
    "layer_norm = paras['layer_norm']\n",
    "weight_norm = paras['weight_norm']\n",
    "stateful = paras['stateful']\n",
    "classifier_type = paras['classifier_type']\n",
    "\n",
    "teacher_forcing_ratio = paras['teacher_forcing_ratio']\n",
    "tfr_decay_rate = paras['tfr_decay_rate']\n",
    "\n",
    "learning_rate = paras['learning_rate']\n",
    "num_epochs = paras['num_epochs']\n",
    "batch_size = paras['batch_size']\n",
    "beam_size = paras['beam_size']\n",
    "model_save_info = paras['model_save_info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VocabDataset(train_src_input_index, train_tgt_input_index, \n",
    "                             train_label_symbolindex, train_indicator, train_data, \n",
    "                             max_src_len_dataloader, max_tgt_len_dataloader)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=vocab_collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "val_dataset = VocabDataset(val_src_input_index, val_tgt_input_index, \n",
    "                           val_label_symbolindex, val_indicator, val_data,\n",
    "                           max_src_len_dataloader, max_tgt_len_dataloader)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=vocab_collate_func,\n",
    "                                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tgt_max_len': 130, 'max_src_len_dataloader': 94, 'max_tgt_len_dataloader': 127, 'hidden_size': 512, 'emb_size': 300, 'num_layers': 2, 'num_heads': 8, 'inner_linear': 2048, 'prenormalized': False, 'dropout': 0.0, 'layer_norm': True, 'weight_norm': False, 'stateful': None, 'classifier_type': 'copy', 'teacher_forcing_ratio': 1, 'tfr_decay_rate': None, 'learning_rate': 0.0001, 'num_epochs': 30, 'batch_size': 64, 'beam_size': 5, 'model_save_info': {'model_path': 'nmt_models/T2/round3/', 'epochs_per_save_model': 1, 'model_path_for_resume': 'nmt_models/T2/round3/epoch_11.pth'}}\n",
      "Encoder:\n",
      "TransformerAttentionEncoder(\n",
      "  (embedder): Embedding(8776, 300, padding_idx=0)\n",
      "  (dropout): Dropout(p=0.0, inplace)\n",
      "  (blocks): ModuleList(\n",
      "    (0): EncoderBlock(\n",
      "      (lnorm1): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      (lnorm2): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.0)\n",
      "      (attention): MultiHeadAttention(\n",
      "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (sdp_attention): SDPAttention(\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "      )\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): Dropout(p=0.0)\n",
      "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): EncoderBlock(\n",
      "      (lnorm1): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      (lnorm2): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.0)\n",
      "      (attention): MultiHeadAttention(\n",
      "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (sdp_attention): SDPAttention(\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "      )\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): Dropout(p=0.0)\n",
      "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Decoder:\n",
      "TransformerAttentionDecoder(\n",
      "  (embedder): Embedding(8776, 300, padding_idx=0)\n",
      "  (dropout): Dropout(p=0.0, inplace)\n",
      "  (blocks): ModuleList(\n",
      "    (0): DecoderBlock(\n",
      "      (lnorm1): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      (lnorm2): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      (lnorm3): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.0)\n",
      "      (attention): MultiHeadAttention(\n",
      "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (sdp_attention): SDPAttention(\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "      )\n",
      "      (masked_attention): MultiHeadAttention(\n",
      "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (sdp_attention): SDPAttention(\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "      )\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): Dropout(p=0.0)\n",
      "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): DecoderBlock(\n",
      "      (lnorm1): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      (lnorm2): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      (lnorm3): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.0)\n",
      "      (attention): MultiHeadAttention(\n",
      "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (sdp_attention): SDPAttention(\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "      )\n",
      "      (masked_attention): MultiHeadAttention(\n",
      "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (sdp_attention): SDPAttention(\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "      )\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): Dropout(p=0.0)\n",
      "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): CopyMechanism(\n",
      "    (generate_linear): Linear(in_features=512, out_features=20, bias=True)\n",
      "    (copy_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (LogSoftmax): LogSoftmax()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# make dir for saving models\n",
    "from seq2seq.models.transformer import TransformerAttentionEncoder, TransformerAttentionDecoder, sequence_mask\n",
    "from seq2seq.models.modules.state import State\n",
    "\n",
    "if not os.path.exists(model_save_info['model_path']):\n",
    "    os.makedirs(model_save_info['model_path'])\n",
    "### save model hyperparameters\n",
    "with open(model_save_info['model_path']+'model_params.pkl', 'wb') as f:\n",
    "    model_hyparams = paras\n",
    "    pickle.dump(model_hyparams, f)\n",
    "print(model_hyparams)\n",
    "\n",
    "encoder = TransformerAttentionEncoder(vocab_size=trainLang.vocab_size, hidden_size=hidden_size,\n",
    "                                      embedding_size=emb_size, num_layers=num_layers,\n",
    "                                      num_heads=num_heads, inner_linear=inner_linear,\n",
    "                                      prenormalized=prenormalized, layer_norm=layer_norm,\n",
    "                                      weight_norm=weight_norm, dropout=dropout\n",
    "                                     )\n",
    "\n",
    "decoder = TransformerAttentionDecoder(vocab_size=trainLang.vocab_size, hidden_size=hidden_size,\n",
    "                                      embedding_size=emb_size, num_layers=num_layers, \n",
    "                                      num_heads=num_heads, dropout=dropout,\n",
    "                                      inner_linear=inner_linear, prenormalized=prenormalized,\n",
    "                                      stateful=stateful, layer_norm=layer_norm,\n",
    "                                      weight_norm=weight_norm,\n",
    "                                      classifier_type=classifier_type\n",
    "                                     )\n",
    "encoder, decoder = encoder.to(device), decoder.to(device)\n",
    "print('Encoder:')\n",
    "print(encoder)\n",
    "print('Decoder:')\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: [0/20](32.38282967805863m), step: [446/447], train_loss:1.9578206757837762, val_precision: 0.026465310724418708, val_recall: 0.029005142328128765, val_loss: 0\n",
      "epoch: [1/20](32.314728510379794m), step: [446/447], train_loss:1.775627501858961, val_precision: 0.03538055774209997, val_recall: 0.03538456542459156, val_loss: 0\n",
      "epoch: [2/20](32.23568295240402m), step: [446/447], train_loss:1.658952917828656, val_precision: 0.041918342620121445, val_recall: 0.0454709062289523, val_loss: 0\n",
      "epoch: [3/20](32.30389209588369m), step: [446/447], train_loss:1.5639776212790402, val_precision: 0.048395390573956164, val_recall: 0.05098580738182111, val_loss: 0\n",
      "epoch: [4/20](32.358332149187724m), step: [446/447], train_loss:1.4770425835445156, val_precision: 0.051291234043235344, val_recall: 0.053251951446690864, val_loss: 0\n",
      "epoch: [5/20](32.289966630935666m), step: [446/447], train_loss:1.4012032376573123, val_precision: 0.0535907221171067, val_recall: 0.05719012422165502, val_loss: 0\n",
      "epoch: [6/20](32.31183933814366m), step: [446/447], train_loss:1.3284853582147518, val_precision: 0.057821319026595944, val_recall: 0.06415164551906914, val_loss: 0\n",
      "epoch: [7/20](32.33260212739309m), step: [446/447], train_loss:1.2597537464743493, val_precision: 0.06348786131909805, val_recall: 0.06391949461709305, val_loss: 0\n",
      "epoch: [8/20](32.356765083471934m), step: [446/447], train_loss:1.1942078139157903, val_precision: 0.06755647237186202, val_recall: 0.06771549485566802, val_loss: 0\n",
      "epoch: [9/20](32.37256229321162m), step: [446/447], train_loss:1.1358988098383482, val_precision: 0.06545271379857232, val_recall: 0.0671871231747069, val_loss: 0\n",
      "epoch: [10/20](32.23488272428513m), step: [446/447], train_loss:1.0756360751137104, val_precision: 0.06553402487073455, val_recall: 0.07119220067267773, val_loss: 0\n",
      "epoch: [11/20](32.21229490439097m), step: [446/447], train_loss:1.0217850616314268, val_precision: 0.06922856183189462, val_recall: 0.06693344276663996, val_loss: 0\n",
      "epoch: [12/20](32.30744541486104m), step: [446/447], train_loss:0.9668519102220301, val_precision: 0.07049323681818372, val_recall: 0.07538003438591577, val_loss: 0\n",
      "epoch: [13/20](32.17654092709223m), step: [446/447], train_loss:0.9208497062358814, val_precision: 0.07256160401888223, val_recall: 0.07139985756224933, val_loss: 0\n",
      "epoch: [14/20](32.27572254737218m), step: [446/447], train_loss:0.870787796424806, val_precision: 0.07034234213208236, val_recall: 0.07200089795662415, val_loss: 0\n",
      "epoch: [15/20](32.25591271320979m), step: [446/447], train_loss:0.8262197085941665, val_precision: 0.07304156933302498, val_recall: 0.07555462789493177, val_loss: 0\n",
      "epoch: [16/20](32.25317574342092m), step: [446/447], train_loss:0.7863373672402145, val_precision: 0.08017708298708134, val_recall: 0.07671061444137739, val_loss: 0\n",
      "epoch: [17/20](32.25108060836792m), step: [446/447], train_loss:0.7417306169300805, val_precision: 0.07421349593788738, val_recall: 0.07787094460840581, val_loss: 0\n",
      "epoch: [18/20](32.26891138553619m), step: [446/447], train_loss:0.7091770385469099, val_precision: 0.07795715466439206, val_recall: 0.07673992356642251, val_loss: 0\n",
      "epoch: [19/20](32.22042087713877m), step: [446/447], train_loss:0.6692324146061669, val_precision: 0.07888185486208685, val_recall: 0.08323519226149514, val_loss: 0\n"
     ]
    }
   ],
   "source": [
    "# learning_rate = 1e-4\n",
    "print_info = []\n",
    "trainIters(train_loader, val_loader, encoder, decoder, num_epochs, learning_rate, \n",
    "               teacher_forcing_ratio, tfr_decay_rate, model_save_info, tgt_max_len, \n",
    "               beam_size, trainLang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: [0/30](32.2570640206337m), step: [446/447], train_loss:0.6397109793069912, val_precision: 0.07990414212845187, val_recall: 0.08063335174509818, val_loss: 0\n",
      "epoch: [1/30](32.25934288104375m), step: [446/447], train_loss:0.5995288051214794, val_precision: 0.07360357607866184, val_recall: 0.07953000728690976, val_loss: 0\n",
      "epoch: [2/30](32.260063874721524m), step: [446/447], train_loss:0.5735238673836326, val_precision: 0.0737824420310942, val_recall: 0.08070097549349305, val_loss: 0\n",
      "epoch: [3/30](32.282189428806305m), step: [446/447], train_loss:0.545445200847566, val_precision: 0.0811959378819369, val_recall: 0.08220211190722544, val_loss: 0\n",
      "epoch: [4/30](32.31865317424138m), step: [446/447], train_loss:0.5276610225905775, val_precision: 0.07864370422367481, val_recall: 0.07758770164831266, val_loss: 0\n",
      "epoch: [5/30](32.33748585383098m), step: [446/447], train_loss:0.5012625121163575, val_precision: 0.08142064510304994, val_recall: 0.08601231584404267, val_loss: 0\n",
      "epoch: [6/30](32.29635155200958m), step: [446/447], train_loss:0.4797505629009315, val_precision: 0.08006735794474726, val_recall: 0.08595896651769876, val_loss: 0\n",
      "epoch: [7/30](32.29454109668732m), step: [446/447], train_loss:0.459998647305256, val_precision: 0.07979256942661615, val_recall: 0.08016307377931296, val_loss: 0\n",
      "epoch: [8/30](32.30582196712494m), step: [446/447], train_loss:0.4374773628359673, val_precision: 0.0800602232271894, val_recall: 0.0826882791553597, val_loss: 0\n",
      "epoch: [9/30](32.380467009544375m), step: [446/447], train_loss:0.4262016880298888, val_precision: 0.079584267862327, val_recall: 0.0817775627773994, val_loss: 0\n",
      "epoch: [10/30](32.29329201380412m), step: [446/447], train_loss:0.40347132353441295, val_precision: 0.07754854423290954, val_recall: 0.08105636852573628, val_loss: 0\n",
      "epoch: [11/30](32.34764767487844m), step: [446/447], train_loss:0.39060721588081426, val_precision: 0.0758795625402389, val_recall: 0.07637802983743189, val_loss: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-fc61660a1aed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m trainIters(train_loader, val_loader, encoder, decoder, num_epochs, learning_rate, \n\u001b[1;32m      4\u001b[0m                \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfr_decay_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_save_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_max_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                beam_size, trainLang)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-84-245f52d0c1b2>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(train_loader, val_loader, encoder, decoder, num_epochs, learning_rate, teacher_forcing_ratio, tfr_decay_rate, model_save_info, tgt_max_len, beam_size, vocab)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mtgt_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtgt_org_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_label_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_label_copy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_true_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             loss = train(src_data, tgt_data, encoder, decoder, encoder_optimizer, \n\u001b[0;32m--> 125\u001b[0;31m                          decoder_optimizer, teacher_forcing_ratio, vocab)\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-245f52d0c1b2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(src_data, tgt_data, encoder, decoder, encoder_optimizer, decoder_optimizer, teacher_forcing_ratio, vocab)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_max_len_batch\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtgt_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mdecoding_token_index\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtgt_max_len_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# state update at each step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;31m#decoder_output = decoder_output.squeeze(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/tx443/NLU/project/NLU_OIE_UnifiedModels/Transformer/seq2seq/models/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, state, get_attention)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m             \u001b[0mupdated_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mget_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/tx443/NLU/project/NLU_OIE_UnifiedModels/Transformer/seq2seq/models/modules/transformer_blocks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, context, state)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lnorm1'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lnorm2'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/tx443/NLU/project/NLU_OIE_UnifiedModels/Transformer/seq2seq/models/modules/attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, v)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mb_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mb_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mqw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mvw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/tx443/NLU/project/NLU_OIE_UnifiedModels/Transformer/seq2seq/models/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sum_tensor_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mx_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# learning_rate = 1e-4\n",
    "print_info = []\n",
    "trainIters(train_loader, val_loader, encoder, decoder, num_epochs, learning_rate, \n",
    "               teacher_forcing_ratio, tfr_decay_rate, model_save_info, tgt_max_len, \n",
    "               beam_size, trainLang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_point_state = torch.load(model_save_info['model_path_for_resume'])\n",
    "encoder.load_state_dict(check_point_state['encoder_state_dict'])\n",
    "# encoder_optimizer.load_state_dict(check_point_state['encoder_optimizer_state_dict'])\n",
    "decoder.load_state_dict(check_point_state['decoder_state_dict'])\n",
    "# decoder_optimizer.load_state_dict(check_point_state['decoder_optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: [0/7](5.341431697209676m), step: [446/447], train_loss:1.7722327897212649, val_precision: 0.17783911300085128, val_recall: 0.16988360982806333, val_loss: 0\n",
      "epoch: [1/7](5.338186713059743m), step: [446/447], train_loss:0.7802168484235503, val_precision: 0.2634523226052388, val_recall: 0.25105687302223817, val_loss: 0\n",
      "epoch: [2/7](5.354854818185171m), step: [446/447], train_loss:0.6185777706054499, val_precision: 0.30115507892423427, val_recall: 0.2845378305188793, val_loss: 0\n",
      "epoch: [3/7](5.347332378228505m), step: [446/447], train_loss:0.5228940870537854, val_precision: 0.334055281972289, val_recall: 0.3126340658198198, val_loss: 0\n",
      "epoch: [4/7](5.331371068954468m), step: [446/447], train_loss:0.4618112228860791, val_precision: 0.354471335993963, val_recall: 0.3192828346479711, val_loss: 0\n",
      "epoch: [5/7](5.336230289936066m), step: [446/447], train_loss:0.4151985651294657, val_precision: 0.3551781143759579, val_recall: 0.32712683584273194, val_loss: 0\n",
      "epoch: [6/7](5.339213335514069m), step: [446/447], train_loss:0.37803823462535335, val_precision: 0.3657257865843064, val_recall: 0.32593924164745924, val_loss: 0\n"
     ]
    }
   ],
   "source": [
    "trainIters(train_loader, val_loader, encoder, decoder, num_epochs, learning_rate, \n",
    "           teacher_forcing_ratio, tfr_decay_rate, model_save_info, tgt_max_len, \n",
    "           beam_size, trainLang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: [0/7](5.338468774159749m), step: [446/447], train_loss:1.7606931774141539, val_precision: 0.20061614582117768, val_recall: 0.17853682105438354, val_loss: 0\n",
      "epoch: [1/7](5.322386189301809m), step: [446/447], train_loss:0.7401965295175045, val_precision: 0.2946695995830124, val_recall: 0.2769553954368527, val_loss: 0\n",
      "epoch: [2/7](5.345347181955973m), step: [446/447], train_loss:0.570334336968343, val_precision: 0.32535961288616083, val_recall: 0.2968977237070686, val_loss: 0\n",
      "epoch: [3/7](5.349120012919108m), step: [446/447], train_loss:0.48958232072109076, val_precision: 0.3423187283953498, val_recall: 0.31296919058722683, val_loss: 0\n",
      "epoch: [4/7](5.362560017903646m), step: [446/447], train_loss:0.4364301985008871, val_precision: 0.3684664166815776, val_recall: 0.34584178777529534, val_loss: 0\n",
      "epoch: [5/7](5.36743247906367m), step: [446/447], train_loss:0.3962817492767735, val_precision: 0.3590141148219893, val_recall: 0.3499998172968276, val_loss: 0\n",
      "epoch: [6/7](5.3403137962023415m), step: [446/447], train_loss:0.3635860640197259, val_precision: 0.3723239525273506, val_recall: 0.32811124642933165, val_loss: 0\n"
     ]
    }
   ],
   "source": [
    "trainIters(train_loader, val_loader, encoder, decoder, num_epochs, learning_rate, \n",
    "           teacher_forcing_ratio, tfr_decay_rate, model_save_info, tgt_max_len, \n",
    "           beam_size, trainLang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permute_factOrder_tgt(val_data, reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = train_loader\n",
    "tgt_max_length = tgt_max_len\n",
    "loss, src_org, tgt_org, tgt_pred = predict_facts(loader, encoder, decoder, tgt_max_length, trainLang)\n",
    "precision, recall = evaluate_prediction(tgt_org, tgt_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510060675022 0.494971937556\n"
     ]
    }
   ],
   "source": [
    "print(precision.mean(), recall.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.340255745725 0.334618997877\n"
     ]
    }
   ],
   "source": [
    "print(precision.mean(), recall.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset1 = VocabDataset(val_src_input_index, val_tgt_input_index, \n",
    "                             val_label_symbolindex, val_indicator, val_data)\n",
    "val_loader1 = torch.utils.data.DataLoader(dataset=val_dataset1,\n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=vocab_collate_func,\n",
    "                                               shuffle=False)\n",
    "\n",
    "train_dataset1 = VocabDataset(train_src_input_index, train_tgt_input_index, \n",
    "                             train_label_symbolindex, train_indicator, train_data, \n",
    "                             max_src_len_dataloader, max_tgt_len_dataloader)\n",
    "\n",
    "train_loader1 = torch.utils.data.DataLoader(dataset=train_dataset1,\n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=vocab_collate_func,\n",
    "                                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(fact1, fact2):\n",
    "    elem1 = fact1.split('@')\n",
    "    elem2 = fact2.split('@')\n",
    "    n1 = len(elem1)\n",
    "    n2 = len(elem2)\n",
    "    sim = 0\n",
    "    for i in range(min(n1,n2)):\n",
    "        sim += difflib.SequenceMatcher(None,elem1[i],elem2[i]).ratio()\n",
    "    return sim/max(n1,n2)\n",
    "\n",
    "def check_fact_same(org_fact, pred_fact):\n",
    "    org_fact_ele = org_fact.split('@')\n",
    "    pred_fact_ele = pred_fact.split('@')\n",
    "    if len(org_fact_ele) == len(pred_fact_ele):\n",
    "        ele_num = len(org_fact_ele)\n",
    "        if difflib.SequenceMatcher(None,org_fact,pred_fact).ratio() > 0.85:\n",
    "            return True       \n",
    "        ele_sim = np.zeros((ele_num,))\n",
    "        for ele_i in range(ele_num):\n",
    "            ele_sim[ele_i] = difflib.SequenceMatcher(None,org_fact_ele[ele_i],pred_fact_ele[ele_i]).ratio()\n",
    "        if ele_sim.min() > 0.85:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_max_length = 130\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    \n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "tgt_pred = []\n",
    "src_org = []\n",
    "tgt_org = []\n",
    "loss = 0\n",
    "loader = val_loader\n",
    "\n",
    "for src_tensor, src_true_len, tgt_tensor, tgt_true_len, tgt_label_vocab, tgt_label_copy, src_org_batch, tgt_org_batch in loader:\n",
    "#     src_tensor, tgt_tensor, tgt_true_len = src_tensor.to(device), tgt_tensor.to(device), tgt_true_len.to(device)\n",
    "#     tgt_label_vocab, tgt_label_copy = tgt_label_vocab.to(device), tgt_label_copy.to(device)\n",
    "    \n",
    "    batch_size = src_tensor.size(0)\n",
    "    encoder_context = encoder(src_tensor)\n",
    "    state = bridge(encoder_context)\n",
    "\n",
    "    decoder_input = torch.tensor([SOS_index]*batch_size, device=device).unsqueeze(1)\n",
    "\n",
    "    decoding_token_index = 0\n",
    "    stop_flag = [False]*batch_size\n",
    "    step_log_likelihoods = []\n",
    "    tgt_pred_batch = [[] for i_batch in range(batch_size)]\n",
    "    tgt_true_len_max = tgt_true_len.cpu().numpy().max()\n",
    "    while decoding_token_index < tgt_max_length:\n",
    "        decoder_output, _ = decoder(decoder_input, state)\n",
    "        # compute loss \n",
    "        if decoding_token_index < tgt_true_len_max:\n",
    "            decoding_label_vocab = tgt_label_vocab[:, decoding_token_index]\n",
    "            decoding_label_copy = tgt_label_copy[:, decoding_token_index, :]\n",
    "            copy_log_probs = decoder_output[:, vocab_pred_size:]+(decoding_label_copy.float()+1e-45).log()\n",
    "            #mask sample which is copied only\n",
    "            gen_mask = ((decoding_label_vocab!=OOV_pred_index) | (decoding_label_copy.sum(-1)==0)).float() \n",
    "            log_gen_mask = (gen_mask + 1e-45).log().unsqueeze(-1)\n",
    "            #mask log_prob value for oov_pred_index when label_vocab==oov_pred_index and is copied \n",
    "            generation_log_probs = decoder_output.gather(1, decoding_label_vocab.unsqueeze(1)) + log_gen_mask\n",
    "            combined_gen_and_copy = torch.cat((generation_log_probs, copy_log_probs), dim=-1)\n",
    "            step_log_likelihood = torch.logsumexp(combined_gen_and_copy, dim=-1)\n",
    "            step_log_likelihoods.append(step_log_likelihood.unsqueeze(1))\n",
    "\n",
    "        #\n",
    "        topv, topi = decoder_output.topk(1, dim=-1)\n",
    "        next_input = topi.detach().cpu().squeeze(1)\n",
    "        decoder_input = []\n",
    "        for i_batch in range(batch_size):\n",
    "            pred_list = vocab_pred+src_org_batch[i_batch]\n",
    "            next_input_token = pred_list[next_input[i_batch].item()]\n",
    "            if next_input_token == vocab_pred[EOS_pred_index]:\n",
    "                stop_flag[i_batch] = True\n",
    "            if not stop_flag[i_batch]:\n",
    "                tgt_pred_batch[i_batch].append(next_input_token)\n",
    "            decoder_input.append(trainLang.word2index.get(next_input_token, UNK_index))\n",
    "        decoder_input = torch.tensor(decoder_input, device=device).unsqueeze(1)\n",
    "        decoding_token_index += 1\n",
    "        if all(stop_flag):\n",
    "            break\n",
    "    log_likelihoods = torch.cat(step_log_likelihoods, dim=-1)\n",
    "    # mask padding for tgt\n",
    "    tgt_pad_mask = sequence_mask(tgt_true_len).float()\n",
    "    log_likelihoods = log_likelihoods*tgt_pad_mask[:,:log_likelihoods.size(1)]\n",
    "    loss += -(log_likelihoods.sum()/tgt_pad_mask.sum()).item()\n",
    "    tgt_pred.extend(tgt_pred_batch)\n",
    "    src_org.extend(src_org_batch)\n",
    "    tgt_org.extend(tgt_org_batch)\n",
    "loss = loss/len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tgt_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "eval_len = len(tgt_pred)\n",
    "precision = np.zeros((eval_len,))\n",
    "recall = np.zeros((eval_len,))\n",
    "Fscore = np.zeros((eval_len,))\n",
    "for i in range(eval_len):\n",
    "    org_facts = ''.join(tgt_org[i]).split('$')\n",
    "    pred_facts = ''.join(tgt_pred[i]).split('$')\n",
    "    pred_facts = list(set(pred_facts))\n",
    "    org_facts_num = len(org_facts)\n",
    "    pred_facts_num = len(pred_facts)\n",
    "    org_match_num = np.zeros((org_facts_num))\n",
    "    pred_match_num = np.zeros((pred_facts_num))\n",
    "    similarity_ma = np.zeros((org_facts_num, pred_facts_num))\n",
    "    for org_i in range(org_facts_num):\n",
    "        for pred_i in range(pred_facts_num):\n",
    "            similarity_ma[org_i, pred_i] = similarity_score(org_facts[org_i], pred_facts[pred_i])\n",
    "    row_ind, col_ind = linear_sum_assignment(-similarity_ma)\n",
    "    \n",
    "    for org_i, pred_i in zip(row_ind, col_ind):\n",
    "        org_fact = org_facts[org_i]\n",
    "        pred_fact = pred_facts[pred_i]\n",
    "        fact_same = check_fact_same(org_fact, pred_fact)\n",
    "        if fact_same:\n",
    "            org_match_num[org_i] = 1\n",
    "            pred_match_num[pred_i] = 1\n",
    "#     print(pred_match_num)\n",
    "#     print(org_match_num)\n",
    "    precision[i] = pred_match_num.mean()\n",
    "    recall[i] = org_match_num.mean()\n",
    "    Fscore[i] = 2*precision[i]*recall[i]/(precision[i]+recall[i]+1e-10)\n",
    "if False:\n",
    "    random_sample = np.random.randint(eval_len)\n",
    "    print('src: ', src_org[random_sample])\n",
    "    print('Ref: ', tgt_org[random_sample])\n",
    "    print('pred: ', tgt_pred[random_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.034 0.033 0.0333333333316\n"
     ]
    }
   ],
   "source": [
    "print(precision.mean(), recall.mean(), Fscore.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src:  这从\"嘉德本\"可以看得很清楚。\n",
      "Ref:  这@可以看得@很清楚\n",
      "pred:  _@嘉@很清本\n"
     ]
    }
   ],
   "source": [
    "random_sample = 300\n",
    "print('src: ', ''.join(src_org[random_sample]))\n",
    "print('Ref: ', ''.join(tgt_org[random_sample]))\n",
    "print('pred: ', ''.join(tgt_pred[random_sample]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': ['067', '章', ' ', '矿', '洞', '抓', '人', '（', '求', '推', '荐', '票', '）'],\n",
       " 'src_org': '067章 矿洞抓人（求推荐票）',\n",
       " 'tgt': ['_', '@', '抓', '@', '人'],\n",
       " 'tgt_list': ['_@抓@人'],\n",
       " 'tgt_org': [{'object': ['人'],\n",
       "   'place': '矿洞',\n",
       "   'predicate': '抓',\n",
       "   'qualifier': '_',\n",
       "   'subject': '_',\n",
       "   'time': '_'}]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(src_org[random_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7741935483870968"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = '企业@大力开发@自主知识产权'\n",
    "t2= '_@大力开发@自主知识产权的新产品'\n",
    "difflib.SequenceMatcher(None, t1, t2).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.    1.    0.75]\n"
     ]
    }
   ],
   "source": [
    "t1_ele = t1.split('@')\n",
    "t2_ele = t2.split('@')\n",
    "ele_num = len(t1_ele)\n",
    "ele_sim = np.zeros((ele_num,))\n",
    "for ele_i in range(ele_num):\n",
    "    ele_sim[ele_i] = difflib.SequenceMatcher(None,t1_ele[ele_i],t2_ele[ele_i]).ratio()\n",
    "print(ele_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
