{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('/scratch/tx443/NLU/project/NLU_OIE_UnifiedModels/Machine_Translation_NLP')\n",
    "# # print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import jieba\n",
    "import re\n",
    "\n",
    "from config import vocab_pred, vocab_pred_size, vocab_prefix\n",
    "from config import UNK_index, PAD_index, SOS_index, EOS_index \n",
    "from config import OOV_pred_index, PAD_pred_index, EOS_pred_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceMisspred(predicate):\n",
    "    '''replace missing predicate\n",
    "    '''\n",
    "    if predicate == '_':\n",
    "        return 'P'\n",
    "    else:\n",
    "        return predicate\n",
    "    \n",
    "def character_segmentation(string):\n",
    "    res = []\n",
    "    for part in list(jieba.cut(string, cut_all=False)):\n",
    "        if re.match('^[\\da-zA-Z]+$', part):\n",
    "            res.append(part)\n",
    "        else:\n",
    "            res.extend(list(part))\n",
    "    return res\n",
    "\n",
    "# def replaceMissinfo(aaa):\n",
    "#     '''replace missing info for subjects/objects\n",
    "#     '''\n",
    "#     placeholder = ['Z','Y','X']\n",
    "#     for i in range(len(aaa)):\n",
    "#         if aaa[i] == '_':\n",
    "#             aaa = aaa[:i] + placeholder.pop() + aaa[i+1:]\n",
    "#     return aaa\n",
    "\n",
    "def load_preprocess_data(data_add):\n",
    "    saoke = []\n",
    "    with open(data_add, 'r') as f:\n",
    "        for line in f:\n",
    "            saoke.append(json.loads(line))\n",
    "    data = []\n",
    "    # list of dict\n",
    "    for sample in saoke:\n",
    "        # remove some exceptions with empty facts\n",
    "        if sample['logic'] == []:\n",
    "            continue\n",
    "        # tokenize src sentence\n",
    "        sample_processed = dict()\n",
    "        sample_processed['src_org'] = sample['natural']\n",
    "        #sample_processed['src'] = list(jieba.cut(sample['natural'], cut_all=False))\n",
    "        sample_processed['src'] = character_segmentation(sample['natural'])\n",
    "        \n",
    "        # transform fact list into str and tokenize\n",
    "        # $ separates facts; @ separate elements for one fact; & separate objects for one fact\n",
    "        sample_processed['tgt_org'] = sample['logic']\n",
    "        logic_list = []\n",
    "        for fact in sample['logic']:\n",
    "            logic_list.append('@'.join([fact['subject'], replaceMisspred(fact['predicate']), \n",
    "                                       '&'.join(fact['object'])]))\n",
    "        sample_processed['tgt_list'] = logic_list\n",
    "        logic_str = '$'.join(logic_list)\n",
    "        sample_processed['tgt'] = character_segmentation(logic_str)\n",
    "        \n",
    "        data.append(sample_processed)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import dropwhile\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name, emb_pretrained_add=None, max_vocab_size=None):\n",
    "        self.name = name\n",
    "        self.word2index = None #{\"$PAD$\": PAD_token, \"$SOS$\": SOS_token, \"$EOS$\": EOS_token, \"$UNK$\": UNK_token}\n",
    "        #self.word2count = None #{\"$PAD$\": 0, \"$SOS$\" : 0, \"$EOS$\": 0, \"$UNK$\": 0}\n",
    "        self.index2word = None #{PAD_token: \"$PAD$\", SOS_token: \"$SOS$\", EOS_token: \"$EOS$\", UNK_token: \"$UNK$\"}\n",
    "        self.max_vocab_size = max_vocab_size  # Count SOS and EOS\n",
    "        self.vocab_size = None\n",
    "        self.emb_pretrained_add = emb_pretrained_add\n",
    "        self.embedding_matrix = None\n",
    "\n",
    "    def build_vocab(self, data):\n",
    "        all_tokens = []\n",
    "        for sample in data:\n",
    "            all_tokens.extend(sample['src'])\n",
    "            all_tokens.extend(sample['tgt'])  \n",
    "        token_counter = Counter(all_tokens)\n",
    "        print('The number of unique tokens totally in dataset: ', len(token_counter))\n",
    "        # remove word with freq==1 \n",
    "        for key, count in dropwhile(lambda key_count: key_count[1] > 1, token_counter.most_common()):\n",
    "            del token_counter[key]\n",
    "        \n",
    "        if self.max_vocab_size:\n",
    "            vocab, count = zip(*token_counter.most_common(self.max_vocab_size))\n",
    "        else:\n",
    "            vocab, count = zip(*token_counter.most_common())\n",
    "        \n",
    "        self.index2word = vocab_prefix + list(vocab)\n",
    "        word2index = dict(zip(self.index2word, range(0, len(self.index2word)))) \n",
    "#         word2index = dict(zip(vocab, range(len(vocab_prefix),len(vocab_prefix)+len(vocab)))) \n",
    "#         for idx, token in enumerate(vocab_prefix):\n",
    "#             word2index[token] = idx\n",
    "        self.word2index = word2index\n",
    "        self.vocab_size = len(self.index2word)\n",
    "        return None \n",
    "\n",
    "    def build_emb_weight(self):\n",
    "        words_emb_dict = load_emb_vectors(self.emb_pretrained_add)\n",
    "        emb_weight = np.zeros([self.vocab_size, 300])\n",
    "        for i in range(len(vocab_prefix), self.vocab_size):\n",
    "            emb = words_emb_dict.get(self.index2word[i], None)\n",
    "            if emb is not None:\n",
    "                try:\n",
    "                    emb_weight[i] = emb\n",
    "                except:\n",
    "                    pass\n",
    "                    #print(len(emb), self.index2word[i], emb)\n",
    "        self.embedding_matrix = emb_weight\n",
    "        return None\n",
    "\n",
    "def load_emb_vectors(fasttest_home):\n",
    "    max_num_load = 500000\n",
    "    words_dict = {}\n",
    "    with open(fasttest_home) as f:\n",
    "        for num_row, line in enumerate(f):\n",
    "            if num_row >= max_num_load:\n",
    "                break\n",
    "            s = line.split()\n",
    "            words_dict[s[0]] = np.asarray(s[1:])\n",
    "    return words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2index(data, key, word2index):\n",
    "    '''\n",
    "    transform tokens into index as input for both src and tgt\n",
    "    '''\n",
    "    indexdata = []\n",
    "    for line in data:\n",
    "        line = line[key]\n",
    "        indexdata.append([word2index[c] if c in word2index.keys() else UNK_index for c in line])\n",
    "        #indexdata[-1].append(EOS_index)\n",
    "    print('finish indexing')\n",
    "    return indexdata\n",
    "\n",
    "def construct_Lang(name, data, emb_pretrained_add = None, max_vocab_size = None):\n",
    "    lang = Lang(name, emb_pretrained_add, max_vocab_size)\n",
    "    lang.build_vocab(data)\n",
    "    if emb_pretrained_add:\n",
    "        lang.build_emb_weight()\n",
    "    return lang\n",
    "\n",
    "def text2symbolindex(data, key, word2index):\n",
    "    '''get generation label for tgt \n",
    "    '''\n",
    "    indexdata = []\n",
    "    for line in data:\n",
    "        line = line[key]\n",
    "        indexdata.append([word2index[c] if c in word2index.keys() else OOV_pred_index for c in line])\n",
    "        #indexdata[-1].append(EOS_index)\n",
    "    print('symbol label finish')\n",
    "    return indexdata\n",
    "\n",
    "def copy_indicator(data, src_key='src', tgt_key='tgt'):\n",
    "    '''get copy label for tgt\n",
    "    '''\n",
    "    indicator = []\n",
    "    for sample in data:\n",
    "        tgt = sample[tgt_key]\n",
    "        src = sample[src_key]\n",
    "        matrix = np.zeros((len(tgt), len(src)), dtype=int)\n",
    "        for m in range(len(tgt)):\n",
    "            for n in range(len(src)):\n",
    "                if tgt[m] == src[n]:\n",
    "                    matrix[m,n] = 1\n",
    "        indicator.append(matrix)\n",
    "    return indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from itertools import dropwhile\n",
    "\n",
    "class VocabDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, src_index, tgt_index, tgt_symbolindex, tgt_indicator, data, src_clip=None, tgt_clip=None):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.src_clip = src_clip\n",
    "        self.tgt_clip = tgt_clip\n",
    "        self.src_list, self.tgt_list = src_index, tgt_index\n",
    "        self.data = data\n",
    "        self.tgt_symbolindex, self.tgt_indicator  = tgt_symbolindex, tgt_indicator\n",
    "        \n",
    "        assert (len(self.src_list) == len(self.tgt_list) == len(self.tgt_symbolindex)== len(self.tgt_indicator))\n",
    "        #self.word2index = word2index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        src = self.src_list[key]\n",
    "        tgt = self.tgt_list[key]\n",
    "        src_org = self.data[key]['src']\n",
    "        tgt_org = self.data[key]['tgt']\n",
    "        tgt_sym = self.tgt_symbolindex[key]\n",
    "        tgt_ind = self.tgt_indicator[key]\n",
    "        \n",
    "        if self.src_clip is not None:\n",
    "            src = src[:self.src_clip]\n",
    "            src_org = src_org[:self.src_clip]\n",
    "            tgt_ind = tgt_ind[:,:self.src_clip]\n",
    "        src_length = len(src)\n",
    "\n",
    "        if self.tgt_clip is not None:\n",
    "            tgt = tgt[:self.tgt_clip]\n",
    "            tgt_org = tgt_org[:self.tgt_clip]\n",
    "            tgt_sym = tgt_sym[:self.tgt_clip]\n",
    "            tgt_ind = tgt_ind[:self.tgt_clip,:]\n",
    "        tgt_length = len(tgt)\n",
    "        \n",
    "        return src, src_length, tgt, tgt_length, tgt_sym, tgt_ind, src_org, tgt_org\n",
    "        \n",
    "        #return src_org, src_tensor, src_true_len, tgt_org, tgt_tensor, tgt_true_len, tgt_label_vocab, tgt_label_copy \n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    src_list = []\n",
    "    tgt_list = []\n",
    "    src_length_list = []\n",
    "    tgt_length_list = []\n",
    "    tgt_symbol_list = []\n",
    "    tgt_indicator_list = []\n",
    "    src_org_list = []\n",
    "    tgt_org_list = []\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for datum in batch:\n",
    "        src_length_list.append(datum[1]) # 不用加1；eos不算\n",
    "        tgt_length_list.append(datum[3]+1) \n",
    "    \n",
    "    batch_max_src_length = np.max(src_length_list)\n",
    "    batch_max_tgt_length = np.max(tgt_length_list)\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        #+[EOS_index] -1\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0, batch_max_src_length-datum[1])),\n",
    "                                mode=\"constant\", constant_values=PAD_index)\n",
    "        src_list.append(padded_vec)\n",
    "        \n",
    "        padded_vec = np.pad(np.array(datum[2]+[EOS_index]),\n",
    "                                pad_width=((0, batch_max_tgt_length-datum[3]-1)),\n",
    "                                mode=\"constant\", constant_values=PAD_index)\n",
    "        tgt_list.append(padded_vec)\n",
    "        \n",
    "        padded_vec = np.pad(np.array(datum[4]+[EOS_pred_index]),\n",
    "                                pad_width=((0, batch_max_tgt_length-datum[3]-1)),\n",
    "                                mode=\"constant\", constant_values=PAD_pred_index)\n",
    "        tgt_symbol_list.append(padded_vec)\n",
    "        \n",
    "        indicator = np.pad(datum[5], pad_width=((0,1),(0,0)), \n",
    "                           mode='constant', constant_values=0)\n",
    "        #indicator[-1,-1] = 1  -1\n",
    "        padded_vec = np.pad(indicator,\n",
    "                            pad_width=((0, batch_max_tgt_length-datum[3]-1),((0, batch_max_src_length-datum[1]))),\n",
    "                            mode=\"constant\", constant_values=0)\n",
    "        #print(padded_vec.dtype, padded_vec.shape)\n",
    "        tgt_indicator_list.append(padded_vec)\n",
    "        \n",
    "        src_org_list.append(datum[6])\n",
    "        tgt_org_list.append(datum[7])\n",
    "    \n",
    "    # re-order\n",
    "    ind_dec_order = np.argsort(src_length_list)[::-1]\n",
    "    \n",
    "    src_list = np.array(src_list)[ind_dec_order]\n",
    "    src_length_list = np.array(src_length_list)[ind_dec_order]\n",
    "    tgt_list = np.array(tgt_list)[ind_dec_order]\n",
    "    tgt_length_list = np.array(tgt_length_list)[ind_dec_order]\n",
    "    tgt_symbol_list = np.array(tgt_symbol_list)[ind_dec_order]\n",
    "    #print(tgt_indicator_list[0].dtype, tgt_indicator_list[0][:5][:5])\n",
    "    tgt_indicator_list = np.array(tgt_indicator_list)[ind_dec_order]\n",
    "    #print(tgt_indicator_list.dtype, tgt_indicator_list.shape)\n",
    "    src_org_list = [src_org_list[i] for i in ind_dec_order]\n",
    "    tgt_org_list = [tgt_org_list[i] for i in ind_dec_order]\n",
    "    \n",
    "    #print(type(np.array(data_list)),type(np.array(label_list)))\n",
    "    \n",
    "    return [torch.from_numpy(src_list).to(device), \n",
    "            torch.LongTensor(src_length_list).to(device), \n",
    "            torch.from_numpy(tgt_list).to(device), \n",
    "            torch.LongTensor(tgt_length_list).to(device),\n",
    "            torch.from_numpy(tgt_symbol_list).to(device),\n",
    "            torch.from_numpy(tgt_indicator_list).to(device),\n",
    "            src_org_list,\n",
    "            tgt_org_list,           \n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_add = '/scratch/tx443/NLU/project/SAOKE_DATA.json'\n",
    "data = load_preprocess_data(data_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train val test\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_train_data = sorted(train_data, key=lambda x: len(x['tgt']), reverse=False)\n",
    "# train_data = sorted_train_data[0:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique tokens totally in dataset:  9364\n"
     ]
    }
   ],
   "source": [
    "# build vocab from train for input indexing\n",
    "trainLang = construct_Lang('train', train_data)\n",
    "\n",
    "# build generation vocab for prediction\n",
    "word2symbolindex = {}\n",
    "for idx, token in enumerate(vocab_pred):\n",
    "        word2symbolindex[token] = idx\n",
    "\n",
    "# check\n",
    "assert(UNK_index==trainLang.word2index['<UNK>'])\n",
    "assert(PAD_index==trainLang.word2index['<PAD>'])\n",
    "assert(SOS_index==trainLang.word2index['<SOS>'])\n",
    "assert(EOS_index==trainLang.word2index['<EOS>'])\n",
    "\n",
    "assert(OOV_pred_index==word2symbolindex['<OOV>'])\n",
    "assert(PAD_pred_index==word2symbolindex['<PAD>'])\n",
    "assert(EOS_pred_index==word2symbolindex['<EOS>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permute facts at this place; data['tgt']\n",
    "def identity(facts_list):\n",
    "    return facts_list\n",
    "\n",
    "def reverse(facts_list):\n",
    "    return facts_list[::-1]\n",
    "\n",
    "def random_pm(facts_list):\n",
    "    random_order = np.random.permutation(len(facts_list))\n",
    "    return [facts_list[idx] for idx in random_order]\n",
    "\n",
    "def last3_pm(facts_list):\n",
    "    if len(facts_list) < 4:\n",
    "        return facts_list\n",
    "    else:\n",
    "        return facts_list[-3:]+facts_list[:-3]\n",
    "\n",
    "def permute_factOrder_tgt(data, pm_fn):\n",
    "    data_len = len(data)\n",
    "    for i in range(data_len):\n",
    "        facts_list = data[i]['tgt_list']\n",
    "        facts_list_pm = pm_fn(facts_list)\n",
    "        data[i]['tgt'] = character_segmentation('$'.join(facts_list_pm))\n",
    "    return None\n",
    "\n",
    "# permute_factOrder_tgt(train_data, last3_pm)\n",
    "# train_len = len(train_data)\n",
    "# for i in range(train_len):\n",
    "#     facts_list = train_data[i]['tgt_list']\n",
    "#     facts_list_pm = facts_list[::-1]\n",
    "#     train_data[i]['tgt'] = character_segmentation('$'.join(facts_list_pm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish indexing\n",
      "finish indexing\n",
      "0.3885641098022461\n"
     ]
    }
   ],
   "source": [
    "# input indexing for src\n",
    "start_time = time.time()\n",
    "train_src_input_index = text2index(train_data, 'src', trainLang.word2index) \n",
    "val_src_input_index = text2index(val_data, 'src', trainLang.word2index) \n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish indexing\n",
      "finish indexing\n"
     ]
    }
   ],
   "source": [
    "# input indexing for tgt\n",
    "train_tgt_input_index = text2index(train_data, 'tgt', trainLang.word2index) \n",
    "val_tgt_input_index = text2index(val_data, 'tgt', trainLang.word2index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symbol label finish\n",
      "symbol label finish\n"
     ]
    }
   ],
   "source": [
    "# get generation label\n",
    "train_label_symbolindex = text2symbolindex(train_data, 'tgt', word2symbolindex)\n",
    "val_label_symbolindex = text2symbolindex(val_data, 'tgt', word2symbolindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.306727170944214\n"
     ]
    }
   ],
   "source": [
    "# get copy label\n",
    "start_time = time.time()\n",
    "train_indicator = copy_indicator(train_data, 'src', 'tgt')\n",
    "val_indicator = copy_indicator(val_data, 'src', 'tgt')\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28564, 28564, 28564, 28564, 28564)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_src_input_index),len(train_tgt_input_index),len(train_label_symbolindex),len(train_indicator),len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6121, 6121, 6121, 6121, 6121)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_src_input_index),len(val_tgt_input_index),len(val_label_symbolindex),len(val_indicator),len(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "# from Data_utils import VocabDataset, vocab_collate_func\n",
    "# from preprocessing_util import preposs_toekn, Lang, text2index, construct_Lang\n",
    "from config import device, embedding_freeze\n",
    "import random\n",
    "from evaluation import similarity_score, check_fact_same, predict_facts, evaluate_prediction\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bridge(context):\n",
    "    return State(context=context,batch_first=True)\n",
    "\n",
    "def train(src_data, tgt_data, encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
    "          teacher_forcing_ratio, vocab):\n",
    "    src_org_batch, src_tensor, src_true_len = src_data\n",
    "    tgt_org_batch, tgt_tensor, tgt_label_vocab, tgt_label_copy, tgt_true_len = tgt_data\n",
    "    '''\n",
    "    finish train for a batch\n",
    "    '''\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    batch_size = src_tensor.size(0)\n",
    "    encoder_context = encoder(src_tensor)\n",
    "    state = bridge(encoder_context)\n",
    "    \n",
    "    decoder_input = torch.tensor([SOS_index]*batch_size, device=device).unsqueeze(1)\n",
    "    step_log_likelihoods = []\n",
    "    #print(decoder_hidden.size())\n",
    "    #print('encoddddddddddder finishhhhhhhhhhhhhhh')\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        ### Teacher forcing: Feed the target as the next input\n",
    "        decoding_token_index = 0\n",
    "        tgt_max_len_batch = tgt_true_len.cpu().max().item()\n",
    "        assert(tgt_max_len_batch==tgt_tensor.size(1))\n",
    "        while decoding_token_index < tgt_max_len_batch:\n",
    "            decoder_output, _ = decoder(decoder_input, state) # state update at each step\n",
    "            #decoder_output = decoder_output.squeeze(1)\n",
    "\n",
    "            decoding_label_vocab = tgt_label_vocab[:, decoding_token_index]\n",
    "            decoding_label_copy = tgt_label_copy[:, decoding_token_index, :]\n",
    "            copy_log_probs = decoder_output[:, vocab_pred_size:]+(decoding_label_copy.float()+1e-45).log()\n",
    "            #mask sample which is copied only\n",
    "            gen_mask = ((decoding_label_vocab!=OOV_pred_index) | (decoding_label_copy.sum(-1)==0)).float() \n",
    "            log_gen_mask = (gen_mask + 1e-45).log().unsqueeze(-1)\n",
    "            #mask log_prob value for oov_pred_index when label_vocab==oov_pred_index and is copied \n",
    "            generation_log_probs = decoder_output.gather(1, decoding_label_vocab.unsqueeze(1)) + log_gen_mask\n",
    "            combined_gen_and_copy = torch.cat((generation_log_probs, copy_log_probs), dim=-1)\n",
    "            step_log_likelihood = torch.logsumexp(combined_gen_and_copy, dim=-1)\n",
    "            step_log_likelihoods.append(step_log_likelihood.unsqueeze(1))\n",
    "            #loss += criterion(decoder_output, tgt_tensor[:,decoding_token_index])\n",
    "            decoder_input = tgt_tensor[:,decoding_token_index].unsqueeze(1)  # Teacher forcing\n",
    "            decoding_token_index += 1\n",
    "\n",
    "    else:\n",
    "        ### Without teacher forcing: use its own predictions as the next input\n",
    "        decoding_token_index = 0\n",
    "        tgt_max_len_batch = tgt_true_len.cpu().max().item()\n",
    "        assert(tgt_max_len_batch==tgt_tensor.size(1))\n",
    "        while decoding_token_index < tgt_max_len_batch:\n",
    "            decoder_output, _ = decoder(decoder_input, state)\n",
    "            #decoder_output = decoder_output.squeeze(1)\n",
    "            \n",
    "            decoding_label_vocab = tgt_label_vocab[:, decoding_token_index]\n",
    "            decoding_label_copy = tgt_label_copy[:, decoding_token_index, :]\n",
    "            copy_log_probs = decoder_output[:, vocab_pred_size:]+(decoding_label_copy.float()+1e-45).log()\n",
    "            #mask sample which is copied only\n",
    "            gen_mask = ((decoding_label_vocab!=OOV_pred_index)|(decoding_label_copy.sum(-1)==0)).float() \n",
    "            log_gen_mask = (gen_mask + 1e-45).log().unsqueeze(-1)\n",
    "            #mask log_prob value for oov_pred_index when label_vocab==oov_pred_index and is copied \n",
    "            generation_log_probs = decoder_output.gather(1, decoding_label_vocab.unsqueeze(1)) + log_gen_mask\n",
    "            combined_gen_and_copy = torch.cat((generation_log_probs, copy_log_probs), dim=-1)\n",
    "            step_log_likelihood = torch.logsumexp(combined_gen_and_copy, dim=-1)\n",
    "            step_log_likelihoods.append(step_log_likelihood.unsqueeze(1))\n",
    "\n",
    "            topv, topi = decoder_output.topk(1, dim=-1)\n",
    "            next_input = topi.detach().cpu().squeeze(1)\n",
    "            decoder_input = []\n",
    "            for i_batch in range(batch_size):\n",
    "                pred_list = vocab_pred+src_org_batch[i_batch]\n",
    "                next_input_token = pred_list[next_input[i_batch].item()]\n",
    "                decoder_input.append(vocab.word2index.get(next_input_token, UNK_index))\n",
    "            decoder_input = torch.tensor(decoder_input, device=device).unsqueeze(1)\n",
    "            decoding_token_index += 1\n",
    "\n",
    "    # average loss\n",
    "    log_likelihoods = torch.cat(step_log_likelihoods, dim=-1)\n",
    "    # mask padding for tgt\n",
    "    tgt_pad_mask = sequence_mask(tgt_true_len).float()\n",
    "    log_likelihoods = log_likelihoods*tgt_pad_mask\n",
    "    loss = -log_likelihoods.sum()/batch_size\n",
    "    loss.backward()\n",
    "\n",
    "    ### TODO\n",
    "    # clip for gradient exploding \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return (loss*batch_size/tgt_pad_mask.sum()).item() #torch.div(loss, tgt_true_len.type_as(loss).mean()).item()  #/tgt_true_len.mean()\n",
    "\n",
    "\n",
    "def trainIters(train_loader, val_loader, encoder, decoder, num_epochs, learning_rate, \n",
    "               teacher_forcing_ratio, tfr_decay_rate, model_save_info, tgt_max_len, \n",
    "               beam_size, vocab):\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    if model_save_info['model_path_for_resume'] is not None:\n",
    "        check_point_state = torch.load(model_save_info['model_path_for_resume'])\n",
    "        encoder.load_state_dict(check_point_state['encoder_state_dict'])\n",
    "        encoder_optimizer.load_state_dict(check_point_state['encoder_optimizer_state_dict'])\n",
    "        decoder.load_state_dict(check_point_state['decoder_state_dict'])\n",
    "        decoder_optimizer.load_state_dict(check_point_state['decoder_optimizer_state_dict'])\n",
    "\n",
    "    for epoch in range(num_epochs): \n",
    "        start_time = time.time()\n",
    "        n_iter = -1\n",
    "        losses = np.zeros((len(train_loader),))\n",
    "        if tfr_decay_rate is not None:\n",
    "            teacher_forcing_ratio *= tfr_decay_rate\n",
    "        for src_tensor, src_true_len, tgt_tensor, tgt_true_len, tgt_label_vocab, tgt_label_copy, src_org_batch, tgt_org_batch in train_loader:\n",
    "            n_iter += 1\n",
    "            #print('start_step: ', n_iter)\n",
    "            src_data = (src_org_batch, src_tensor, src_true_len)\n",
    "            tgt_data = (tgt_org_batch, tgt_tensor, tgt_label_vocab, tgt_label_copy, tgt_true_len)\n",
    "            loss = train(src_data, tgt_data, encoder, decoder, encoder_optimizer, \n",
    "                         decoder_optimizer, teacher_forcing_ratio, vocab)\n",
    "            losses[n_iter] = loss\n",
    "            if n_iter % 500 == 0:\n",
    "                pass\n",
    "                #print('Loss:', loss)\n",
    "                #eva_start = time.time()\n",
    "#                 precision, recall, val_loss = evaluate_batch(val_loader, encoder, decoder, tgt_max_len, vocab, vocab_pred_size)\n",
    "#                 #print((time.time()-eva_start)/60)\n",
    "#                 print('epoch: [{}/{}], step: [{}/{}], train_loss:{}, val_precision: {}, val_recall: {}, val_loss: {}'.format(\n",
    "#                     epoch, num_epochs, n_iter, len(train_loader), loss, precision.mean(), recall.mean(), val_loss))\n",
    "               # print('Decoder parameters grad:')\n",
    "               # for p in decoder.named_parameters():\n",
    "               #     print(p[0], ': ',  p[1].grad.data.abs().mean().item(), p[1].grad.data.abs().max().item(), p[1].data.abs().mean().item(), p[1].data.abs().max().item(), end=' ')\n",
    "               # print('\\n')\n",
    "               # print('Encoder Parameters grad:')\n",
    "               # for p in encoder.named_parameters():\n",
    "               #     print(p[0], ': ',  p[1].grad.data.abs().mean().item(), p[1].grad.data.abs().max().item(), p[1].data.abs().mean().item(), p[1].data.abs().max().item(), end=' ')\n",
    "               # print('\\n')\n",
    "        val_loss, src_org, tgt_org, tgt_pred = predict_facts(val_loader, encoder, decoder, tgt_max_len, vocab)\n",
    "        precision, recall = evaluate_prediction(tgt_org, tgt_pred)\n",
    "        epoch_train_time = (time.time()-start_time)/60\n",
    "        print('epoch: [{}/{}]({}m), step: [{}/{}], train_loss:{}, val_precision: {}, val_recall: {}, val_loss: {}'.format(\n",
    "            epoch, num_epochs, epoch_train_time, n_iter, len(train_loader), losses.mean(), precision.mean(), recall.mean(), val_loss))\n",
    "\n",
    "        if (epoch+1) % model_save_info['epochs_per_save_model'] == 0:\n",
    "            check_point_state = {\n",
    "                'epoch': epoch,\n",
    "                'encoder_state_dict': encoder.state_dict(),\n",
    "                'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                'decoder_optimizer_state_dict': decoder_optimizer.state_dict()\n",
    "                }\n",
    "            torch.save(check_point_state, '{}epoch_{}.pth'.format(model_save_info['model_path'], epoch))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paras = dict( \n",
    "#     tgt_max_len = 130,\n",
    "#     max_src_len_dataloader =94,\n",
    "#     max_tgt_len_dataloader =127,\n",
    "\n",
    "#     emb_size = 200,\n",
    "#     en_hidden_size = 128,\n",
    "#     en_num_layers = 2,\n",
    "#     en_num_direction = 2,\n",
    "#     de_hidden_size = 256,\n",
    "#     de_num_layers = 3,\n",
    "#     rnn_type = 'GRU', # {LSTM, GRU}\n",
    "#     attention_type = 'dot_prod', #'dot_prod', general, concat #dot-product need pre-process\n",
    "#     teacher_forcing_ratio = 1,\n",
    "#     tfr_decay_rate = None, #'None means no decay'\n",
    "\n",
    "#     learning_rate = 1e-3,\n",
    "#     num_epochs = 7,\n",
    "#     batch_size = 64, \n",
    "#     beam_size = 5,\n",
    "#     dropout_rate = 0.0,\n",
    "\n",
    "#     model_save_info = dict(\n",
    "#         model_path = 'nmt_models/model1/',\n",
    "#         epochs_per_save_model = 2,\n",
    "#         model_path_for_resume = None #'nmt_models/epoch_0.pth'\n",
    "#         )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tgt_max_len = paras['tgt_max_len']\n",
    "# max_src_len_dataloader = paras['max_src_len_dataloader']\n",
    "# max_tgt_len_dataloader = paras['max_tgt_len_dataloader']\n",
    "\n",
    "# teacher_forcing_ratio = paras['teacher_forcing_ratio']\n",
    "# tfr_decay_rate = paras['tfr_decay_rate']\n",
    "# emb_size = paras['emb_size']\n",
    "# en_hidden_size = paras['en_hidden_size']\n",
    "# en_num_layers = paras['en_num_layers']\n",
    "# en_num_direction = paras['en_num_direction']\n",
    "# de_hidden_size = paras['de_hidden_size']\n",
    "# de_num_layers = paras['de_num_layers']\n",
    "\n",
    "# learning_rate = paras['learning_rate']\n",
    "# num_epochs = paras['num_epochs']\n",
    "# batch_size = paras['batch_size']\n",
    "# rnn_type = paras['rnn_type']\n",
    "# attention_type = paras['attention_type']\n",
    "# beam_size = paras['beam_size']\n",
    "# model_save_info = paras['model_save_info']\n",
    "# dropout_rate = paras['dropout_rate']\n",
    "\n",
    "batch_size = 64\n",
    "teacher_forcing_ratio = 1\n",
    "tfr_decay_rate = None\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "max_src_len_dataloader = 94\n",
    "max_tgt_len_dataloader = 127\n",
    "tgt_max_len = 130\n",
    "model_save_info = dict(\n",
    "            model_path = 'nmt_models/T2/',\n",
    "            epochs_per_save_model = 1,\n",
    "            model_path_for_resume = None #'nmt_models/epoch_0.pth'\n",
    "            )\n",
    "\n",
    "emb_size = 300\n",
    "# vocab_size = None\n",
    "num_layers = 2\n",
    "stateful = None\n",
    "beam_size = 1\n",
    "classifier_type = 'copy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VocabDataset(train_src_input_index, train_tgt_input_index, \n",
    "                             train_label_symbolindex, train_indicator, train_data, \n",
    "                             max_src_len_dataloader, max_tgt_len_dataloader)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=vocab_collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "val_dataset = VocabDataset(val_src_input_index, val_tgt_input_index, \n",
    "                           val_label_symbolindex, val_indicator, val_data,\n",
    "                           max_src_len_dataloader, max_tgt_len_dataloader)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=vocab_collate_func,\n",
    "                                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder:\n",
      "TransformerAttentionEncoder(\n",
      "  (embedder): Embedding(8776, 300, padding_idx=0)\n",
      "  (dropout): Dropout(p=0, inplace)\n",
      "  (blocks): ModuleList(\n",
      "    (0): EncoderBlock(\n",
      "      (lnorm1): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      (lnorm2): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0)\n",
      "      (attention): MultiHeadAttention(\n",
      "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (sdp_attention): SDPAttention(\n",
      "          (dropout): Dropout(p=0)\n",
      "        )\n",
      "      )\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): Dropout(p=0)\n",
      "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): EncoderBlock(\n",
      "      (lnorm1): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      (lnorm2): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0)\n",
      "      (attention): MultiHeadAttention(\n",
      "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (sdp_attention): SDPAttention(\n",
      "          (dropout): Dropout(p=0)\n",
      "        )\n",
      "      )\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): Dropout(p=0)\n",
      "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Decoder:\n",
      "TransformerAttentionDecoder(\n",
      "  (embedder): Embedding(8776, 300, padding_idx=0)\n",
      "  (dropout): Dropout(p=0, inplace)\n",
      "  (blocks): ModuleList(\n",
      "    (0): DecoderBlock(\n",
      "      (lnorm1): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      (lnorm2): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      (lnorm3): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0)\n",
      "      (attention): MultiHeadAttention(\n",
      "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (sdp_attention): SDPAttention(\n",
      "          (dropout): Dropout(p=0)\n",
      "        )\n",
      "      )\n",
      "      (masked_attention): MultiHeadAttention(\n",
      "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (sdp_attention): SDPAttention(\n",
      "          (dropout): Dropout(p=0)\n",
      "        )\n",
      "      )\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): Dropout(p=0)\n",
      "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): DecoderBlock(\n",
      "      (lnorm1): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      (lnorm2): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      (lnorm3): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0)\n",
      "      (attention): MultiHeadAttention(\n",
      "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (sdp_attention): SDPAttention(\n",
      "          (dropout): Dropout(p=0)\n",
      "        )\n",
      "      )\n",
      "      (masked_attention): MultiHeadAttention(\n",
      "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (sdp_attention): SDPAttention(\n",
      "          (dropout): Dropout(p=0)\n",
      "        )\n",
      "      )\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): Dropout(p=0)\n",
      "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): CopyMechanism(\n",
      "    (generate_linear): Linear(in_features=512, out_features=20, bias=True)\n",
      "    (copy_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (LogSoftmax): LogSoftmax()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# make dir for saving models\n",
    "from seq2seq.models.transformer import TransformerAttentionEncoder, TransformerAttentionDecoder, sequence_mask\n",
    "from seq2seq.models.modules.state import State\n",
    "\n",
    "if not os.path.exists(model_save_info['model_path']):\n",
    "    os.makedirs(model_save_info['model_path'])\n",
    "### save model hyperparameters\n",
    "# with open(model_save_info['model_path']+'model_params.pkl', 'wb') as f:\n",
    "#     model_hyparams = paras\n",
    "#     pickle.dump(model_hyparams, f)\n",
    "# print(model_hyparams)\n",
    "\n",
    "# read all data\n",
    "### save srcLang and tgtLang\n",
    "\n",
    "#for src; keep original src_org and index based on vocab src_tensor\n",
    "\n",
    "#for tgt; vocab_pred_label, copy_label\n",
    "\n",
    "\n",
    "# test_dataset = VocabDataset(test_data)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "#                                            batch_size=BATCH_SIZE,\n",
    "#                                            collate_fn=vocab_collate_func,\n",
    "#                                            shuffle=False)\n",
    "\n",
    "# embedding_src_weight = torch.from_numpy(srcLang.embedding_matrix).type(torch.FloatTensor).to(device)\n",
    "# embedding_tgt_weight = torch.from_numpy(tgtLang.embedding_matrix).type(torch.FloatTensor).to(device)\n",
    "# print(embedding_src_weight.size(), embedding_tgt_weight.size())\n",
    "\n",
    "encoder = TransformerAttentionEncoder(vocab_size=trainLang.vocab_size, num_layers=num_layers, embedding_size = emb_size)\n",
    "decoder = TransformerAttentionDecoder(vocab_size=trainLang.vocab_size, num_layers=num_layers, embedding_size = emb_size, classifier_type=classifier_type, stateful=None)\n",
    "\n",
    "encoder, decoder = encoder.to(device), decoder.to(device)\n",
    "print('Encoder:')\n",
    "print(encoder)\n",
    "print('Decoder:')\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIters(train_loader, val_loader, encoder, decoder, num_epochs, learning_rate, \n",
    "               teacher_forcing_ratio, tfr_decay_rate, model_save_info, tgt_max_len, \n",
    "               beam_size, trainLang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIters(train_loader, val_loader, encoder, decoder, num_epochs, learning_rate, \n",
    "           teacher_forcing_ratio, tfr_decay_rate, model_save_info, tgt_max_len, \n",
    "           beam_size, trainLang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: [0/7](5.341431697209676m), step: [446/447], train_loss:1.7722327897212649, val_precision: 0.17783911300085128, val_recall: 0.16988360982806333, val_loss: 0\n",
      "epoch: [1/7](5.338186713059743m), step: [446/447], train_loss:0.7802168484235503, val_precision: 0.2634523226052388, val_recall: 0.25105687302223817, val_loss: 0\n",
      "epoch: [2/7](5.354854818185171m), step: [446/447], train_loss:0.6185777706054499, val_precision: 0.30115507892423427, val_recall: 0.2845378305188793, val_loss: 0\n",
      "epoch: [3/7](5.347332378228505m), step: [446/447], train_loss:0.5228940870537854, val_precision: 0.334055281972289, val_recall: 0.3126340658198198, val_loss: 0\n",
      "epoch: [4/7](5.331371068954468m), step: [446/447], train_loss:0.4618112228860791, val_precision: 0.354471335993963, val_recall: 0.3192828346479711, val_loss: 0\n",
      "epoch: [5/7](5.336230289936066m), step: [446/447], train_loss:0.4151985651294657, val_precision: 0.3551781143759579, val_recall: 0.32712683584273194, val_loss: 0\n",
      "epoch: [6/7](5.339213335514069m), step: [446/447], train_loss:0.37803823462535335, val_precision: 0.3657257865843064, val_recall: 0.32593924164745924, val_loss: 0\n"
     ]
    }
   ],
   "source": [
    "trainIters(train_loader, val_loader, encoder, decoder, num_epochs, learning_rate, \n",
    "           teacher_forcing_ratio, tfr_decay_rate, model_save_info, tgt_max_len, \n",
    "           beam_size, trainLang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: [0/7](5.338468774159749m), step: [446/447], train_loss:1.7606931774141539, val_precision: 0.20061614582117768, val_recall: 0.17853682105438354, val_loss: 0\n",
      "epoch: [1/7](5.322386189301809m), step: [446/447], train_loss:0.7401965295175045, val_precision: 0.2946695995830124, val_recall: 0.2769553954368527, val_loss: 0\n",
      "epoch: [2/7](5.345347181955973m), step: [446/447], train_loss:0.570334336968343, val_precision: 0.32535961288616083, val_recall: 0.2968977237070686, val_loss: 0\n",
      "epoch: [3/7](5.349120012919108m), step: [446/447], train_loss:0.48958232072109076, val_precision: 0.3423187283953498, val_recall: 0.31296919058722683, val_loss: 0\n",
      "epoch: [4/7](5.362560017903646m), step: [446/447], train_loss:0.4364301985008871, val_precision: 0.3684664166815776, val_recall: 0.34584178777529534, val_loss: 0\n",
      "epoch: [5/7](5.36743247906367m), step: [446/447], train_loss:0.3962817492767735, val_precision: 0.3590141148219893, val_recall: 0.3499998172968276, val_loss: 0\n",
      "epoch: [6/7](5.3403137962023415m), step: [446/447], train_loss:0.3635860640197259, val_precision: 0.3723239525273506, val_recall: 0.32811124642933165, val_loss: 0\n"
     ]
    }
   ],
   "source": [
    "trainIters(train_loader, val_loader, encoder, decoder, num_epochs, learning_rate, \n",
    "           teacher_forcing_ratio, tfr_decay_rate, model_save_info, tgt_max_len, \n",
    "           beam_size, trainLang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permute_factOrder_tgt(val_data, reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = val_loader\n",
    "tgt_max_length = tgt_max_len\n",
    "loss, src_org, tgt_org, tgt_pred = predict_facts(loader, encoder, decoder, tgt_max_length, trainLang)\n",
    "precision, recall = evaluate_prediction(tgt_org, tgt_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.346662081497 0.345182040726\n"
     ]
    }
   ],
   "source": [
    "print(precision.mean(), recall.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.340255745725 0.334618997877\n"
     ]
    }
   ],
   "source": [
    "print(precision.mean(), recall.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset1 = VocabDataset(val_src_input_index, val_tgt_input_index, \n",
    "                             val_label_symbolindex, val_indicator, val_data)\n",
    "val_loader1 = torch.utils.data.DataLoader(dataset=val_dataset1,\n",
    "                                               batch_size=32,\n",
    "                                               collate_fn=vocab_collate_func,\n",
    "                                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(fact1, fact2):\n",
    "    elem1 = fact1.split('@')\n",
    "    elem2 = fact2.split('@')\n",
    "    n1 = len(elem1)\n",
    "    n2 = len(elem2)\n",
    "    sim = 0\n",
    "    for i in range(min(n1,n2)):\n",
    "        sim += difflib.SequenceMatcher(None,elem1[i],elem2[i]).ratio()\n",
    "    return sim/max(n1,n2)\n",
    "\n",
    "def check_fact_same(org_fact, pred_fact):\n",
    "    org_fact_ele = org_fact.split('@')\n",
    "    pred_fact_ele = pred_fact.split('@')\n",
    "    if len(org_fact_ele) == len(pred_fact_ele):\n",
    "        ele_num = len(org_fact_ele)\n",
    "        if difflib.SequenceMatcher(None,org_fact,pred_fact).ratio() > 0.85:\n",
    "            return True       \n",
    "        ele_sim = np.zeros((ele_num,))\n",
    "        for ele_i in range(ele_num):\n",
    "            ele_sim[ele_i] = difflib.SequenceMatcher(None,org_fact_ele[ele_i],pred_fact_ele[ele_i]).ratio()\n",
    "        if ele_sim.min() > 0.85:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-61952622d042>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtgt_true_len_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt_true_len\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mdecoding_token_index\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtgt_max_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;31m# compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdecoding_token_index\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtgt_true_len_max\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/tx443/NLU/project/NLU_OIE_UnifiedModels/Transformer/seq2seq/models/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, state, get_attention)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m             \u001b[0mupdated_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mget_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/tx443/NLU/project/NLU_OIE_UnifiedModels/Transformer/seq2seq/models/modules/transformer_blocks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, context, state)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lnorm1'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lnorm2'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/tx443/NLU/project/NLU_OIE_UnifiedModels/Transformer/seq2seq/models/modules/attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, v)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mb_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mqw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0mvw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mqw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/tx443/NLU/project/NLU_OIE_UnifiedModels/Transformer/seq2seq/models/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sum_tensor_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mx_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tgt_max_length = 130\n",
    "device = \"cpu\" #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    \n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "tgt_pred = []\n",
    "src_org = []\n",
    "tgt_org = []\n",
    "loss = 0\n",
    "loader = val_loader1\n",
    "\n",
    "for src_tensor, src_true_len, tgt_tensor, tgt_true_len, tgt_label_vocab, tgt_label_copy, src_org_batch, tgt_org_batch in loader:\n",
    "    src_tensor, tgt_tensor, tgt_true_len = src_tensor.to(device), tgt_tensor.to(device), tgt_true_len.to(device)\n",
    "    tgt_label_vocab, tgt_label_copy = tgt_label_vocab.to(device), tgt_label_copy.to(device)\n",
    "    \n",
    "    batch_size = src_tensor.size(0)\n",
    "    encoder_context = encoder(src_tensor)\n",
    "    state = bridge(encoder_context)\n",
    "\n",
    "    decoder_input = torch.tensor([SOS_index]*batch_size, device=device).unsqueeze(1)\n",
    "\n",
    "    decoding_token_index = 0\n",
    "    stop_flag = [False]*batch_size\n",
    "    step_log_likelihoods = []\n",
    "    tgt_pred_batch = [[] for i_batch in range(batch_size)]\n",
    "    tgt_true_len_max = tgt_true_len.cpu().numpy().max()\n",
    "    while decoding_token_index < tgt_max_length:\n",
    "        decoder_output, _ = decoder(decoder_input, state)\n",
    "        # compute loss \n",
    "        if decoding_token_index < tgt_true_len_max:\n",
    "            decoding_label_vocab = tgt_label_vocab[:, decoding_token_index]\n",
    "            decoding_label_copy = tgt_label_copy[:, decoding_token_index, :]\n",
    "            copy_log_probs = decoder_output[:, vocab_pred_size:]+(decoding_label_copy.float()+1e-45).log()\n",
    "            #mask sample which is copied only\n",
    "            gen_mask = ((decoding_label_vocab!=OOV_pred_index) | (decoding_label_copy.sum(-1)==0)).float() \n",
    "            log_gen_mask = (gen_mask + 1e-45).log().unsqueeze(-1)\n",
    "            #mask log_prob value for oov_pred_index when label_vocab==oov_pred_index and is copied \n",
    "            generation_log_probs = decoder_output.gather(1, decoding_label_vocab.unsqueeze(1)) + log_gen_mask\n",
    "            combined_gen_and_copy = torch.cat((generation_log_probs, copy_log_probs), dim=-1)\n",
    "            step_log_likelihood = torch.logsumexp(combined_gen_and_copy, dim=-1)\n",
    "            step_log_likelihoods.append(step_log_likelihood.unsqueeze(1))\n",
    "\n",
    "        #\n",
    "        topv, topi = decoder_output.topk(1, dim=-1)\n",
    "        next_input = topi.detach().cpu().squeeze(1)\n",
    "        decoder_input = []\n",
    "        for i_batch in range(batch_size):\n",
    "            pred_list = vocab_pred+src_org_batch[i_batch]\n",
    "            next_input_token = pred_list[next_input[i_batch].item()]\n",
    "            if next_input_token == vocab_pred[EOS_pred_index]:\n",
    "                stop_flag[i_batch] = True\n",
    "            if not stop_flag[i_batch]:\n",
    "                tgt_pred_batch[i_batch].append(next_input_token)\n",
    "            decoder_input.append(trainLang.word2index.get(next_input_token, UNK_index))\n",
    "        decoder_input = torch.tensor(decoder_input, device=device).unsqueeze(1)\n",
    "        decoding_token_index += 1\n",
    "        if all(stop_flag):\n",
    "            break\n",
    "    log_likelihoods = torch.cat(step_log_likelihoods, dim=-1)\n",
    "    # mask padding for tgt\n",
    "    tgt_pad_mask = sequence_mask(tgt_true_len).float()\n",
    "    log_likelihoods = log_likelihoods*tgt_pad_mask[:,:log_likelihoods.size(1)]\n",
    "    loss += -(log_likelihoods.sum()/tgt_pad_mask.sum()).item()\n",
    "    tgt_pred.extend(tgt_pred_batch)\n",
    "    src_org.extend(src_org_batch)\n",
    "    tgt_org.extend(tgt_org_batch)\n",
    "loss = loss/len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4768"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tgt_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tx443/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from Multilayers_Decoder import sequence_mask\n",
    "\n",
    "eval_len = len(tgt_pred)\n",
    "precision = np.zeros((eval_len,))\n",
    "recall = np.zeros((eval_len,))\n",
    "Fscore = np.zeros((eval_len,))\n",
    "for i in range(eval_len):\n",
    "    org_facts = ''.join(tgt_org[i]).split('$')\n",
    "    pred_facts = ''.join(tgt_pred[i]).split('$')\n",
    "    pred_facts = list(set(pred_facts))\n",
    "    org_facts_num = len(org_facts)\n",
    "    pred_facts_num = len(pred_facts)\n",
    "    org_match_num = np.zeros((org_facts_num))\n",
    "    pred_match_num = np.zeros((pred_facts_num))\n",
    "    similarity_ma = np.zeros((org_facts_num, pred_facts_num))\n",
    "    for org_i in range(org_facts_num):\n",
    "        for pred_i in range(pred_facts_num):\n",
    "            similarity_ma[org_i, pred_i] = similarity_score(org_facts[org_i], pred_facts[pred_i])\n",
    "    row_ind, col_ind = linear_sum_assignment(-similarity_ma)\n",
    "    \n",
    "    for org_i, pred_i in zip(row_ind, col_ind):\n",
    "        org_fact = org_facts[org_i]\n",
    "        pred_fact = pred_facts[pred_i]\n",
    "        fact_same = check_fact_same(org_fact, pred_fact)\n",
    "        if fact_same:\n",
    "            org_match_num[org_i] = 1\n",
    "            pred_match_num[pred_i] = 1\n",
    "#     print(pred_match_num)\n",
    "#     print(org_match_num)\n",
    "    precision[i] = pred_match_num.mean()\n",
    "    recall[i] = org_match_num.mean()\n",
    "    Fscore[i] = 2*precision[i]*recall[i]/(precision[i]+recall[i])\n",
    "if False:\n",
    "    random_sample = np.random.randint(eval_len)\n",
    "    print('src: ', src_org[random_sample])\n",
    "    print('Ref: ', tgt_org[random_sample])\n",
    "    print('pred: ', tgt_pred[random_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0291599252956 0.0324864173857 nan\n"
     ]
    }
   ],
   "source": [
    "print(precision.mean(), recall.mean(), Fscore.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision1 = precision\n",
    "recall1 = recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tx443/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "Fscore = np.zeros((eval_len,))\n",
    "for i in range(eval_len):\n",
    "    Fscore[i] = 2*precision[i]*recall[i]/(precision[i]+recall[i]+e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.16666667,  0.16666667,  0.        ,\n",
       "        0.        ,  0.        ,  0.2       ,  0.125     ,  0.5       ,\n",
       "        0.2       ,  0.2       ,  0.25      ,  0.        ,  0.09090909,\n",
       "        0.        ,  0.14285714,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.2       ,\n",
       "        0.        ,  0.5       ,  0.33333333,  0.        ,  0.        ,\n",
       "        0.        ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
       "        1.        ,  0.5       ,  1.        ,  1.        ,  1.        ,\n",
       "        0.5       ,  1.        ,  0.        ,  1.        ,  0.        ,\n",
       "        1.        ,  0.        ,  0.        ,  0.5       ,  1.        ,\n",
       "        1.        ,  0.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "        1.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "        0.5       ,  0.        ,  0.16666667,  0.2       ,  1.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.33333333,\n",
       "        0.66666667,  0.25      ,  0.        ,  0.4       ,  0.25      ,\n",
       "        0.33333333,  0.5       ,  1.        ,  0.        ,  0.33333333,\n",
       "        0.        ,  0.5       ,  0.        ,  0.        ,  0.66666667,\n",
       "        1.        ,  0.        ,  0.        ,  0.5       ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src:  华夏投资自创业初始就确立了“以人为本”的人才战略，十分注重对人才的培养与使用，它的人力资源政策主要是不断创造出“事业留人、待遇留人、感情留人”的亲情化企业氛围，让员工在华夏投资有成就感和归属感。\n",
      "Ref:  华夏投资@确立了@“以人为本”的人才战略$华夏投资@十分注重对X的使用@人才$华夏投资的人力资源政策@主要是不断创造出@亲情化企业氛围$亲情化企业氛围@DESC@[“事业留人|待遇留人|感情留人”]的$亲情化企业氛围@让X有归属感@员工\n",
      "pred:  华夏投资@不断遇@[“十分化|感情|感情]$[“十分”|“十分化]@对X留了@[感情|“十分化]$[自养”|“十分化]@对X为“十分化”|“十分化”|“十分化”]$夏投资源”@DESC@[“十分化]$[自养”|“十分@DESC@[“十分化|“十分化]$[自养”|“十分化]\n"
     ]
    }
   ],
   "source": [
    "random_sample = 0\n",
    "print('src: ', ''.join(src_org[random_sample]))\n",
    "print('Ref: ', ''.join(tgt_org[random_sample]))\n",
    "print('pred: ', ''.join(tgt_pred[random_sample]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(src_org[random_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7741935483870968"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = '企业@大力开发@自主知识产权'\n",
    "t2= '_@大力开发@自主知识产权的新产品'\n",
    "difflib.SequenceMatcher(None, t1, t2).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.    1.    0.75]\n"
     ]
    }
   ],
   "source": [
    "t1_ele = t1.split('@')\n",
    "t2_ele = t2.split('@')\n",
    "ele_num = len(t1_ele)\n",
    "ele_sim = np.zeros((ele_num,))\n",
    "for ele_i in range(ele_num):\n",
    "    ele_sim[ele_i] = difflib.SequenceMatcher(None,t1_ele[ele_i],t2_ele[ele_i]).ratio()\n",
    "print(ele_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
