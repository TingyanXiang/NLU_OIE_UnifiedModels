{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import jieba\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('Machine_Translation_NLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saoke = []\n",
    "# with open('/scratch/tx443/NLU/project/SAOKE_DATA.json', 'r') as f:\n",
    "#     for line in f:\n",
    "#         saoke.append(json.loads(line))\n",
    "        \n",
    "# # preprocess saoke data \n",
    "# data = []\n",
    "# for sample in saoke:\n",
    "#     # remove some exceptions with empty facts\n",
    "#     if sample['logic'] == []:\n",
    "#         continue\n",
    "#     # tokenize src sentence\n",
    "#     sample_processed = dict()\n",
    "#     sample_processed['src_org'] = sample['natural']\n",
    "#     sample_processed['src'] = list(jieba.cut(sample['natural'], cut_all=False))\n",
    "    \n",
    "#     # transform fact list into str and tokenize\n",
    "#     # $ separates facts; @ separate elements for one fact; & separate objects for one fact\n",
    "#     sample_processed['tgt_org'] = sample['logic']\n",
    "#     logic_list = []\n",
    "#     for fact in sample['logic']:\n",
    "#         logic_list.append('@'.join([fact['subject'], replaceMisspred(fact['predicate']), \n",
    "#                                    '&'.join(fact['object'])]))\n",
    "#     logic_str = '$'.join(logic_list)\n",
    "#     sample_processed['tgt'] = list(jieba.cut(logic_str, cut_all=False))\n",
    "    \n",
    "#     data.append(sample_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我', '是', '说', '的', 'jks', '大', '100', '方']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_segmentation('我是说的jks大100方')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceMisspred(predicate):\n",
    "    '''replace missing predicate\n",
    "    '''\n",
    "    if predicate == '_':\n",
    "        return 'P'\n",
    "    else:\n",
    "        return predicate\n",
    "    \n",
    "def character_segmentation(string):\n",
    "    res = []\n",
    "    for part in list(jieba.cut(string, cut_all=False)):\n",
    "        if re.match('^[\\da-zA-Z]+$', part):\n",
    "            res.append(part)\n",
    "        else:\n",
    "            res.extend(list(part))\n",
    "    return res\n",
    "\n",
    "# def replaceMissinfo(aaa):\n",
    "#     '''replace missing info for subjects/objects\n",
    "#     '''\n",
    "#     placeholder = ['Z','Y','X']\n",
    "#     for i in range(len(aaa)):\n",
    "#         if aaa[i] == '_':\n",
    "#             aaa = aaa[:i] + placeholder.pop() + aaa[i+1:]\n",
    "#     return aaa\n",
    "\n",
    "def load_preprocess_data(data_add):\n",
    "    saoke = []\n",
    "    with open(data_add, 'r') as f:\n",
    "        for line in f:\n",
    "            saoke.append(json.loads(line))\n",
    "    data = []\n",
    "    # list of dict\n",
    "    for sample in saoke:\n",
    "        # remove some exceptions with empty facts\n",
    "        if sample['logic'] == []:\n",
    "            continue\n",
    "        # tokenize src sentence\n",
    "        sample_processed = dict()\n",
    "        sample_processed['src_org'] = sample['natural']\n",
    "        #sample_processed['src'] = list(jieba.cut(sample['natural'], cut_all=False))\n",
    "        sample_processed['src'] = character_segmentation(sample['natural'])\n",
    "        \n",
    "        # transform fact list into str and tokenize\n",
    "        # $ separates facts; @ separate elements for one fact; & separate objects for one fact\n",
    "        sample_processed['tgt_org'] = sample['logic']\n",
    "        logic_list = []\n",
    "        for fact in sample['logic']:\n",
    "            logic_list.append('@'.join([fact['subject'], replaceMisspred(fact['predicate']), \n",
    "                                       '&'.join(fact['object'])]))\n",
    "        logic_str = '$'.join(logic_list)\n",
    "        sample_processed['tgt'] = character_segmentation(logic_str)\n",
    "        #list(jieba.cut(logic_str, cut_all=False))\n",
    "\n",
    "        data.append(sample_processed)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_add = '/scratch/tx443/NLU/project/SAOKE_DATA.json'\n",
    "data = load_preprocess_data(data_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train val test\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import dropwhile\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name, emb_pretrained_add=None, max_vocab_size=None):\n",
    "        self.name = name\n",
    "        self.word2index = None #{\"$PAD$\": PAD_token, \"$SOS$\": SOS_token, \"$EOS$\": EOS_token, \"$UNK$\": UNK_token}\n",
    "        #self.word2count = None #{\"$PAD$\": 0, \"$SOS$\" : 0, \"$EOS$\": 0, \"$UNK$\": 0}\n",
    "        self.index2word = None #{PAD_token: \"$PAD$\", SOS_token: \"$SOS$\", EOS_token: \"$EOS$\", UNK_token: \"$UNK$\"}\n",
    "        self.max_vocab_size = max_vocab_size  # Count SOS and EOS\n",
    "        self.vocab_size = None\n",
    "        self.emb_pretrained_add = emb_pretrained_add\n",
    "        self.embedding_matrix = None\n",
    "\n",
    "    def build_vocab(self, data):\n",
    "        all_tokens = []\n",
    "        for sample in data:\n",
    "            all_tokens.extend(sample['src'])\n",
    "            all_tokens.extend(sample['tgt'])  \n",
    "        token_counter = Counter(all_tokens)\n",
    "        print('The number of unique tokens totally in dataset: ', len(token_counter))\n",
    "        # remove word with freq==1 \n",
    "        for key, count in dropwhile(lambda key_count: key_count[1] > 1, token_counter.most_common()):\n",
    "            del token_counter[key]\n",
    "        \n",
    "        if self.max_vocab_size:\n",
    "            vocab, count = zip(*token_counter.most_common(self.max_vocab_size))\n",
    "        else:\n",
    "            vocab, count = zip(*token_counter.most_common())\n",
    "        \n",
    "        self.index2word = vocab_prefix + list(vocab)\n",
    "        word2index = dict(zip(self.index2word, range(0, len(self.index2word)))) \n",
    "#         word2index = dict(zip(vocab, range(len(vocab_prefix),len(vocab_prefix)+len(vocab)))) \n",
    "#         for idx, token in enumerate(vocab_prefix):\n",
    "#             word2index[token] = idx\n",
    "        self.word2index = word2index\n",
    "        self.vocab_size = len(self.index2word)\n",
    "        return None \n",
    "\n",
    "    def build_emb_weight(self):\n",
    "        words_emb_dict = load_emb_vectors(self.emb_pretrained_add)\n",
    "        emb_weight = np.zeros([self.vocab_size, 300])\n",
    "        for i in range(len(vocab_prefix), self.vocab_size):\n",
    "            emb = words_emb_dict.get(self.index2word[i], None)\n",
    "            if emb is not None:\n",
    "                try:\n",
    "                    emb_weight[i] = emb\n",
    "                except:\n",
    "                    pass\n",
    "                    #print(len(emb), self.index2word[i], emb)\n",
    "        self.embedding_matrix = emb_weight\n",
    "        return None\n",
    "\n",
    "def load_emb_vectors(fasttest_home):\n",
    "    max_num_load = 500000\n",
    "    words_dict = {}\n",
    "    with open(fasttest_home) as f:\n",
    "        for num_row, line in enumerate(f):\n",
    "            if num_row >= max_num_load:\n",
    "                break\n",
    "            s = line.split()\n",
    "            words_dict[s[0]] = np.asarray(s[1:])\n",
    "    return words_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import vocab_pred, vocab_pred_size, vocab_prefix\n",
    "from config import UNK_index, PAD_index, SOS_index, EOS_index \n",
    "from config import OOV_pred_index, PAD_pred_index, EOS_pred_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2index(data, key, word2index):\n",
    "    '''\n",
    "    transform tokens into index as input for both src and tgt\n",
    "    '''\n",
    "    indexdata = []\n",
    "    for line in data:\n",
    "        line = line[key]\n",
    "        indexdata.append([word2index[c] if c in word2index.keys() else UNK_index for c in line])\n",
    "        #indexdata[-1].append(EOS_index)\n",
    "    print('finish indexing')\n",
    "    return indexdata\n",
    "\n",
    "def construct_Lang(name, data, emb_pretrained_add = None, max_vocab_size = None):\n",
    "    lang = Lang(name, emb_pretrained_add, max_vocab_size)\n",
    "    lang.build_vocab(data)\n",
    "    if emb_pretrained_add:\n",
    "        lang.build_emb_weight()\n",
    "    return lang\n",
    "\n",
    "def text2symbolindex(data, key, word2index):\n",
    "    '''get generation label for tgt \n",
    "    '''\n",
    "    indexdata = []\n",
    "    for line in data:\n",
    "        line = line[key]\n",
    "        indexdata.append([word2index[c] if c in word2index.keys() else OOV_pred_index for c in line])\n",
    "        #indexdata[-1].append(EOS_index)\n",
    "    print('symbol label finish')\n",
    "    return indexdata\n",
    "\n",
    "def copy_indicator(data, src_key='src', tgt_key='tgt'):\n",
    "    '''get copy label for tgt\n",
    "    '''\n",
    "    indicator = []\n",
    "    for sample in data:\n",
    "        tgt = sample[tgt_key]\n",
    "        src = sample[src_key]\n",
    "        matrix = np.zeros((len(tgt), len(src)), dtype=int)\n",
    "        for m in range(len(tgt)):\n",
    "            for n in range(len(src)):\n",
    "                if tgt[m] == src[n]:\n",
    "                    matrix[m,n] = 1\n",
    "        indicator.append(matrix)\n",
    "    return indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique tokens totally in dataset:  9364\n"
     ]
    }
   ],
   "source": [
    "# build vocab from train for input indexing\n",
    "trainLang = construct_Lang('train', train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish indexing\n",
      "finish indexing\n"
     ]
    }
   ],
   "source": [
    "train_src_input_index = text2index(train_data, 'src', trainLang.word2index) \n",
    "val_src_input_index = text2index(val_data, 'src', trainLang.word2index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish indexing\n",
      "finish indexing\n"
     ]
    }
   ],
   "source": [
    "train_tgt_input_index = text2index(train_data, 'tgt', trainLang.word2index) \n",
    "val_tgt_input_index = text2index(val_data, 'tgt', trainLang.word2index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_pred = ['<PAD>','<OOV>','<EOS>','ISA','DESC','IN','BIRTH',\"DEATH\", \n",
    "#               \"=\",\"$\", \"[\",\"]\",\"|\",\"X\",\"Y\",\"Z\",\"P\",\"@\", \"&\"]\n",
    "# vocab_pred_size = len(vocab_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbol_index = []\n",
    "# for o in symbol_dict:\n",
    "#     symbol_index.append(trainLang.word2index[o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build generation vocab for prediction\n",
    "word2symbolindex = {}\n",
    "for idx, token in enumerate(vocab_pred):\n",
    "        word2symbolindex[token] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(UNK_index==trainLang.word2index['<UNK>'])\n",
    "assert(PAD_index==trainLang.word2index['<PAD>'])\n",
    "assert(SOS_index==trainLang.word2index['<SOS>'])\n",
    "assert(EOS_index==trainLang.word2index['<EOS>'])\n",
    "\n",
    "assert(OOV_pred_index==word2symbolindex['<OOV>'])\n",
    "assert(PAD_pred_index==word2symbolindex['<PAD>'])\n",
    "assert(EOS_pred_index==word2symbolindex['<EOS>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symbol label finish\n",
      "symbol label finish\n"
     ]
    }
   ],
   "source": [
    "# get generation label\n",
    "train_label_symbolindex = text2symbolindex(train_data, 'tgt', word2symbolindex)\n",
    "val_label_symbolindex = text2symbolindex(val_data, 'tgt', word2symbolindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check valid\n",
    "# for idx,i in enumerate(label_input_symbolindex[0]):\n",
    "#     if symbol_dict[i] == 'OOV':\n",
    "#         continue\n",
    "#     elif symbol_dict[i] == labels[0][idx]:\n",
    "#         continue\n",
    "#     else:\n",
    "#         print(symbol_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indicator = copy_indicator(train_data, 'src', 'tgt')\n",
    "val_indicator = copy_indicator(val_data, 'src', 'tgt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 28\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data[0]['src']), len(train_data[0]['tgt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计 算 机 操 作 与 信 息 处 理 系 统 使 用 媒 介 的 保 管 等 。 _ @ 保 管 @ 媒 介 $ [ 计 算 机 操 作 | 信 息 处 理 系 统 ] @ 使 用 @ 媒 介\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(train_data[0]['src']), ' '.join(train_data[0]['tgt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indicator[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28564, 28564, 28564, 28564, 28564)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_src_input_index),len(train_tgt_input_index),len(train_label_symbolindex),len(train_indicator),len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6121, 6121, 6121, 6121, 6121)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_src_input_index),len(val_tgt_input_index),len(val_label_symbolindex),len(val_indicator),len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class VocabDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, src_index, tgt_index, tgt_symbolindex, tgt_indicator, data, src_clip=None, tgt_clip=None):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.src_clip = src_clip\n",
    "        self.tgt_clip = tgt_clip\n",
    "        self.src_list, self.tgt_list = src_index, tgt_index\n",
    "        self.data = data\n",
    "        self.tgt_symbolindex, self.tgt_indicator  = tgt_symbolindex, tgt_indicator\n",
    "        \n",
    "        assert (len(self.src_list) == len(self.tgt_list) == len(self.tgt_symbolindex)== len(self.tgt_indicator))\n",
    "        #self.word2index = word2index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        src = self.src_list[key]\n",
    "        tgt = self.tgt_list[key]\n",
    "        src_org = self.data[key]['src']\n",
    "        tgt_org = self.data[key]['tgt']\n",
    "        tgt_sym = self.tgt_symbolindex[key]\n",
    "        tgt_ind = self.tgt_indicator[key]\n",
    "        \n",
    "        if self.src_clip is not None:\n",
    "            src = src[:self.src_clip]\n",
    "            src_org = src_org[:self.src_clip]\n",
    "            tgt_ind = tgt_ind[:,:self.src_clip]\n",
    "        src_length = len(src)\n",
    "\n",
    "        if self.tgt_clip is not None:\n",
    "            tgt = tgt[:self.tgt_clip]\n",
    "            tgt_org = tgt_org[:self.tgt_clip]\n",
    "            tgt_sym = tgt_sym[:self.tgt_clip]\n",
    "            tgt_ind = tgt_ind[:self.tgt_clip,:]\n",
    "        tgt_length = len(tgt)\n",
    "        \n",
    "        return src, src_length, tgt, tgt_length, tgt_sym, tgt_ind, src_org, tgt_org\n",
    "        \n",
    "        #return src_org, src_tensor, src_true_len, tgt_org, tgt_tensor, tgt_true_len, tgt_label_vocab, tgt_label_copy \n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    src_list = []\n",
    "    tgt_list = []\n",
    "    src_length_list = []\n",
    "    tgt_length_list = []\n",
    "    tgt_symbol_list = []\n",
    "    tgt_indicator_list = []\n",
    "    src_org_list = []\n",
    "    tgt_org_list = []\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for datum in batch:\n",
    "        src_length_list.append(datum[1]+1) # 不用加1；eos不算\n",
    "        tgt_length_list.append(datum[3]+1) \n",
    "    \n",
    "    batch_max_src_length = np.max(src_length_list)\n",
    "    batch_max_tgt_length = np.max(tgt_length_list)\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]+[EOS_index]),\n",
    "                                pad_width=((0, batch_max_src_length-datum[1]-1)),\n",
    "                                mode=\"constant\", constant_values=PAD_index)\n",
    "        src_list.append(padded_vec)\n",
    "        \n",
    "        padded_vec = np.pad(np.array(datum[2]+[EOS_index]),\n",
    "                                pad_width=((0, batch_max_tgt_length-datum[3]-1)),\n",
    "                                mode=\"constant\", constant_values=PAD_index)\n",
    "        tgt_list.append(padded_vec)\n",
    "        \n",
    "        padded_vec = np.pad(np.array(datum[4]+[EOS_pred_index]),\n",
    "                                pad_width=((0, batch_max_tgt_length-datum[3]-1)),\n",
    "                                mode=\"constant\", constant_values=PAD_pred_index)\n",
    "        tgt_symbol_list.append(padded_vec)\n",
    "        \n",
    "        indicator = np.pad(datum[5], pad_width=((0,1),(0,1)), \n",
    "                           mode='constant', constant_values=0)\n",
    "        indicator[-1,-1] = 1\n",
    "        padded_vec = np.pad(indicator,\n",
    "                            pad_width=((0, batch_max_tgt_length-datum[3]-1),((0, batch_max_src_length-datum[1]-1))),\n",
    "                            mode=\"constant\", constant_values=0)\n",
    "        #print(padded_vec.dtype, padded_vec.shape)\n",
    "        tgt_indicator_list.append(padded_vec)\n",
    "        \n",
    "        src_org_list.append(datum[6])\n",
    "        tgt_org_list.append(datum[7])\n",
    "    \n",
    "    # re-order\n",
    "    ind_dec_order = np.argsort(src_length_list)[::-1]\n",
    "    \n",
    "    src_list = np.array(src_list)[ind_dec_order]\n",
    "    src_length_list = np.array(src_length_list)[ind_dec_order]\n",
    "    tgt_list = np.array(tgt_list)[ind_dec_order]\n",
    "    tgt_length_list = np.array(tgt_length_list)[ind_dec_order]\n",
    "    tgt_symbol_list = np.array(tgt_symbol_list)[ind_dec_order]\n",
    "    #print(tgt_indicator_list[0].dtype, tgt_indicator_list[0][:5][:5])\n",
    "    tgt_indicator_list = np.array(tgt_indicator_list)[ind_dec_order]\n",
    "    #print(tgt_indicator_list.dtype, tgt_indicator_list.shape)\n",
    "    src_org_list = [src_org_list[i] for i in ind_dec_order]\n",
    "    tgt_org_list = [tgt_org_list[i] for i in ind_dec_order]\n",
    "    \n",
    "    #print(type(np.array(data_list)),type(np.array(label_list)))\n",
    "    \n",
    "    return [torch.from_numpy(src_list).to(device), \n",
    "            torch.LongTensor(src_length_list).to(device), \n",
    "            torch.from_numpy(tgt_list).to(device), \n",
    "            torch.LongTensor(tgt_length_list).to(device),\n",
    "            torch.from_numpy(tgt_symbol_list).to(device),\n",
    "            torch.from_numpy(tgt_indicator_list).to(device),\n",
    "            src_org_list,\n",
    "            tgt_org_list,           \n",
    "           ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for src_tensor, src_true_len, tgt_tensor, tgt_true_len, tgt_label_vocab, tgt_label_copy, src_org, tgt_org in train_loader:\n",
    "#     print(src_tensor[:10])\n",
    "#     print(src_true_len)\n",
    "#     print(tgt_tensor[:10])\n",
    "#     print(tgt_true_len)\n",
    "#     print(tgt_label_vocab[:10])\n",
    "#     print(tgt_label_copy[:10,:10])\n",
    "#     print(src_org[:10])\n",
    "#     print(tgt_org[:10])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "# from Data_utils import VocabDataset, vocab_collate_func\n",
    "# from preprocessing_util import preposs_toekn, Lang, text2index, construct_Lang\n",
    "from Multilayers_Encoder import EncoderRNN\n",
    "from Multilayers_Decoder import DecoderAtten, sequence_mask\n",
    "from config import device, embedding_freeze\n",
    "import random\n",
    "from evaluation import evaluate_batch, evaluate_beam_batch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(src_data, tgt_data, encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
    "          teacher_forcing_ratio, vocab, vocab_pred_size):\n",
    "    src_org_batch, src_tensor, src_true_len = src_data\n",
    "    tgt_org_batch, tgt_tensor, tgt_label_vocab, tgt_label_copy, tgt_true_len = tgt_data\n",
    "    '''\n",
    "    finish train for a batch\n",
    "    '''\n",
    "    batch_size = src_tensor.size(0)\n",
    "    encoder_hidden, encoder_cell = encoder.initHidden(batch_size)\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    loss = 0\n",
    "    encoder_outputs, encoder_hidden, encoder_cell = encoder(src_tensor, encoder_hidden, src_true_len, encoder_cell)\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_index]*batch_size], device=device).transpose(0,1)\n",
    "    decoder_hidden, decoder_cell = encoder_hidden, decoder.initHidden(batch_size)\n",
    "    step_log_likelihoods = []\n",
    "    #print(decoder_hidden.size())\n",
    "    #print('encoddddddddddder finishhhhhhhhhhhhhhh')\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        ### Teacher forcing: Feed the target as the next input\n",
    "        decoding_token_index = 0\n",
    "        tgt_max_len_batch = tgt_true_len.cpu().max().item()\n",
    "        assert(tgt_max_len_batch==tgt_tensor.size(1))\n",
    "        while decoding_token_index < tgt_max_len_batch:\n",
    "            decoder_output, decoder_hidden, decoder_attention, decoder_cell = decoder(\n",
    "                decoder_input, decoder_hidden, src_true_len, encoder_outputs, decoder_cell)\n",
    "\n",
    "            decoding_label_vocab = tgt_label_vocab[:, decoding_token_index]\n",
    "            decoding_label_copy = tgt_label_copy[:, decoding_token_index, :]\n",
    "            copy_log_probs = decoder_output[:, vocab_pred_size:]+(decoding_label_copy.float()+1e-45).log()\n",
    "            #mask sample which is copied only\n",
    "            gen_mask = ((decoding_label_vocab!=OOV_pred_index) | (decoding_label_copy.sum(-1)==0)).float() \n",
    "            log_gen_mask = (gen_mask + 1e-45).log().unsqueeze(-1)\n",
    "            #mask log_prob value for oov_pred_index when label_vocab==oov_pred_index and is copied \n",
    "            generation_log_probs = decoder_output.gather(1, decoding_label_vocab.unsqueeze(1)) + log_gen_mask\n",
    "            combined_gen_and_copy = torch.cat((generation_log_probs, copy_log_probs), dim=-1)\n",
    "            step_log_likelihood = torch.logsumexp(combined_gen_and_copy, dim=-1)\n",
    "            step_log_likelihoods.append(step_log_likelihood.unsqueeze(1))\n",
    "            #loss += criterion(decoder_output, tgt_tensor[:,decoding_token_index])\n",
    "            decoder_input = tgt_tensor[:,decoding_token_index].unsqueeze(1)  # Teacher forcing\n",
    "            decoding_token_index += 1\n",
    "\n",
    "    else:\n",
    "        ### Without teacher forcing: use its own predictions as the next input\n",
    "        decoding_token_index = 0\n",
    "        tgt_max_len_batch = tgt_true_len.cpu().max().item()\n",
    "        assert(tgt_max_len_batch==tgt_tensor.size(1))\n",
    "        while decoding_token_index < tgt_max_len_batch:\n",
    "            decoder_output, decoder_hidden, decoder_attention_weights, decoder_cell = decoder(\n",
    "                decoder_input, decoder_hidden, src_true_len, encoder_outputs, decoder_cell)\n",
    "\n",
    "            decoding_label_vocab = tgt_label_vocab[:, decoding_token_index]\n",
    "            decoding_label_copy = tgt_label_copy[:, decoding_token_index, :]\n",
    "            copy_log_probs = decoder_output[:, vocab_pred_size:]+(decoding_label_copy.float()+1e-45).log()\n",
    "            #mask sample which is copied only\n",
    "            gen_mask = ((decoding_label_vocab!=OOV_pred_index) | (decoding_label_copy.sum(-1)==0)).float() \n",
    "            log_gen_mask = (gen_mask + 1e-45).log().unsqueeze(-1)\n",
    "            #mask log_prob value for oov_pred_index when label_vocab==oov_pred_index and is copied \n",
    "            generation_log_probs = decoder_output.gather(1, decoding_label_vocab.unsqueeze(1)) + log_gen_mask\n",
    "            combined_gen_and_copy = torch.cat((generation_log_probs, copy_log_probs), dim=-1)\n",
    "            step_log_likelihood = torch.logsumexp(combined_gen_and_copy, dim=-1)\n",
    "            step_log_likelihoods.append(step_log_likelihood.unsqueeze(1))\n",
    "\n",
    "            topv, topi = copy_log_probs.topk(1, dim=-1)\n",
    "            next_input = topi.detach().cpu().squeeze(1)\n",
    "            decoder_input = []\n",
    "            for i_batch in range(batch_size):\n",
    "                pred_list = vocab_pred+src_org_batch[i_batch]\n",
    "                next_input_token = pred_list[next_input[i_batch].item()]\n",
    "                decoder_input.append(vocab.word2index.get(next_input_token, UNK_index))\n",
    "            decoder_input = torch.tensor(decoder_input, device=device).unsqueeze(1)\n",
    "            #loss += criterion(decoder_output, tgt_tensor[:,decoding_token_index])\n",
    "            #topv, topi = decoder_output.topk(1)\n",
    "            #decoder_input = topi.detach()  # detach from history as input\n",
    "            decoding_token_index += 1\n",
    "\n",
    "    # average loss\n",
    "    log_likelihoods = torch.cat(step_log_likelihoods, dim=-1)\n",
    "    # mask padding for tgt\n",
    "    tgt_pad_mask = sequence_mask(tgt_true_len).float()\n",
    "    log_likelihoods = log_likelihoods*tgt_pad_mask\n",
    "    loss = -log_likelihoods.sum()/batch_size\n",
    "    loss.backward()\n",
    "\n",
    "    ### TODO\n",
    "    # clip for gradient exploding \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return (loss*batch_size/tgt_pad_mask.sum()).item() #torch.div(loss, tgt_true_len.type_as(loss).mean()).item()  #/tgt_true_len.mean()\n",
    "\n",
    "\n",
    "def trainIters(train_loader, val_loader, encoder, decoder, num_epochs, learning_rate, \n",
    "               teacher_forcing_ratio, model_save_info, tgt_max_len, beam_size, vocab):\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    if model_save_info['model_path_for_resume'] is not None:\n",
    "        check_point_state = torch.load(model_save_info['model_path_for_resume'])\n",
    "        encoder.load_state_dict(check_point_state['encoder_state_dict'])\n",
    "        encoder_optimizer.load_state_dict(check_point_state['encoder_optimizer_state_dict'])\n",
    "        decoder.load_state_dict(check_point_state['decoder_state_dict'])\n",
    "        decoder_optimizer.load_state_dict(check_point_state['decoder_optimizer_state_dict'])\n",
    "\n",
    "    for epoch in range(num_epochs): \n",
    "        #teacher_forcing_ratio *= 0.98\n",
    "        n_iter = -1\n",
    "        losses = np.zeros((len(train_loader),))\n",
    "        start_time = time.time()\n",
    "        for src_tensor, src_true_len, tgt_tensor, tgt_true_len, tgt_label_vocab, tgt_label_copy, src_org_batch, tgt_org_batch in train_loader:\n",
    "            n_iter += 1\n",
    "            #print('start_step: ', n_iter)\n",
    "            src_data = (src_org_batch, src_tensor, src_true_len)\n",
    "            tgt_data = (tgt_org_batch, tgt_tensor, tgt_label_vocab, tgt_label_copy, tgt_true_len)\n",
    "            loss = train(src_data, tgt_data, encoder, decoder, encoder_optimizer, \n",
    "                         decoder_optimizer, teacher_forcing_ratio, vocab, vocab_pred_size)\n",
    "            losses[n_iter] = loss\n",
    "            if n_iter % 500 == 0:\n",
    "                pass\n",
    "                #print('Loss:', loss)\n",
    "                #eva_start = time.time()\n",
    "#                 precision, recall, val_loss = evaluate_batch(val_loader, encoder, decoder, tgt_max_len, vocab, vocab_pred_size)\n",
    "#                 #print((time.time()-eva_start)/60)\n",
    "#                 print('epoch: [{}/{}], step: [{}/{}], train_loss:{}, val_precision: {}, val_recall: {}, val_loss: {}'.format(\n",
    "#                     epoch, num_epochs, n_iter, len(train_loader), loss, precision.mean(), recall.mean(), val_loss))\n",
    "               # print('Decoder parameters grad:')\n",
    "               # for p in decoder.named_parameters():\n",
    "               #     print(p[0], ': ',  p[1].grad.data.abs().mean().item(), p[1].grad.data.abs().max().item(), p[1].data.abs().mean().item(), p[1].data.abs().max().item(), end=' ')\n",
    "               # print('\\n')\n",
    "               # print('Encoder Parameters grad:')\n",
    "               # for p in encoder.named_parameters():\n",
    "               #     print(p[0], ': ',  p[1].grad.data.abs().mean().item(), p[1].grad.data.abs().max().item(), p[1].data.abs().mean().item(), p[1].data.abs().max().item(), end=' ')\n",
    "               # print('\\n')\n",
    "        precision, recall, val_loss = evaluate_batch(val_loader, encoder, decoder, tgt_max_len, vocab, vocab_pred_size)\n",
    "        print('epoch: [{}/{}], step: [{}/{}], train_loss:{}, val_precision: {}, val_recall: {}, val_loss: {}'.format(\n",
    "            epoch, num_epochs, n_iter, len(train_loader), losses.mean(), precision.mean(), recall.mean(), val_loss))\n",
    "        #val_bleu_sacre_beam, _, _ = evaluate_beam_batch(beam_size, val_loader, encoder, decoder, criterion, tgt_max_len, tgtLang.index2word)\n",
    "        #print('epoch: [{}/{}] (Running time {:.3f} min), val_bleu_sacre_beam: {}'.format(epoch, num_epochs, (time.time()-start_time)/60, val_bleu_sacre_beam))\n",
    "#         if max_val_bleu < val_bleu_sacre.score:\n",
    "#             max_val_bleu = val_bleu_sacre.score\n",
    "            ### TODO save best model\n",
    "#         if (epoch+1) % model_save_info['epochs_per_save_model'] == 0:\n",
    "#             check_point_state = {\n",
    "#                 'epoch': epoch,\n",
    "#                 'encoder_state_dict': encoder.state_dict(),\n",
    "#                 'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "#                 'decoder_state_dict': decoder.state_dict(),\n",
    "#                 'decoder_optimizer_state_dict': decoder_optimizer.state_dict()\n",
    "#                 }\n",
    "#             torch.save(check_point_state, '{}epoch_{}.pth'.format(model_save_info['model_path'], epoch))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = dict( \n",
    "    tgt_max_len = 130,\n",
    "    max_src_len_dataloader =94,\n",
    "    max_tgt_len_dataloader =127,\n",
    "\n",
    "    emb_size = 200,\n",
    "    en_hidden_size = 128,\n",
    "    en_num_layers = 2,\n",
    "    en_num_direction = 2,\n",
    "    de_hidden_size = 256,\n",
    "    de_num_layers = 3,\n",
    "    #deal_bi = 'linear', #'linear', #{'linear', 'sum'} #linear layer en-hid to de_hid\n",
    "    rnn_type = 'GRU', # {LSTM, GRU}\n",
    "    attention_type = 'dot_prod', #'dot_prod', general, concat #dot-product need pre-process\n",
    "    teacher_forcing_ratio = 1,\n",
    "\n",
    "    learning_rate = 1e-3,\n",
    "    num_epochs = 40,\n",
    "    batch_size = 64, \n",
    "    beam_size = 5,\n",
    "    dropout_rate = 0.0,\n",
    "\n",
    "    model_save_info = dict(\n",
    "        model_path = 'nmt_models/model1/',\n",
    "        epochs_per_save_model = 2,\n",
    "        model_path_for_resume = None #'nmt_models/epoch_0.pth'\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_max_len = paras['tgt_max_len']\n",
    "max_src_len_dataloader = paras['max_src_len_dataloader']\n",
    "max_tgt_len_dataloader = paras['max_tgt_len_dataloader']\n",
    "\n",
    "teacher_forcing_ratio = paras['teacher_forcing_ratio']\n",
    "emb_size = paras['emb_size']\n",
    "en_hidden_size = paras['en_hidden_size']\n",
    "en_num_layers = paras['en_num_layers']\n",
    "en_num_direction = paras['en_num_direction']\n",
    "de_hidden_size = paras['de_hidden_size']\n",
    "de_num_layers = paras['de_num_layers']\n",
    "\n",
    "learning_rate = paras['learning_rate']\n",
    "num_epochs = paras['num_epochs']\n",
    "batch_size = paras['batch_size']\n",
    "rnn_type = paras['rnn_type']\n",
    "attention_type = paras['attention_type']\n",
    "beam_size = paras['beam_size']\n",
    "model_save_info = paras['model_save_info']\n",
    "dropout_rate = paras['dropout_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VocabDataset(train_src_input_index, train_tgt_input_index, \n",
    "                             train_label_symbolindex, train_indicator, train_data, \n",
    "                             max_src_len_dataloader, max_tgt_len_dataloader)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=vocab_collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "val_dataset = VocabDataset(val_src_input_index, val_tgt_input_index, \n",
    "                           val_label_symbolindex, val_indicator, val_data,\n",
    "                           max_src_len_dataloader, max_tgt_len_dataloader)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=vocab_collate_func,\n",
    "                                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tgt_max_len': 130, 'max_src_len_dataloader': 94, 'max_tgt_len_dataloader': 127, 'emb_size': 200, 'en_hidden_size': 128, 'en_num_layers': 2, 'en_num_direction': 2, 'de_hidden_size': 256, 'de_num_layers': 3, 'rnn_type': 'GRU', 'attention_type': 'dot_prod', 'teacher_forcing_ratio': 1, 'learning_rate': 0.001, 'num_epochs': 40, 'batch_size': 64, 'beam_size': 5, 'dropout_rate': 0.0, 'model_save_info': {'model_path': 'nmt_models/model1/', 'epochs_per_save_model': 2, 'model_path_for_resume': None}}\n",
      "dot_prod\n",
      "Encoder:\n",
      "EncoderRNN(\n",
      "  (embedding): Embedding(8776, 200)\n",
      "  (dropout): Dropout(p=0.0)\n",
      "  (gru): GRU(200, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (transform_en_hid): Linear(in_features=512, out_features=768, bias=False)\n",
      ")\n",
      "Decoder:\n",
      "DecoderAtten(\n",
      "  (dropout): Dropout(p=0.0)\n",
      "  (embedding): Embedding(8776, 200)\n",
      "  (gru): GRU(200, 256, num_layers=3, batch_first=True)\n",
      "  (atten): AttentionLayer()\n",
      "  (linear): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (copy_mech): CopyMechanism(\n",
      "    (generate_linear): Linear(in_features=256, out_features=20, bias=True)\n",
      "    (copy_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (LogSoftmax): LogSoftmax()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# make dir for saving models\n",
    "if not os.path.exists(model_save_info['model_path']):\n",
    "    os.makedirs(model_save_info['model_path'])\n",
    "### save model hyperparameters\n",
    "with open(model_save_info['model_path']+'model_params.pkl', 'wb') as f:\n",
    "    model_hyparams = paras\n",
    "    pickle.dump(model_hyparams, f)\n",
    "print(model_hyparams)\n",
    "\n",
    "# read all data\n",
    "### save srcLang and tgtLang\n",
    "\n",
    "#for src; keep original src_org and index based on vocab src_tensor\n",
    "\n",
    "#for tgt; vocab_pred_label, copy_label\n",
    "\n",
    "\n",
    "# test_dataset = VocabDataset(test_data)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "#                                            batch_size=BATCH_SIZE,\n",
    "#                                            collate_fn=vocab_collate_func,\n",
    "#                                            shuffle=False)\n",
    "\n",
    "# embedding_src_weight = torch.from_numpy(srcLang.embedding_matrix).type(torch.FloatTensor).to(device)\n",
    "# embedding_tgt_weight = torch.from_numpy(tgtLang.embedding_matrix).type(torch.FloatTensor).to(device)\n",
    "# print(embedding_src_weight.size(), embedding_tgt_weight.size())\n",
    "\n",
    "encoder = EncoderRNN(trainLang.vocab_size, emb_size, en_hidden_size, en_num_layers, \n",
    "                     en_num_direction, (de_num_layers, de_hidden_size), rnn_type=rnn_type, \n",
    "                     dropout_rate=dropout_rate)\n",
    "decoder = DecoderAtten(trainLang.vocab_size, vocab_pred_size, emb_size, de_hidden_size, \n",
    "                       de_num_layers, (en_num_layers, en_num_direction, en_hidden_size), \n",
    "                       rnn_type=rnn_type, atten_type=attention_type, \n",
    "                       dropout_rate=dropout_rate)\n",
    "\n",
    "\n",
    "encoder, decoder = encoder.to(device), decoder.to(device)\n",
    "print('Encoder:')\n",
    "print(encoder)\n",
    "print('Decoder:')\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: [0/40], step: [446/447], train_loss:1.8329041478884567, val_precision: 0.00032674399607907203, val_recall: 4.208066616169867e-05, val_loss: 4.244439636667569\n",
      "epoch: [1/40], step: [446/447], train_loss:0.843647823104389, val_precision: 0.00032674399607907203, val_recall: 4.208066616169867e-05, val_loss: 3.51235623160998\n",
      "epoch: [2/40], step: [446/447], train_loss:0.7503537124968749, val_precision: 0.00032674399607907203, val_recall: 4.208066616169867e-05, val_loss: 3.4568135564525924\n",
      "epoch: [3/40], step: [446/447], train_loss:0.7275948947174704, val_precision: 0.000490115994118608, val_recall: 6.930933250162134e-05, val_loss: 3.3650567630926767\n",
      "epoch: [4/40], step: [446/447], train_loss:0.6870638378114509, val_precision: 0.00032674399607907203, val_recall: 4.208066616169867e-05, val_loss: 3.086298326651255\n",
      "epoch: [5/40], step: [446/447], train_loss:0.643341842713772, val_precision: 0.00038120132875891735, val_recall: 6.930933250162134e-05, val_loss: 2.946455833812555\n",
      "epoch: [6/40], step: [446/447], train_loss:0.6326793579580533, val_precision: 0.000490115994118608, val_recall: 6.930933250162134e-05, val_loss: 2.9053206741809845\n",
      "epoch: [7/40], step: [446/447], train_loss:0.7065182022200335, val_precision: 0.000490115994118608, val_recall: 6.930933250162134e-05, val_loss: 2.648409361640612\n",
      "epoch: [8/40], step: [446/447], train_loss:0.7538006626719597, val_precision: 0.00032674399607907203, val_recall: 4.208066616169867e-05, val_loss: 2.541276010374228\n",
      "epoch: [9/40], step: [446/447], train_loss:0.7889875039891642, val_precision: 0.00040842999509884006, val_recall: 7.475506576960586e-05, val_loss: 2.725083520015081\n",
      "epoch: [10/40], step: [446/447], train_loss:0.8168229247699648, val_precision: 0.00032674399607907203, val_recall: 4.208066616169867e-05, val_loss: 2.3828082978725433\n",
      "epoch: [11/40], step: [446/447], train_loss:0.7465804026564229, val_precision: 0.00032674399607907203, val_recall: 4.208066616169867e-05, val_loss: 2.4055683637658754\n",
      "epoch: [12/40], step: [446/447], train_loss:0.7734818131198286, val_precision: 0.00040842999509884006, val_recall: 7.475506576960586e-05, val_loss: 2.376318039993445\n",
      "epoch: [13/40], step: [446/447], train_loss:0.7122814115261872, val_precision: 0.00032674399607907203, val_recall: 4.208066616169867e-05, val_loss: 2.351408580938975\n",
      "epoch: [14/40], step: [446/447], train_loss:0.6771427793657486, val_precision: 0.00032674399607907203, val_recall: 4.208066616169867e-05, val_loss: 2.256504343201717\n",
      "epoch: [15/40], step: [446/447], train_loss:0.725079153474812, val_precision: 0.00032674399607907203, val_recall: 4.208066616169867e-05, val_loss: 2.291473276913166\n",
      "epoch: [16/40], step: [446/447], train_loss:0.7400176955816197, val_precision: 0.00032674399607907203, val_recall: 4.208066616169867e-05, val_loss: 2.172605729351441\n",
      "epoch: [17/40], step: [446/447], train_loss:0.7756234951467322, val_precision: 0.00032674399607907203, val_recall: 4.208066616169867e-05, val_loss: 2.167250464359919\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-9431065f0299>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m trainIters(train_loader, val_loader, encoder, decoder, num_epochs, learning_rate, \n\u001b[0;32m----> 2\u001b[0;31m            teacher_forcing_ratio, model_save_info, tgt_max_len, beam_size, trainLang)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-65-e6b32cda1a38>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(train_loader, val_loader, encoder, decoder, num_epochs, learning_rate, teacher_forcing_ratio, model_save_info, tgt_max_len, beam_size, vocab)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mtgt_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtgt_org_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_label_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_label_copy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_true_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             loss = train(src_data, tgt_data, encoder, decoder, encoder_optimizer, \n\u001b[0;32m--> 125\u001b[0;31m                          decoder_optimizer, teacher_forcing_ratio, vocab, vocab_pred_size)\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-e6b32cda1a38>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(src_data, tgt_data, encoder, decoder, encoder_optimizer, decoder_optimizer, teacher_forcing_ratio, vocab, vocab_pred_size)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mdecoding_token_index\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtgt_max_len_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             decoder_output, decoder_hidden, decoder_attention, decoder_cell = decoder(\n\u001b[0;32m---> 34\u001b[0;31m                 decoder_input, decoder_hidden, src_true_len, encoder_outputs, decoder_cell)\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mdecoding_label_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt_label_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoding_token_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/tx443/NLU/project/NLU_OIE_UnifiedModels/Machine_Translation_NLP/Multilayers_Decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt_input, hidden, true_len, encoder_outputs, cell)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# add copy mechanism here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mprob_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_mech\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/tx443/NLU/project/NLU_OIE_UnifiedModels/Machine_Translation_NLP/Multilayers_Decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, logits, encoder_outputs, true_len)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mcopy_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(bz, src_sen_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# mask copy_scores for padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mmask_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mcopy_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmask_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/tx443/NLU/project/NLU_OIE_UnifiedModels/Machine_Translation_NLP/Multilayers_Decoder.py\u001b[0m in \u001b[0;36msequence_mask\u001b[0;34m(lengths)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msequence_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainIters(train_loader, val_loader, encoder, decoder, num_epochs, learning_rate, \n",
    "           teacher_forcing_ratio, model_save_info, tgt_max_len, beam_size, trainLang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainIters(train_loader, val_loader, encoder, decoder, num_epochs, learning_rate, \n",
    "#            teacher_forcing_ratio, model_save_info, tgt_max_len, beam_size, trainLang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset1 = VocabDataset(val_src_input_index, val_tgt_input_index, \n",
    "                             val_label_symbolindex, val_indicator, val_data)\n",
    "val_loader1 = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                               batch_size=2,\n",
    "                                               collate_fn=vocab_collate_func,\n",
    "                                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "from Multilayers_Decoder import sequence_mask\n",
    "\n",
    "tgt_max_length = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    \n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "tgt_pred = []\n",
    "src_org = []\n",
    "tgt_org = []\n",
    "loss = 0\n",
    "loader = train_loader\n",
    "\n",
    "for src_tensor, src_true_len, tgt_tensor, tgt_true_len, tgt_label_vocab, tgt_label_copy, src_org_batch, tgt_org_batch in loader:\n",
    "    batch_size = src_tensor.size(0)\n",
    "    encoder_hidden, encoder_cell = encoder.initHidden(batch_size)\n",
    "    encoder_outputs, encoder_hidden, encoder_cell = encoder(src_tensor, encoder_hidden, src_true_len, encoder_cell)\n",
    "    decoder_input = torch.tensor([SOS_index]*batch_size, device=device).unsqueeze(1)\n",
    "    decoder_hidden, decoder_cell = encoder_hidden, decoder.initHidden(batch_size)\n",
    "\n",
    "    decoding_token_index = 0\n",
    "    stop_flag = [False]*batch_size\n",
    "    step_log_likelihoods = []\n",
    "    tgt_pred_batch = [[] for i_batch in range(batch_size)]\n",
    "    tgt_true_len_max = tgt_true_len.cpu().numpy().max()\n",
    "    #sent_not_end_index = list(range(batch_size))\n",
    "#     print(''.join(src_org_batch[0]), '\\n', \n",
    "#           ''.join(tgt_org_batch[0]), '\\n', ''.join(tgt_pred_batch[0]))\n",
    "    while decoding_token_index < tgt_max_length:\n",
    "        decoder_output, decoder_hidden, _, decoder_cell = decoder(decoder_input, decoder_hidden, src_true_len, encoder_outputs, decoder_cell)\n",
    "\n",
    "        # compute loss \n",
    "        if decoding_token_index < tgt_true_len_max:\n",
    "            decoding_label_vocab = tgt_label_vocab[:, decoding_token_index]\n",
    "            decoding_label_copy = tgt_label_copy[:, decoding_token_index, :]\n",
    "            copy_log_probs = decoder_output[:, vocab_pred_size:]+(decoding_label_copy.float()+1e-45).log()\n",
    "            #mask sample which is copied only\n",
    "            gen_mask = ((decoding_label_vocab!=OOV_pred_index) | (decoding_label_copy.sum(-1)==0)).float() \n",
    "            log_gen_mask = (gen_mask + 1e-45).log().unsqueeze(-1)\n",
    "            #mask log_prob value for oov_pred_index when label_vocab==oov_pred_index and is copied \n",
    "            generation_log_probs = decoder_output.gather(1, decoding_label_vocab.unsqueeze(1)) + log_gen_mask\n",
    "            combined_gen_and_copy = torch.cat((generation_log_probs, copy_log_probs), dim=-1)\n",
    "            step_log_likelihood = torch.logsumexp(combined_gen_and_copy, dim=-1)\n",
    "            step_log_likelihoods.append(step_log_likelihood.unsqueeze(1))\n",
    "\n",
    "        #\n",
    "        topv, topi = decoder_output.topk(1, dim=-1)\n",
    "        next_input = topi.detach().cpu().squeeze(1)\n",
    "        decoder_input = []\n",
    "        for i_batch in range(batch_size):\n",
    "            pred_list = vocab_pred+src_org_batch[i_batch]+['<EOS>']\n",
    "#             if decoding_token_index == 0:\n",
    "#                 print(i_batch,'pred_list', pred_list)\n",
    "            next_input_token = pred_list[next_input[i_batch].item()]\n",
    "            #if i_batch == 0:\n",
    "#             print(i_batch, decoding_token_index, next_input_token)\n",
    "            if next_input_token == vocab_pred[EOS_pred_index]:\n",
    "                stop_flag[i_batch] = True\n",
    "            if not stop_flag[i_batch]:\n",
    "                tgt_pred_batch[i_batch].append(next_input_token)\n",
    "                #if i_batch == 0:\n",
    "#                 print(i_batch, 'tgt_pred', (len(tgt_pred_batch[0]),len(tgt_pred_batch[1])),\n",
    "#                       (''.join(tgt_pred_batch[0]), ''.join(tgt_pred_batch[1])))\n",
    "            decoder_input.append(trainLang.word2index.get(next_input_token, UNK_index))\n",
    "        decoder_input = torch.tensor(decoder_input, device=device).unsqueeze(1)\n",
    "        decoding_token_index += 1\n",
    "        if all(stop_flag):\n",
    "            break\n",
    "#     print(len(tgt_pred_batch[0]), ''.join(tgt_pred_batch[0]))\n",
    "    log_likelihoods = torch.cat(step_log_likelihoods, dim=-1)\n",
    "    # mask padding for tgt\n",
    "    tgt_pad_mask = sequence_mask(tgt_true_len).float()\n",
    "    log_likelihoods = log_likelihoods*tgt_pad_mask[:,:log_likelihoods.size(1)]\n",
    "    loss += -(log_likelihoods.sum()/tgt_pad_mask.sum()).item()\n",
    "    tgt_pred.extend(tgt_pred_batch)\n",
    "    src_org.extend(src_org_batch)\n",
    "    tgt_org.extend(tgt_org_batch)\n",
    "loss = loss/len(loader)\n",
    "eval_len = len(tgt_pred)\n",
    "precision = np.zeros((eval_len,))\n",
    "recall = np.zeros((eval_len,))\n",
    "for i in range(eval_len):\n",
    "    org_facts = ''.join(tgt_org[i]).split('$')\n",
    "    pred_facts = ''.join(tgt_pred[i]).split('$')\n",
    "    org_facts_num = len(org_facts)\n",
    "    pred_facts_num = len(pred_facts)\n",
    "    org_match_num = np.zeros((org_facts_num))\n",
    "    pred_match_num = np.zeros((pred_facts_num))\n",
    "    for org_i in range(org_facts_num):\n",
    "        for pred_i in range(pred_facts_num):\n",
    "            org_fact = org_facts[org_i]\n",
    "            pred_fact = pred_facts[pred_i]\n",
    "            org_fact_ele = org_fact.split('@')\n",
    "            pred_fact_ele = pred_fact.split('@')\n",
    "            if len(org_fact_ele) == len(pred_fact_ele):\n",
    "                ele_num = len(org_fact_ele)\n",
    "                if difflib.SequenceMatcher(None,org_fact,pred_fact).ratio() > 0.85:\n",
    "                    org_match_num[org_i] += 1\n",
    "                    pred_match_num[pred_i] += 1\n",
    "                    break\n",
    "                ele_sim = np.zeros((ele_num,))\n",
    "                for ele_i in range(ele_num):\n",
    "                    ele_sim[ele_i] = difflib.SequenceMatcher(None,org_fact_ele[ele_i],pred_fact_ele[ele_i]).ratio()\n",
    "                if ele_sim.mean() > 0.85:\n",
    "                    org_match_num[org_i] += 1\n",
    "                    pred_match_num[pred_i] += 1\n",
    "    precision[i] = pred_match_num.mean()\n",
    "    recall[i] = org_match_num.mean()\n",
    "if False:\n",
    "    random_sample = np.random.randint(eval_len)\n",
    "    print('src:', src_org[random_sample])\n",
    "    print('Ref: ', tgt_org[random_sample])\n",
    "    print('pred: ', tgt_pred[random_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: ['在', '奎', '因', '德', '基', '姆', '一', '直', '对', '态', '度', '傲', '慢', '的', '藤', '井', '感', '到', '害', '怕', '。']\n",
      "Ref:  ['奎', '因', '德', '基', '姆', '@', '一', '直', '对', 'X', '感', '到', 'Y', '@', '藤', '井', '&', '害', '怕', '$', '藤', '井', '@', 'DESC', '@', '态', '度', '傲', '慢', '的']\n",
      "pred:  ['在', '奎', '因', '德', '基', '姆', '@', '一', '直', '对', 'X', '感', '到', '害', '怕', '@', '态', '度', '$', '藤']\n"
     ]
    }
   ],
   "source": [
    "random_sample = np.random.randint(eval_len)\n",
    "print('src:', src_org[random_sample])\n",
    "print('Ref: ', tgt_org[random_sample])\n",
    "print('pred: ', tgt_pred[random_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38366558696869574"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#precision.mean()\n",
    "recall.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample = 8\n",
    "print('src:', src_org[random_sample])\n",
    "print('Ref: ', tgt_org[random_sample])\n",
    "print('pred: ', tgt_pred[random_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.zeros((3,3))\n",
    "a2 = np.zeros((3,3))\n",
    "# a1 = np.zeros((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([a1,a2]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
