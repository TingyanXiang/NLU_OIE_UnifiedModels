{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /state/partition1/job-346663/jieba.cache\n",
      "Loading model cost 0.768 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "saoke = []\n",
    "for line in open('../data/SAOKE_DATA.json', 'r'):\n",
    "    saoke.append(json.loads(line))\n",
    "\n",
    "def replaceMissinfo(aaa):\n",
    "    placeholder = ['Z','Y','X']\n",
    "    for i in range(len(aaa)):\n",
    "        if aaa[i] == '_':\n",
    "            aaa = aaa[:i] + placeholder.pop() + aaa[i+1:]\n",
    "    return aaa\n",
    "def replaceMisspred(aaa):\n",
    "    if aaa == '_':\n",
    "        return 'P'\n",
    "    else:\n",
    "        return aaa\n",
    "    \n",
    "import jieba\n",
    "sentences = []\n",
    "labels = []\n",
    "original = []\n",
    "original_label = []\n",
    "for sentence in saoke:\n",
    "    if sentence['logic'] == []:\n",
    "        continue\n",
    "    seg_list = jieba.cut(sentence['natural'], cut_all=False)\n",
    "    sentences.append(list(seg_list))\n",
    "    original.append(sentence)\n",
    "    sent = ''\n",
    "    for o in sentence['logic']:\n",
    "        sent += str(tuple([replaceMissinfo(o['subject'])]+ [replaceMisspred(o['predicate'])] + [replaceMissinfo(o['object'][0])]))\n",
    "        sent += '$'\n",
    "        sent = sent.replace(\"'\",\"\").replace(\",\",\"@\").replace(\" \",\"\").replace(\"(\",\"\").replace(\")\",\"\")\n",
    "    seg_list = jieba.cut(sent[:-1], cut_all=False)\n",
    "    original_label.append(sent[:-1])\n",
    "    labels.append(list(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_prefix = ['PAD', 'UNK_token', 'EOS', 'SOS']\n",
    "UNK_token = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import dropwhile\n",
    "\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name, emb_pretrained_add, max_vocab_size = None):\n",
    "        self.name = name\n",
    "        self.word2index = None #{\"$PAD$\": PAD_token, \"$SOS$\": SOS_token, \"$EOS$\": EOS_token, \"$UNK$\": UNK_token}\n",
    "        #self.word2count = None #{\"$PAD$\": 0, \"$SOS$\" : 0, \"$EOS$\": 0, \"$UNK$\": 0}\n",
    "        self.index2word = None #{PAD_token: \"$PAD$\", SOS_token: \"$SOS$\", EOS_token: \"$EOS$\", UNK_token: \"$UNK$\"}\n",
    "        self.max_vocab_size = max_vocab_size  # Count SOS and EOS\n",
    "        self.vocab_size = None\n",
    "        self.emb_pretrained_add = emb_pretrained_add\n",
    "        self.embedding_matrix = None\n",
    "\n",
    "    def build_vocab(self, train_data, label_data):\n",
    "        all_tokens = []\n",
    "        for sent in train_data:\n",
    "            all_tokens.extend(sent)\n",
    "        for sent in label_data:\n",
    "            all_tokens.extend(sent)    \n",
    "        token_counter = Counter(all_tokens)\n",
    "        print('The number of unique tokens totally in dataset: ', len(token_counter))\n",
    "        for key, count in dropwhile(lambda key_count: key_count[1] > 1, token_counter.most_common()):\n",
    "            del token_counter[key]\n",
    "        if self.max_vocab_size:\n",
    "            vocab, count = zip(*token_counter.most_common(self.max_vocab_size))\n",
    "        else:\n",
    "            vocab, count = zip(*token_counter.most_common())\n",
    "        self.index2word = vocab_prefix + list(vocab)\n",
    "        word2index = dict(zip(vocab, range(len(vocab_prefix),len(vocab_prefix)+len(vocab)))) \n",
    "        for idx, token in enumerate(vocab_prefix):\n",
    "            word2index[token] = idx\n",
    "        self.word2index = word2index\n",
    "        return None \n",
    "\n",
    "    def build_emb_weight(self):\n",
    "        words_emb_dict = load_emb_vectors(self.emb_pretrained_add)\n",
    "        vocab_size = len(self.index2word)\n",
    "        self.vocab_size = vocab_size\n",
    "        emb_weight = np.zeros([vocab_size, 300])\n",
    "        for i in range(len(vocab_prefix), vocab_size):\n",
    "            emb = words_emb_dict.get(self.index2word[i], None)\n",
    "            if emb is not None:\n",
    "                try:\n",
    "                    emb_weight[i] = emb\n",
    "                except:\n",
    "                    pass\n",
    "                    #print(len(emb), self.index2word[i], emb)\n",
    "        self.embedding_matrix = emb_weight\n",
    "        return None\n",
    "\n",
    "def load_emb_vectors(fasttest_home):\n",
    "    max_num_load = 500000\n",
    "    words_dict = {}\n",
    "    with open(fasttest_home) as f:\n",
    "        for num_row, line in enumerate(f):\n",
    "            if num_row >= max_num_load:\n",
    "                break\n",
    "            s = line.split()\n",
    "            words_dict[s[0]] = np.asarray(s[1:])\n",
    "    return words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_token = OOV = 1\n",
    "EOS_token = 2\n",
    "\n",
    "def text2index(data, word2index):\n",
    "    indexdata = []\n",
    "    for line in data:\n",
    "        indexdata.append([word2index[c] if c in word2index.keys() else UNK_token for c in line])\n",
    "        #indexdata[-1].append(EOS_token)\n",
    "    print('finish')\n",
    "    return indexdata\n",
    "\n",
    "def construct_Lang(name, train_data, train_label, max_vocab_size = None, emb_pretrained_add = None):\n",
    "    lang = Lang(name, max_vocab_size, emb_pretrained_add)\n",
    "    lang.build_vocab(train_data, train_label)\n",
    "    if emb_pretrained_add:\n",
    "        lang.build_emb_weight()\n",
    "    return lang\n",
    "\n",
    "def text2symbolindex(data, word2index):\n",
    "    indexdata = []\n",
    "    for line in data:\n",
    "        indexdata.append([word2index[c] if c in word2index.keys() else OOV for c in line])\n",
    "        #indexdata[-1].append(EOS_token)\n",
    "    print('finish')\n",
    "    return indexdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique tokens totally in dataset:  86302\n"
     ]
    }
   ],
   "source": [
    "trainLang = construct_Lang('train', sentences, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "train_input_index = text2index(sentences, trainLang.word2index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "label_input_index = text2index(labels, trainLang.word2index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_dict = ['PAD','OOV','EOS','ISA','DESC','IN','BIRTH',\"DEATH\", \"=\",\"$\", \"[\",\"]\",\"|\",\"X\",\"Y\",\"Z\",\"P\",\"@\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbol_index = []\n",
    "# for o in symbol_dict:\n",
    "#     symbol_index.append(trainLang.word2index[o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2symbolindex =  {}\n",
    "for idx, token in enumerate(symbol_dict):\n",
    "        word2symbolindex[token] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "label_input_symbolindex = text2symbolindex(labels, word2symbolindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check valid\n",
    "# for idx,i in enumerate(label_input_symbolindex[0]):\n",
    "#     if symbol_dict[i] == 'OOV':\n",
    "#         continue\n",
    "#     elif symbol_dict[i] == labels[0][idx]:\n",
    "#         continue\n",
    "#     else:\n",
    "#         print(symbol_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_indicator(sentences,labels):\n",
    "    indicator = []\n",
    "    for i in range(len(sentences)):\n",
    "        tgt = labels[i]\n",
    "        src = sentences[i]\n",
    "        matrix = np.zeros((len(tgt)+1,len(src)+1))\n",
    "        for m in range(len(tgt)):\n",
    "            for n in range(len(src)):\n",
    "                if tgt[m] == src[n]:\n",
    "                    matrix[m,n] = 1\n",
    "        matrix[len(tgt),len(src)] = 1\n",
    "        indicator.append(matrix)\n",
    "    return indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator = copy_indicator(sentences,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40806, 40806, 40806, 40806, 40806, 40806)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_input_index),len(label_input_index),len(label_input_symbolindex),len(indicator),len(sentences),len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_token = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os.path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "\n",
    "class VocabDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, src_index, tgt_index, tgt_symbolindex, tgt_indicator, src_original, tgt_original, src_clip = None, tgt_clip = None):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.src_clip = src_clip\n",
    "        self.tgt_clip = tgt_clip\n",
    "        self.src_list, self.tgt_list = src_index, tgt_index\n",
    "        self.src_original, self.tgt_original = src_original, tgt_original\n",
    "        self.tgt_symbolindex, self.tgt_indicator  = tgt_symbolindex, tgt_indicator\n",
    "        \n",
    "        assert (len(self.src_list) == len(self.tgt_list)== len(self.src_original)== len(self.tgt_original)== len(self.tgt_symbolindex)== len(self.tgt_indicator))\n",
    "        #self.word2index = word2index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        src = self.src_list[key]\n",
    "        tgt = self.tgt_list[key]\n",
    "        src_org = self.src_original[key]\n",
    "        tgt_org = self.tgt_original[key]\n",
    "        tgt_sym = self.tgt_symbolindex[key]\n",
    "        tgt_ind = self.tgt_indicator[key]\n",
    "        \n",
    "        if self.src_clip is not None:\n",
    "            src = src[:self.src_clip]\n",
    "            src_org = src_org[:self.src_clip]\n",
    "            tgt_ind = tgt_ind[:,:self.src_clip]\n",
    "        src_length = len(src)\n",
    "\n",
    "        if self.tgt_clip is not None:\n",
    "            tgt = tgt[:self.tgt_clip]\n",
    "            tgt_org = tgt_org[:self.tgt_clip]\n",
    "            tgt_sym = tgt_sym[:self.tgt_clip]\n",
    "            tgt_ind = tgt_ind[:self.tgt_clip,:]\n",
    "        tgt_length = len(tgt)\n",
    "        \n",
    "        return src,src_length,tgt,tgt_length, tgt_sym, tgt_ind, src_org, tgt_org, \n",
    "        \n",
    "        #return src_org, src_tensor, src_true_len, tgt_org, tgt_tensor, tgt_true_len, tgt_label_vocab, tgt_label_copy \n",
    "\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    src_list = []\n",
    "    tgt_list = []\n",
    "    src_length_list = []\n",
    "    tgt_length_list = []\n",
    "    tgt_symbol_list = []\n",
    "    tgt_indicator_list = []\n",
    "    src_org_list = []\n",
    "    tgt_org_list = []\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for datum in batch:\n",
    "        src_length_list.append(datum[1]+1)\n",
    "        tgt_length_list.append(datum[3]+1)\n",
    "    \n",
    "    batch_max_src_length = np.max(src_length_list)\n",
    "    batch_max_tgt_length = np.max(tgt_length_list)\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]+[EOS_token]),\n",
    "                                pad_width=((0, batch_max_src_length-datum[1]-1)),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        src_list.append(padded_vec)\n",
    "        \n",
    "        padded_vec = np.pad(np.array(datum[2]+[EOS_token]),\n",
    "                                pad_width=((0, batch_max_tgt_length-datum[3]-1)),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        tgt_list.append(padded_vec)\n",
    "        \n",
    "        padded_vec = np.pad(np.array(datum[4]+[EOS_token]),\n",
    "                                pad_width=((0, batch_max_tgt_length-datum[3]-1)),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        tgt_symbol_list.append(padded_vec)\n",
    "        \n",
    "        padded_vec = np.pad(np.array(datum[5]),\n",
    "                                pad_width=((0, batch_max_tgt_length-datum[3]-1),((0, batch_max_src_length-datum[1]-1))),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        tgt_indicator_list.append(padded_vec)\n",
    "        \n",
    "        src_org_list.append(datum[6])\n",
    "        tgt_org_list.append(datum[7])\n",
    "    \n",
    "    # re-order\n",
    "#     ind_dec_order = np.argsort(src_length_list)[::-1]\n",
    "#     data_list = np.array(data_list)[ind_dec_order]\n",
    "#     train_length_list = np.array(train_length_list)[ind_dec_order]\n",
    "#     label_list = np.array(label_list)[ind_dec_order]\n",
    "#     label_length_list = np.array(label_length_list)[ind_dec_order]\n",
    "\n",
    "    src_list = np.array(src_list)\n",
    "    src_length_list = np.array(src_length_list)\n",
    "    tgt_list = np.array(tgt_list)\n",
    "    tgt_length_list = np.array(tgt_length_list)\n",
    "    tgt_symbol_list = np.array(tgt_symbol_list)\n",
    "    tgt_indicator_list = np.array(tgt_indicator_list)\n",
    "    \n",
    "    #print(type(np.array(data_list)),type(np.array(label_list)))\n",
    "    \n",
    "    return [torch.from_numpy(src_list).to(device), \n",
    "            torch.LongTensor(src_length_list).to(device), \n",
    "            torch.from_numpy(tgt_list).to(device), \n",
    "            torch.LongTensor(tgt_length_list).to(device),\n",
    "            torch.from_numpy(tgt_symbol_list).to(device),\n",
    "            torch.from_numpy(tgt_indicator_list).to(device),\n",
    "            src_org_list,\n",
    "            tgt_org_list,           \n",
    "           ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VocabDataset(train_input_index, label_input_index, label_input_symbolindex, indicator, sentences, labels)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=1,\n",
    "                                               collate_fn=vocab_collate_func,\n",
    "                                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   66  5126    59 30342    16 14787  4429     3 12152  5636     9  5636\n",
      "     16  4430 21069 21070    35     5    43  5015    59 23081    16 26988\n",
      "      5  5637    15  3122  3324 45328    50  1020 23082   232     3    35\n",
      "      9  3273  4429     5    15  3325  5637   891    46    50  1020  1969\n",
      "  12933     3  1970    35     5  4897 45329    50 14787    16 13724 18827\n",
      "      3 12152  5636     9 23081    16 21071  9141 45330     5  3324    43\n",
      "  45331  9531     9 18828    16     0 45332     9 45333 30343    16 13725\n",
      "     35     3  9142     8     2]]\n",
      "tensor([[   66,  5126,    59, 30342,    16, 14787,  4429,     3, 12152,  5636,\n",
      "             9,  5636,    16,  4430, 21069, 21070,    35,     5,    43,  5015,\n",
      "            59, 23081,    16, 26988,     5,  5637,    15,  3122,  3324, 45328,\n",
      "            50,  1020, 23082,   232,     3,    35,     9,  3273,  4429,     5,\n",
      "            15,  3325,  5637,   891,    46,    50,  1020,  1969, 12933,     3,\n",
      "          1970,    35,     5,  4897, 45329,    50, 14787,    16, 13724, 18827,\n",
      "             3, 12152,  5636,     9, 23081,    16, 21071,  9141, 45330,     5,\n",
      "          3324,    43, 45331,  9531,     9, 18828,    16,     0, 45332,     9,\n",
      "         45333, 30343,    16, 13725,    35,     3,  9142,     8,     2]],\n",
      "       device='cuda:0')\n",
      "tensor([89], device='cuda:0')\n",
      "tensor([[    6,     2,    66,  5126,    59,     2,    11, 30342,     7, 14787,\n",
      "          4429,    10,     3,    11, 12152,  5636,     7,  5636,     7,  4430,\n",
      "         21069, 21070,    10,    35,     4,     6,     2,    43,  5015,    59,\n",
      "             2,    11, 23081,     7, 26988,    10,     4,     6,     2,  3324,\n",
      "         45328,    50,     2,  1020, 23082,   232,     3,    11,    35,     7,\n",
      "           136,    10,  2723,   789,     4,     6,     2,   891,    46,    50,\n",
      "             2,  1020,  1969, 12933,     3,  1970,    35,     4,     6,     2,\n",
      "         45329,    50,     2,    11, 14787,     7, 13724, 18827,    10,     3,\n",
      "            11, 12152,  5636,     7, 23081,     7, 21071,  9141, 45330,    10,\n",
      "             4,     6,     2,  3324,    43, 45331,     2,    11,  9531,     7,\n",
      "         18828,    10,     7,    11,    92,  3495, 45332,     7,    11, 45333,\n",
      "         30343,     7, 13725,    35,    10,     3,  9142,    10,     2]],\n",
      "       device='cuda:0')\n",
      "tensor([119], device='cuda:0')\n",
      "tensor([[13, 17,  1,  1,  1, 17, 10,  1, 12,  1,  1, 11,  1, 10,  1,  1, 12,  1,\n",
      "         12,  1,  1,  1, 11,  1,  9, 13, 17,  1,  1,  1, 17, 10,  1, 12,  1, 11,\n",
      "          9, 13, 17,  1,  1,  1, 17,  1,  1,  1,  1, 10,  1, 12,  1, 11,  1,  1,\n",
      "          9, 13, 17,  1,  1,  1, 17,  1,  1,  1,  1,  1,  1,  9, 13, 17,  1,  1,\n",
      "         17, 10,  1, 12,  1,  1, 11,  1, 10,  1,  1, 12,  1, 12,  1,  1,  1, 11,\n",
      "          9, 13, 17,  1,  1,  1, 17, 10,  1, 12,  1, 11, 12, 10,  1,  1,  1, 12,\n",
      "         10,  1,  1, 12,  1,  1, 11,  1,  1, 11,  2]], device='cuda:0')\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.]]], device='cuda:0', dtype=torch.float64)\n",
      "[['主要', '栖息', '于', '低山', '和', '山麓', '地带', '的', '次生', '阔叶林', '、', '阔叶林', '和', '针', '阔叶', '混交林', '中', '，', '也', '出入', '于', '人工林', '和', '针叶林', '，', '夏季', '在', '北方', '有时', '可上', '到', '海拔', '1700', '米', '的', '中', '、', '高山', '地带', '，', '在', '南方', '夏季', '甚至', '上', '到', '海拔', '3000', '米左右', '的', '森林', '中', '，', '冬季', '多下', '到', '山麓', '和', '邻近', '平原地带', '的', '次生', '阔叶林', '、', '人工林', '和', '林缘', '疏', '林灌丛', '，', '有时', '也', '进到', '果园', '、', '道旁', '和', '地边', '树丛', '、', '房前', '屋后', '和', '庭院', '中', '的', '树上', '。']]\n",
      "[['X', '@', '主要', '栖息', '于', '@', '[', '低山', '|', '山麓', '地带', ']', '的', '[', '次生', '阔叶林', '|', '阔叶林', '|', '针', '阔叶', '混交林', ']', '中', '$', 'X', '@', '也', '出入', '于', '@', '[', '人工林', '|', '针叶林', ']', '$', 'X', '@', '有时', '可上', '到', '@', '海拔', '1700', '米', '的', '[', '中', '|', '高', ']', '山地', '带', '$', 'X', '@', '甚至', '上', '到', '@', '海拔', '3000', '米左右', '的', '森林', '中', '$', 'X', '@', '多下', '到', '@', '[', '山麓', '|', '邻近', '平原地带', ']', '的', '[', '次生', '阔叶林', '|', '人工林', '|', '林缘', '疏', '林灌丛', ']', '$', 'X', '@', '有时', '也', '进到', '@', '[', '果园', '|', '道旁', ']', '|', '[', '地', '边', '树丛', '|', '[', '房前', '屋后', '|', '庭院', '中', ']', '的', '树上', ']']]\n"
     ]
    }
   ],
   "source": [
    "for src_tensor, src_true_len, tgt_tensor, tgt_true_len, tgt_label_vocab, tgt_label_copy, src_org, tgt_org in train_loader:\n",
    "    print(src_tensor[:10])\n",
    "    print(src_true_len)\n",
    "    print(tgt_tensor[:10])\n",
    "    print(tgt_true_len)\n",
    "    print(tgt_label_vocab[:10])\n",
    "    print(tgt_label_copy[:10,:10])\n",
    "    print(src_org[:10])\n",
    "    print(tgt_org[:10])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
