{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "saoke = []\n",
    "with open('SAOKE_DATA.json', 'r') as f:\n",
    "    for line in f:\n",
    "        saoke.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceMisspred(predicate):\n",
    "    '''replace missing predicate\n",
    "    '''\n",
    "    if predicate == '_':\n",
    "        return 'P'\n",
    "    else:\n",
    "        return predicate\n",
    "\n",
    "# def replaceMissinfo(aaa):\n",
    "#     '''replace missing info for subjects/objects\n",
    "#     '''\n",
    "#     placeholder = ['Z','Y','X']\n",
    "#     for i in range(len(aaa)):\n",
    "#         if aaa[i] == '_':\n",
    "#             aaa = aaa[:i] + placeholder.pop() + aaa[i+1:]\n",
    "#     return aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess saoke data \n",
    "data = []\n",
    "for sample in saoke:\n",
    "    # remove some exceptions with empty facts\n",
    "    if sample['logic'] == []:\n",
    "        continue\n",
    "    # tokenize src sentence\n",
    "    sample_processed = dict()\n",
    "    sample_processed['src_org'] = sample['natural']\n",
    "    sample_processed['src'] = list(jieba.cut(sample['natural'], cut_all=False))\n",
    "    \n",
    "    # transform fact list into str and tokenize\n",
    "    # $ separates facts; @ separate elements for one fact; & separate objects for one fact\n",
    "    sample_processed['tgt_org'] = sample['logic']\n",
    "    logic_list = []\n",
    "    for fact in sample['logic']:\n",
    "        logic_list.append('@'.join([fact['subject'], replaceMisspred(fact['predicate']), \n",
    "                                   '&'.join(fact['object'])]))\n",
    "    logic_str = '$'.join(logic_list)\n",
    "    sample_processed['tgt'] = list(jieba.cut(logic_str, cut_all=False))\n",
    "    \n",
    "    data.append(sample_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[5]['tgt_org']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_prefix = ['<PAD>', '<UNK>', '<EOS>', '<SOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import dropwhile\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name, emb_pretrained_add=None, max_vocab_size=None):\n",
    "        self.name = name\n",
    "        self.word2index = None #{\"$PAD$\": PAD_token, \"$SOS$\": SOS_token, \"$EOS$\": EOS_token, \"$UNK$\": UNK_token}\n",
    "        #self.word2count = None #{\"$PAD$\": 0, \"$SOS$\" : 0, \"$EOS$\": 0, \"$UNK$\": 0}\n",
    "        self.index2word = None #{PAD_token: \"$PAD$\", SOS_token: \"$SOS$\", EOS_token: \"$EOS$\", UNK_token: \"$UNK$\"}\n",
    "        self.max_vocab_size = max_vocab_size  # Count SOS and EOS\n",
    "        self.vocab_size = None\n",
    "        self.emb_pretrained_add = emb_pretrained_add\n",
    "        self.embedding_matrix = None\n",
    "\n",
    "    def build_vocab(self, data):\n",
    "        all_tokens = []\n",
    "        for sample in data:\n",
    "            all_tokens.extend(sample['src'])\n",
    "            all_tokens.extend(sample['tgt'])  \n",
    "        token_counter = Counter(all_tokens)\n",
    "        print('The number of unique tokens totally in dataset: ', len(token_counter))\n",
    "        # remove word with freq==1 \n",
    "        for key, count in dropwhile(lambda key_count: key_count[1] > 1, token_counter.most_common()):\n",
    "            del token_counter[key]\n",
    "        \n",
    "        if self.max_vocab_size:\n",
    "            vocab, count = zip(*token_counter.most_common(self.max_vocab_size))\n",
    "        else:\n",
    "            vocab, count = zip(*token_counter.most_common())\n",
    "        \n",
    "        self.index2word = vocab_prefix + list(vocab)\n",
    "        word2index = dict(zip(self.index2word, range(0, len(self.index2word)))) \n",
    "#         word2index = dict(zip(vocab, range(len(vocab_prefix),len(vocab_prefix)+len(vocab)))) \n",
    "#         for idx, token in enumerate(vocab_prefix):\n",
    "#             word2index[token] = idx\n",
    "        self.word2index = word2index\n",
    "        return None \n",
    "\n",
    "    def build_emb_weight(self):\n",
    "        words_emb_dict = load_emb_vectors(self.emb_pretrained_add)\n",
    "        vocab_size = len(self.index2word)\n",
    "        self.vocab_size = vocab_size\n",
    "        emb_weight = np.zeros([vocab_size, 300])\n",
    "        for i in range(len(vocab_prefix), vocab_size):\n",
    "            emb = words_emb_dict.get(self.index2word[i], None)\n",
    "            if emb is not None:\n",
    "                try:\n",
    "                    emb_weight[i] = emb\n",
    "                except:\n",
    "                    pass\n",
    "                    #print(len(emb), self.index2word[i], emb)\n",
    "        self.embedding_matrix = emb_weight\n",
    "        return None\n",
    "\n",
    "def load_emb_vectors(fasttest_home):\n",
    "    max_num_load = 500000\n",
    "    words_dict = {}\n",
    "    with open(fasttest_home) as f:\n",
    "        for num_row, line in enumerate(f):\n",
    "            if num_row >= max_num_load:\n",
    "                break\n",
    "            s = line.split()\n",
    "            words_dict[s[0]] = np.asarray(s[1:])\n",
    "    return words_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_token = 1\n",
    "oov_pred_index = 1\n",
    "EOS_token = 2\n",
    "\n",
    "def text2index(data, key, word2index):\n",
    "    indexdata = []\n",
    "    for line in data:\n",
    "        line = line[key]\n",
    "        indexdata.append([word2index[c] if c in word2index.keys() else UNK_token for c in line])\n",
    "        #indexdata[-1].append(EOS_token)\n",
    "    print('finish indexing')\n",
    "    return indexdata\n",
    "\n",
    "def construct_Lang(name, data, emb_pretrained_add = None, max_vocab_size = None):\n",
    "    lang = Lang(name, emb_pretrained_add, max_vocab_size)\n",
    "    lang.build_vocab(data)\n",
    "    if emb_pretrained_add:\n",
    "        lang.build_emb_weight()\n",
    "    return lang\n",
    "\n",
    "def text2symbolindex(data, key, word2index):\n",
    "    indexdata = []\n",
    "    for line in data:\n",
    "        line = line[key]\n",
    "        indexdata.append([word2index[c] if c in word2index.keys() else oov_pred_index for c in line])\n",
    "        #indexdata[-1].append(EOS_token)\n",
    "    print('finish')\n",
    "    return indexdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique tokens totally in dataset:  85719\n"
     ]
    }
   ],
   "source": [
    "trainLang = construct_Lang('train', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish indexing\n"
     ]
    }
   ],
   "source": [
    "src_input_index = text2index(data, 'src', trainLang.word2index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish indexing\n"
     ]
    }
   ],
   "source": [
    "tgt_input_index = text2index(data, 'tgt', trainLang.word2index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_pred = ['<PAD>','<OOV>','<EOS>','ISA','DESC','IN','BIRTH',\"DEATH\", \"=\",\"$\", \"[\",\"]\",\"|\",\"X\",\"Y\",\"Z\",\"P\",\"@\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbol_index = []\n",
    "# for o in symbol_dict:\n",
    "#     symbol_index.append(trainLang.word2index[o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2symbolindex = {}\n",
    "for idx, token in enumerate(vocab_pred):\n",
    "        word2symbolindex[token] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "label_symbolindex = text2symbolindex(data, 'tgt', word2symbolindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check valid\n",
    "# for idx,i in enumerate(label_input_symbolindex[0]):\n",
    "#     if symbol_dict[i] == 'OOV':\n",
    "#         continue\n",
    "#     elif symbol_dict[i] == labels[0][idx]:\n",
    "#         continue\n",
    "#     else:\n",
    "#         print(symbol_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_indicator(data, src_key='src', tgt_key='tgt'):\n",
    "    indicator = []\n",
    "    for sample in data:\n",
    "        tgt = sample[tgt_key]\n",
    "        src = sample[src_key]\n",
    "        matrix = np.zeros((len(tgt)+1, len(src)+1))\n",
    "        for m in range(len(tgt)):\n",
    "            for n in range(len(src)):\n",
    "                if tgt[m] == src[n]:\n",
    "                    matrix[m,n] = 1\n",
    "        matrix[len(tgt),len(src)] = 1\n",
    "        indicator.append(matrix)\n",
    "    return indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator = copy_indicator(data, 'src', 'tgt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indicator[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40806, 40806, 40806, 40806, 40806)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(src_input_index),len(tgt_input_index),len(label_symbolindex),len(indicator),len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os.path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "\n",
    "class VocabDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, src_index, tgt_index, tgt_symbolindex, tgt_indicator, data, src_clip = None, tgt_clip = None):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.src_clip = src_clip\n",
    "        self.tgt_clip = tgt_clip\n",
    "        self.src_list, self.tgt_list = src_index, tgt_index\n",
    "        self.data = data\n",
    "        self.tgt_symbolindex, self.tgt_indicator  = tgt_symbolindex, tgt_indicator\n",
    "        \n",
    "        assert (len(self.src_list) == len(self.tgt_list) == len(self.tgt_symbolindex)== len(self.tgt_indicator))\n",
    "        #self.word2index = word2index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        src = self.src_list[key]\n",
    "        tgt = self.tgt_list[key]\n",
    "        src_org = self.data[key]['src']\n",
    "        tgt_org = self.data[key]['tgt']\n",
    "        tgt_sym = self.tgt_symbolindex[key]\n",
    "        tgt_ind = self.tgt_indicator[key]\n",
    "        \n",
    "        if self.src_clip is not None:\n",
    "            src = src[:self.src_clip]\n",
    "            src_org = src_org[:self.src_clip]\n",
    "            #tgt_ind = tgt_ind[:,:self.src_clip]\n",
    "        src_length = len(src)\n",
    "\n",
    "        if self.tgt_clip is not None:\n",
    "            tgt = tgt[:self.tgt_clip]\n",
    "            tgt_org = tgt_org[:self.tgt_clip]\n",
    "            tgt_sym = tgt_sym[:self.tgt_clip]\n",
    "            tgt_ind = tgt_ind[:self.tgt_clip,:]\n",
    "        tgt_length = len(tgt)\n",
    "        \n",
    "        return src, src_length, tgt, tgt_length, tgt_sym, tgt_ind, src_org, tgt_org\n",
    "        \n",
    "        #return src_org, src_tensor, src_true_len, tgt_org, tgt_tensor, tgt_true_len, tgt_label_vocab, tgt_label_copy \n",
    "\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    src_list = []\n",
    "    tgt_list = []\n",
    "    src_length_list = []\n",
    "    tgt_length_list = []\n",
    "    tgt_symbol_list = []\n",
    "    tgt_indicator_list = []\n",
    "    src_org_list = []\n",
    "    tgt_org_list = []\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for datum in batch:\n",
    "        src_length_list.append(datum[1]+1)\n",
    "        tgt_length_list.append(datum[3]+1)\n",
    "    \n",
    "    batch_max_src_length = np.max(src_length_list)\n",
    "    batch_max_tgt_length = np.max(tgt_length_list)\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]+[EOS_token]),\n",
    "                                pad_width=((0, batch_max_src_length-datum[1]-1)),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        src_list.append(padded_vec)\n",
    "        \n",
    "        padded_vec = np.pad(np.array(datum[2]+[EOS_token]),\n",
    "                                pad_width=((0, batch_max_tgt_length-datum[3]-1)),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        tgt_list.append(padded_vec)\n",
    "        \n",
    "        padded_vec = np.pad(np.array(datum[4]+[EOS_token]),\n",
    "                                pad_width=((0, batch_max_tgt_length-datum[3]-1)),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        tgt_symbol_list.append(padded_vec)\n",
    "        \n",
    "        padded_vec = np.pad(np.array(datum[5]),\n",
    "                                pad_width=((0, batch_max_tgt_length-datum[3]-1),((0, batch_max_src_length-datum[1]-1))),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        tgt_indicator_list.append(padded_vec)\n",
    "        \n",
    "        src_org_list.append(datum[6])\n",
    "        tgt_org_list.append(datum[7])\n",
    "    \n",
    "    # re-order\n",
    "#     ind_dec_order = np.argsort(src_length_list)[::-1]\n",
    "#     data_list = np.array(data_list)[ind_dec_order]\n",
    "#     train_length_list = np.array(train_length_list)[ind_dec_order]\n",
    "#     label_list = np.array(label_list)[ind_dec_order]\n",
    "#     label_length_list = np.array(label_length_list)[ind_dec_order]\n",
    "\n",
    "    src_list = np.array(src_list)\n",
    "    src_length_list = np.array(src_length_list)\n",
    "    tgt_list = np.array(tgt_list)\n",
    "    tgt_length_list = np.array(tgt_length_list)\n",
    "    tgt_symbol_list = np.array(tgt_symbol_list)\n",
    "    tgt_indicator_list = np.array(tgt_indicator_list)\n",
    "    \n",
    "    #print(type(np.array(data_list)),type(np.array(label_list)))\n",
    "    \n",
    "    return [torch.from_numpy(src_list).to(device), \n",
    "            torch.LongTensor(src_length_list).to(device), \n",
    "            torch.from_numpy(tgt_list).to(device), \n",
    "            torch.LongTensor(tgt_length_list).to(device),\n",
    "            torch.from_numpy(tgt_symbol_list).to(device),\n",
    "            torch.from_numpy(tgt_indicator_list).to(device),\n",
    "            src_org_list,\n",
    "            tgt_org_list,           \n",
    "           ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VocabDataset(src_input_index, tgt_input_index, label_symbolindex, indicator, data)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=1,\n",
    "                                               collate_fn=vocab_collate_func,\n",
    "                                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   70,  5166,    63, 30553,    19, 14922,  4470,     5, 12243,  5682,\n",
      "            11,  5682,    19,  4471, 21254, 21255,    39,     7,    47,  5056,\n",
      "            63, 23252,    19, 27224,     7,  5683,    18,  3159,  3353, 45583,\n",
      "            54,  1034, 23253,   239,     5,    39,    11,  3302,  4470,     7,\n",
      "            18,  3354,  5683,   903,    51,    54,  1034,  1977, 13040,     5,\n",
      "          1994,    39,     7,  4938, 45584,    54, 14922,    19, 13846, 18972,\n",
      "             5, 12243,  5682,    11, 23252,    19, 21256,  9221, 45585,     7,\n",
      "          3353,    47, 45586,  9619,    11, 17503,    19,     1, 45587,    11,\n",
      "         45588, 30554,    19, 13847,    39,     5,  9222,    10,     2]])\n",
      "tensor([89])\n",
      "tensor([[    9,     4,    70,  5166,    63,     4,    13, 30553,     8, 14922,\n",
      "          4470,    12,     5,    13, 12243,  5682,     8,  5682,     8,  4471,\n",
      "         21254, 21255,    12,    39,     6,     9,     4,    47,  5056,    63,\n",
      "             4,    13, 23252,     8, 27224,    12,     6,     9,     4,  3353,\n",
      "         45583,    54,     4,  1034, 23253,   239,     5,    13,    39,     8,\n",
      "           140,    12,  2750,   799,     6,     9,     4,   903,    51,    54,\n",
      "             4,  1034,  1977, 13040,     5,  1994,    39,     6,     9,     4,\n",
      "         45584,    54,     4,    13, 14922,     8, 13846, 18972,    12,     5,\n",
      "            13, 12243,  5682,     8, 23252,     8, 21256,  9221, 45585,    12,\n",
      "             6,     9,     4,  3353,    47, 45586,     4,    13,  9619,     8,\n",
      "         17503,    12,     8,    13,    96,  3412, 45587,     8,    13, 45588,\n",
      "         30554,     8, 13847,    39,    12,     5,  9222,    12,     2]])\n",
      "tensor([119])\n",
      "tensor([[ 1, 17,  1,  1,  1, 17, 10,  1, 12,  1,  1, 11,  1, 10,  1,  1, 12,  1,\n",
      "         12,  1,  1,  1, 11,  1,  9,  1, 17,  1,  1,  1, 17, 10,  1, 12,  1, 11,\n",
      "          9,  1, 17,  1,  1,  1, 17,  1,  1,  1,  1, 10,  1, 12,  1, 11,  1,  1,\n",
      "          9,  1, 17,  1,  1,  1, 17,  1,  1,  1,  1,  1,  1,  9,  1, 17,  1,  1,\n",
      "         17, 10,  1, 12,  1,  1, 11,  1, 10,  1,  1, 12,  1, 12,  1,  1,  1, 11,\n",
      "          9,  1, 17,  1,  1,  1, 17, 10,  1, 12,  1, 11, 12, 10,  1,  1,  1, 12,\n",
      "         10,  1,  1, 12,  1,  1, 11,  1,  1, 11,  2]])\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.]]], dtype=torch.float64)\n",
      "[['主要', '栖息', '于', '低山', '和', '山麓', '地带', '的', '次生', '阔叶林', '、', '阔叶林', '和', '针', '阔叶', '混交林', '中', '，', '也', '出入', '于', '人工林', '和', '针叶林', '，', '夏季', '在', '北方', '有时', '可上', '到', '海拔', '1700', '米', '的', '中', '、', '高山', '地带', '，', '在', '南方', '夏季', '甚至', '上', '到', '海拔', '3000', '米左右', '的', '森林', '中', '，', '冬季', '多下', '到', '山麓', '和', '邻近', '平原地带', '的', '次生', '阔叶林', '、', '人工林', '和', '林缘', '疏', '林灌丛', '，', '有时', '也', '进到', '果园', '、', '道旁', '和', '地边', '树丛', '、', '房前', '屋后', '和', '庭院', '中', '的', '树上', '。']]\n",
      "[['_', '@', '主要', '栖息', '于', '@', '[', '低山', '|', '山麓', '地带', ']', '的', '[', '次生', '阔叶林', '|', '阔叶林', '|', '针', '阔叶', '混交林', ']', '中', '$', '_', '@', '也', '出入', '于', '@', '[', '人工林', '|', '针叶林', ']', '$', '_', '@', '有时', '可上', '到', '@', '海拔', '1700', '米', '的', '[', '中', '|', '高', ']', '山地', '带', '$', '_', '@', '甚至', '上', '到', '@', '海拔', '3000', '米左右', '的', '森林', '中', '$', '_', '@', '多下', '到', '@', '[', '山麓', '|', '邻近', '平原地带', ']', '的', '[', '次生', '阔叶林', '|', '人工林', '|', '林缘', '疏', '林灌丛', ']', '$', '_', '@', '有时', '也', '进到', '@', '[', '果园', '|', '道旁', ']', '|', '[', '地', '边', '树丛', '|', '[', '房前', '屋后', '|', '庭院', '中', ']', '的', '树上', ']']]\n"
     ]
    }
   ],
   "source": [
    "for src_tensor, src_true_len, tgt_tensor, tgt_true_len, tgt_label_vocab, tgt_label_copy, src_org, tgt_org in train_loader:\n",
    "    print(src_tensor[:10])\n",
    "    print(src_true_len)\n",
    "    print(tgt_tensor[:10])\n",
    "    print(tgt_true_len)\n",
    "    print(tgt_label_vocab[:10])\n",
    "    print(tgt_label_copy[:10,:10])\n",
    "    print(src_org[:10])\n",
    "    print(tgt_org[:10])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
